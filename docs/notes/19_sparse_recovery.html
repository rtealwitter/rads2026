<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Sparse Recovery</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c5a5d5e27fcc88644031c24cff017230.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../favicon.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>
</head><body class="nav-fixed quarto-light">\usepackage{amsbsy}

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Spring 2026</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://discord.gg/dES3fSPEeC"> 
<span class="menu-text">Discord</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/1091652"> 
<span class="menu-text">Gradescope</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Sparse Recovery</strong></h1>
</div>



<div class="quarto-title-meta column-page">

    
  
    
  </div>
  


</header>


<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>When we considered regression and the Fast JL Transform, we wanted to solve the regression problem by finding a vector <span class="math inline">\(\mathbf{x}\)</span> that minimized the mean squared error <span class="math display">\[\begin{align*}
\|\mathbf{Ax} - \mathbf{b} \|_2^2
\end{align*}\]</span> where <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{n \times d}\)</span> is a feature matrix and <span class="math inline">\(\mathbf{b} \in \mathbb{R}^n\)</span> is a target vector. In particular, we considered the setting where <span class="math inline">\(n \gg d\)</span> and we could only hope to return a solution that made the mean squared error small.</p>
<p>In sparse recovery, we consider the same problem from a different perspective and we assume that <span class="math inline">\(n \ll d\)</span>. Now, the the idea is that <span class="math inline">\(\mathbf{x}\)</span> is a vector that is <em>hidden</em> from us and we want to recover it by multiplying it by a matrix <span class="math inline">\(\mathbf{A}\)</span> and observe the result <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p align="center">
<img src="images/underdetermined_regression.png" width="200px">
</p>
<p>Because we can control so many coefficients in <span class="math inline">\(\mathbf{x}\)</span>, there are likely many solutions that make the mean squared error zero. We will make the problem more interesting by assuming that <span class="math inline">\(\mathbf{x}\)</span> has a special structure. In particular, we will assume that <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(k\)</span>-sparse i.e., there are at most <span class="math inline">\(k\)</span> non-zero entries in <span class="math inline">\(\mathbf{x}\)</span>. Typically, <span class="math inline">\(k \ll d\)</span>.</p>
<p>Since we know we <em>can</em> recover <span class="math inline">\(\mathbf{x}\)</span>, the question becomes how many measurements i.e., rows in <span class="math inline">\(\mathbf{A}\)</span> do we need to recover it?</p>
<p>We’ll start with a trivial solution. Notice that if we make each row of <span class="math inline">\(\mathbf{A}\)</span> a standard basis vector, then we can recover each entry of <span class="math inline">\(\mathbf{x}\)</span> by observing the corresponding entry of <span class="math inline">\(\mathbf{b}\)</span>. Unfortunately, this approach requires that we have as many rows in <span class="math inline">\(\mathbf{A}\)</span> as entries in <span class="math inline">\(\mathbf{x}\)</span> which requires <span class="math inline">\(O(d)\)</span> measurements and <span class="math inline">\(O(d^2)\)</span> space to even store <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Today, we will do better by coming up with an algorithm which makes <span class="math inline">\(O(k \log k)\)</span> measurements. This should be surprising because, while we know that <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(k\)</span>-sparse, we don’t know which <span class="math inline">\(k\)</span> entries are non-zero.</p>
<section id="applications" class="level3">
<h3 class="anchored" data-anchor-id="applications">Applications</h3>
<p><strong>Photography:</strong> Typically, cameras acquire images by measuring the intensity of light with one sensor per pixel. We could instead imagine a camera that measures the intensity of light with just one sensor. If the pixel intensities are <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>, then the single pixel returns <span class="math display">\[
b = \frac1{n} \sum_{i=1}^n x_i
= \begin{bmatrix} \frac1{n} &amp; \frac1{n} &amp; \cdots &amp; \frac1{n} \end{bmatrix} \mathbf{x}
\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
\]</span> which is not very much information about the image. But we can get more information from other linear measurements via masking.</p>
<p align="center">
<img src="images/maskedImage.png" width="200px">
</p>
<p>Let <span class="math display">\[
b_i = \frac1{n} \sum_{j \in S_i} x_j
= \begin{bmatrix} 0 &amp; \frac1{n} &amp; \cdots &amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}.
\]</span> If we take enough measurements <span class="math inline">\(b_i\)</span>, then we can recover the whole image.</p>
<p>There are applications of this approach in imaging outside of the visible spectrum, microscopy, and other scientific imaging. The theory we will discuss does not exactly describe these problems but it has been very valuable in modeling them.</p>
<p><strong>Medical Imaging (MRI):</strong> How do we measure entries of the Fourier transform <span class="math inline">\(\mathbf{Fx}\)</span> for an MRI? We blast the body with sound waves of varying frequency. If we can reduce the number of measurements, we can reduce the time the patient spends in the machine and the energy use of the procedure.</p>
<p><strong>Geophysics:</strong> How do we measure entries of the Fourier transform <span class="math inline">\(\mathbf{Fx}\)</span> for geophysical analysis? We blast the ground with sound waves of varying frequency (e.g.,using airguns, controlled explosions, and vibrations from drilling). If we can reduce the number of measurements, we can make the data acquisition process cheaper and less disruptive.</p>
</section>
<section id="fourier-transforms" class="level3">
<h3 class="anchored" data-anchor-id="fourier-transforms">Fourier Transforms</h3>
<p>The main roadblock to directly applying the sparse recovery problem to the examples we described is that the data we measure is not necessarily sparse. For example, an image is not sparse if we represent as a vector of pixel intensities. However, we can often represent the data in a different basis where it is sparse.</p>
<p>The Fourier transform is a linear transformation that decomposes data into frequencies. By the Uncertainty Principle in harmonic analysis, a vector and its Fourier transform cannot both be too dense. The figure below gives an example. On the left we have data with many non-zero entries and, on the left, we have the Fourier transform of the data which is (close to) sparse.</p>
<p align="center">
<img src="images/fourier_frequency.png" width="200px"> <img src="images/fourier_vector.png" width="200px">
</p>
<p>Generally, we can take dense data and make it sparse by applying the Fourier transform. The Fourier transform is closely related to the Hadamard matrix we analyzed for the Fast JL Transform algorithm. The discrete Fourier transform (DFT) <span class="math inline">\(\mathbf{F} \in \mathbb{C}^{n \times n}\)</span> is defined on complex numbers <span class="math display">\[
\mathbf{F}_{j,k} = e^{-2\pi i \frac{j k}{n}}
\]</span> where <span class="math inline">\(i\)</span> is the imaginary unit. The DFT is a unitary matrix so <span class="math inline">\(\mathbf{F}^* \mathbf{F} = \mathbf{I}\)</span> where <span class="math inline">\(\mathbf{F}^*\)</span> is the conjugate transpose of <span class="math inline">\(\mathbf{F}\)</span>.</p>
<p>Using the same divide-and-conquer algorithm as the Hadamard matrix, we can compute <span class="math inline">\(\mathbf{Fy}\)</span> the DFT of the vector <span class="math inline">\(\mathbf{y}\)</span> in <span class="math inline">\(O(n \log n)\)</span> time.</p>
<p>The real part of the <span class="math inline">\(j,k\)</span> entry is <span class="math inline">\(\cos(2\pi j k)\)</span> so the <span class="math inline">\(j\)</span>th row of <span class="math inline">\(\mathbf{F}\)</span> looks like a cosine wave with frequency <span class="math inline">\(j\)</span>. Computing <span class="math inline">\(\mathbf{Fy}\)</span> computes inner products of <span class="math inline">\(\mathbf{y}\)</span> with many different frequencies, which can be used to decompose the vector into a sum of those frequencies.</p>
<p align="center">
<img src="images/dftExample.png" width="200px">
</p>
<p>As we saw before, sampling does not preserve norms when <span class="math inline">\(\mathbf{y}\)</span> has a few large entries. Before, the hard case was when <span class="math inline">\(\mathbf{y}\)</span> had a few large entries and now the hard case is when <span class="math inline">\(\mathbf{y}\)</span> has too many non-zero entries. Taking the Fourier transform, just like taking the Hadamard transform, eliminates the hard case without changing the norm. Because of this property, the Fourier transform is one of the central tools in sparse recovery (also sometimes called compressed sensing).</p>
<p>The goal in sparse recover is to recover a vector <span class="math inline">\(\mathbf{x}\)</span> from linear measurements. We can choose <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> with <span class="math inline">\(m &lt; n\)</span>. Assume we can access the <em>measurement</em> <span class="math inline">\(\mathbf{b} = \mathbf{Ax}\)</span>. We will try to recover <span class="math inline">\(\mathbf{x}\)</span> from <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p>Since <span class="math inline">\(m &lt; n\)</span>, there are infinitely many vectors <span class="math inline">\(\mathbf{x}\)</span> that satisfy <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span>.</p>
</section>
</section>
<section id="sparse-recovery" class="level2">
<h2 class="anchored" data-anchor-id="sparse-recovery">Sparse Recovery</h2>
<p>Now that we have a sense of the problem, we will discuss how to solve it. We want to recover <span class="math inline">\(\mathbf{x}\)</span> from <span class="math inline">\(\mathbf{b} = \mathbf{Ax}\)</span> under the assumption that <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(k\)</span>-sparse i.e., it has at most <span class="math inline">\(k\)</span> non-zero entries.</p>
<p>In this problem, we get to choose the matrix <span class="math inline">\(\mathbf{A}\)</span> that we will use to measure <span class="math inline">\(\mathbf{x}\)</span>. Let’s consider the properties of <span class="math inline">\(\mathbf{A}\)</span> that would allow us to recover <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>One property that would <em>not</em> allow us to recover <span class="math inline">\(\mathbf{x}\)</span> is if <span class="math inline">\(\mathbf{A}\)</span> has repeated columns. Then we cannot distinguish between the entries of <span class="math inline">\(\mathbf{x}\)</span> that correspond to those columns.</p>
<p align="center">
<img src="images/sparseRegression.png" width="200px">
</p>
<p>There are several ways to formalize the property that <span class="math inline">\(\mathbf{A}\)</span> allows us to recover <span class="math inline">\(\mathbf{x}\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{A}\)</span> has Kruskal rank <span class="math inline">\(r\)</span> if all sets of <span class="math inline">\(r\)</span> columns are linearly independent.</p></li>
<li><p><span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(\mu\)</span>-incoherent if <span class="math inline">\(|\mathbf{A}_{i}^\top \mathbf{A}_j | \leq \mu \|\mathbf{A}_i \|_2 \| \mathbf{A}_j \|_2\)</span> for all distinct columns <span class="math inline">\(i,j\)</span>.</p></li>
</ul>
<p>Today, we will consider matrices that satisfy the RIP.</p>
<p><strong>Restricted Isometry Property (RIP):</strong> <span class="math inline">\(\mathbf{A}\)</span> satisfies the <span class="math inline">\((q,\epsilon)\)</span>-Restricted Isometry Property (RIP) if <span class="math inline">\((1-\epsilon) \| \mathbf{x} \|_2^2 \leq \| \mathbf{Ax} \|_2^2 \leq (1+\epsilon) \| \mathbf{x} \|_2^2\)</span> for all <span class="math inline">\(q\)</span>-sparse vectors <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Notice that RIP is a Johnson-Lindenstrauss type condition. However, it is not quite the same as the JL Lemma because it applies to all <span class="math inline">\(q\)</span>-sparse vectors instead of a discrete set of vectors or all vectors in a subspace.</p>
<p>A natural question is whether we can find matrices that satisfy RIP and how difficult it is to find them.</p>
<section id="random-matrices" class="level3">
<h3 class="anchored" data-anchor-id="random-matrices">Random Matrices</h3>
<p>Matrices that are satisfy RIP are not too rare. In fact, random Johnson-Lindenstrauss matrices (e.g., Gaussian, sign, sparse) with <span class="math inline">\(m=O\left(\frac{k \log(n/k}{\epsilon^2}\right)\)</span> rows are <span class="math inline">\((q, \epsilon)\)</span>-RIP.</p>
<p>We will show this by applying the JL subspace embedding theorem to a collection of subspaces.</p>
<p>We’ll use the following subspace embedding theorem that we proved previously.</p>
<p><strong>Subspace Embedding Theorem</strong> Let <span class="math inline">\(\mathcal{U} \subset \mathbb{R}^n\)</span> be a <span class="math inline">\(k\)</span>-dimensional subspace. If <span class="math inline">\(\mathbf{\Pi}\)</span> is chosen from any distribution satisfying the distributional JL Lemma, then with probability <span class="math inline">\(1-\delta\)</span>, <span class="math display">\[
(1-\epsilon) \| \mathbf{y} \|_2^2
\leq \| \mathbf{\Pi} \mathbf{y} \|_2^2
\leq (1+\epsilon) \| \mathbf{y} \|_2^2
\]</span> for all <span class="math inline">\(\mathbf{y} \in \mathcal{U}\)</span> as long as <span class="math inline">\(m = O \left( \frac{k + \log(1/\delta)}{\epsilon^2} \right)\)</span>.</p>
<p>We will use union bound to apply this theorem to a collection of linear subspaces. Let <span class="math inline">\(\mathcal{S}_k = \{ \mathbf{x} \in \mathbb{R}^n : \|\mathbf{x}\|_0 \leq k \}\)</span> be the set of all <span class="math inline">\(k\)</span>-sparse vectors. We can write <span class="math inline">\(\mathcal{S}_k\)</span> as a union of disjoint <span class="math inline">\(k\)</span>-dimensional subspaces <span class="math inline">\(\mathcal{S}_k = \mathcal{U}_1 \cup \ldots \mathcal{U}_T\)</span>. The number of subspaces <span class="math inline">\(T\)</span> is the number of ways to choose <span class="math inline">\(k\)</span> coordinates from <span class="math inline">\(n\)</span> coordinates, which is <span class="math inline">\(T = \binom{n}{k}\)</span>. Consider all <span class="math inline">\(k\)</span>-sparse vectors <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. We can partition the set by which coordinates are non-zero. For one set of non-zero coordinates, we only need to preserve the basis vectors for that set of coordinates. Therefore <span class="math inline">\(T\)</span> is the number of ways to choose <span class="math inline">\(q\)</span> coordinates from <span class="math inline">\(n\)</span> coordinates, which is <span class="math inline">\(T = \binom{n}{k}\)</span>.</p>
<p align="center">
<img src="images/subspaces_matrices.png" width="200px"> <img src="images/subspaces_image.png" width="200px">
</p>
<p>We’ll apply union bound to all <span class="math inline">\(T\)</span> subspaces. In particular, we’ll set <span class="math inline">\(\delta' = \frac{\delta}{T}\)</span> and apply the subspace embedding theorem with failure probability <span class="math inline">\(\delta'\)</span>. It remains to analyze <span class="math display">\[\begin{align*}
\log(1/\delta') &amp;= \log(1/\delta) + \log(T) \\
&amp;= \log(1/\delta) + \log \left( \frac{n!}{k!(n-k)!} \right) \\
&amp;\approx \log(1/\delta) + k \log \left( \frac{n}{k} \right).
\end{align*}\]</span> We won’t prove the approximate equality but you can <a href="https://en.wikipedia.org/wiki/Binomial_coefficient#Bounds_and_asymptotic_formulas">check</a> that <span class="math display">\[\begin{align*}
\left( \frac{n}{k} \right)^k \leq \binom{n}{k} \leq \left( \frac{en}{k} \right)^k.
\end{align*}\]</span></p>
<p>Then we have the following.</p>
<p><strong>Theorem:</strong> If <span class="math inline">\(\mathbf{\Pi} \in \mathbb{R}^{m \times n}\)</span> is chosen from any distribution <span class="math inline">\(\mathcal{D}\)</span> satisfying the distributional JL lemma, then with probability <span class="math inline">\(1-\delta\)</span>, <span class="math display">\[
(1-\epsilon) \| \mathbf{x} \|_2^2 \leq \| \mathbf{\Pi x} \|_2^2 \leq (1+\epsilon) \| \mathbf{x} \|_2^2
\]</span> for all <span class="math inline">\(k\)</span>-sparse <span class="math inline">\(\mathbf{x}\)</span> as long as <span class="math inline">\(m = O\left( \frac{k\log(n) + \log(1/\delta)}{\epsilon^2} \right)\)</span>.</p>
</section>
<section id="inefficient-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="inefficient-algorithm">Inefficient Algorithm</h3>
<p>Now that we have established what type of matrices satisfy RIP and how to find them, we will discuss how to recover <span class="math inline">\(\mathbf{x}\)</span> from <span class="math inline">\(\mathbf{b} = \mathbf{Ax}\)</span>.</p>
<p>By the next theorem, there is a simple algorithm that recovers <span class="math inline">\(\mathbf{x}\)</span> from <span class="math inline">\(\mathbf{b}\)</span> <em>exactly</em>.</p>
<p><strong><span class="math inline">\(\ell_0\)</span>-Minimization Theorem:</strong> Suppose we are given <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(\mathbf{b} = \mathbf{Ax}\)</span> for an unknown <span class="math inline">\(k\)</span>-sparse <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. If <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\((2k,\epsilon)\)</span>-RIP for any <span class="math inline">\(\epsilon&lt; 1\)</span>, then <span class="math inline">\(\mathbf{x}\)</span> is the unique minimizer of <span class="math display">\[
\min_{\mathbf{z} \in \mathbb{R}^d} \| \mathbf{z} \|_0 \quad \textrm{subject to} \quad \mathbf{Az} = \mathbf{b}.
\]</span></p>
<p><strong>Proof:</strong> Consider any $ with <span class="math inline">\(\mathbf{Ay} = \mathbf{b}\)</span> and <span class="math inline">\(\| \mathbf{y} \|_0 \leq \| \mathbf{x} \|_0 \leq k\)</span>.</p>
<p>We know that <span class="math display">\[\begin{align*}
\mathbf{A}(\mathbf{y} - \mathbf{x}) =
\mathbf{Ay} - \mathbf{Ax}
= \mathbf{b} - \mathbf{b} = 0
\end{align*}\]</span> so <span class="math inline">\(\| \mathbf{A(y-x)} \|_2 = 0\)</span>. But <span class="math inline">\(\mathbf{y-x}\)</span> is <span class="math inline">\(2k\)</span>-sparse and <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\((2k,\epsilon)\)</span>-RIP so <span class="math display">\[\begin{align*}
(1-\epsilon) \| \mathbf{y-x} \|_2^2 \leq \| \mathbf{A(y-x)} \|_2^2 \leq (1+\epsilon) \| \mathbf{y-x} \|_2^2.
\end{align*}\]</span> So then <span class="math inline">\(\| \mathbf{y-x}\|_2 \leq 0\)</span> and we must have <span class="math inline">\(\mathbf{y} = \mathbf{x}\)</span>.</p>
<p>The theorem establishes that <em>information theoretically</em> we can recover <span class="math inline">\(\mathbf{x}\)</span> from <span class="math inline">\(\mathbf{b}\)</span>. However, solving the <span class="math inline">\(\ell_0\)</span>-minimization problem is computationally difficult. In fact, (one of) the best algorithms we know that solves it is to try all possible <span class="math inline">\(\binom{d}{k}\)</span> subsets of <span class="math inline">\(\mathbf{x}\)</span> and choose the one that minimizes <span class="math inline">\(\| \mathbf{Ax} - \mathbf{b} \|_2\)</span>. This algorithm takes <span class="math inline">\(O(d^k)\)</span> time which is not practical for large <span class="math inline">\(d\)</span>. We will address a faster method shortly.</p>
</section>
<section id="efficient-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="efficient-algorithm">Efficient Algorithm</h3>
<p>Solving the <span class="math inline">\(\ell_0\)</span>-minimization problem is computationally difficult. Thankfully, we can relax the problem to a convex optimization problem that is exponentially faster in <span class="math inline">\(k\)</span> and <em>gives the same solution</em>.</p>
<p><strong><span class="math inline">\(\ell_1\)</span>-Minimization Theorem:</strong> Suppose we are given <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(\mathbf{b} = \mathbf{Ax}\)</span> for an unknown <span class="math inline">\(k\)</span>-sparse <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. If <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\((2k,\epsilon)\)</span>-RIP for any <span class="math inline">\(\epsilon&lt; .17\)</span>, then <span class="math inline">\(\mathbf{x}\)</span> is the unique minimizer of <span class="math display">\[
\min_{\mathbf{z} \in \mathbb{R}^d} \| \mathbf{z} \|_1 \quad \textrm{subject to} \quad \mathbf{Az} = \mathbf{b}.
\]</span></p>
<p>Before we prove the theorem, let’s get some intuition for why it holds. The <span class="math inline">\(\ell_1\)</span>-norm is the sum of the absolute values of the entries of <span class="math inline">\(\mathbf{z}\)</span>. As depicted on the left figure below, the sets with equal <span class="math inline">\(\ell_1\)</span>-norm form a diamond. At the same time, the constraint that <span class="math inline">\(\mathbf{Az} = \mathbf{b}\)</span> is a hyperplane that is random (since <span class="math inline">\(\mathbf{A}\)</span> is random). Then, with high probability, the intersection of the constraint and the objective will be a vertex of the diamond which corresponds to a <em>sparse</em> solution.</p>
<p align="center">
<img src="images/l1opt.png" width="200px"> <img src="images/l2opt.png" width="200px">
</p>
<p>The same intuition does not hold for the <span class="math inline">\(\ell_2\)</span>-norm. The sets with equal <span class="math inline">\(\ell_2\)</span>-norm form a sphere. So the intersection of the constraint and the objective will be a random point on the circle which corresponds to a <em>dense</em> solution.</p>
<p>We will next prove the <span class="math inline">\(\ell_1\)</span>-minimization theorem. There are two tools that we’ll use.</p>
<p><strong>Tool 1:</strong> For any vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, <span class="math display">\[\begin{align*}
\|\mathbf{a+b} \| \geq \| \mathbf{a} \| - \| \mathbf{b} \|.
\end{align*}\]</span> We can see this from the triangle inequality that <span class="math inline">\(\| \mathbf{x} + \mathbf{y} \| \leq \| \mathbf{x} \| + \| \mathbf{y} \|\)</span>. Plugging in <span class="math inline">\(\mathbf{x} = \mathbf{a} - \mathbf{b}\)</span> and <span class="math inline">\(\mathbf{y} = \mathbf{b}\)</span> then rearranging gives the result. Notice that this works for any norm.</p>
<p><strong>Tool 2:</strong> For any vector <span class="math inline">\(k\)</span>-sparse vector <span class="math inline">\(\mathbf{w}\)</span>, we have <span class="math display">\[\begin{align*}
\|\mathbf{w}\|_2 \leq \| \mathbf{w} \|_1 \leq \sqrt{k} \| \mathbf{w} \|_2.
\end{align*}\]</span> The first inequality follows from the definition of the <span class="math inline">\(\ell_2\)</span>-norm and the second inequality follows from the Cauchy-Schwarz inequality. In particular, <span class="math display">\[\begin{align*}
\| \mathbf{w} \|_2^2 = \sum_{i=1}^d w_i^2 \leq \sum_{i=1}^d w_i^2 \sum_{i=1}^d \sum{j=1}^d |w_i| |w_j| = \sum_{i=1}^d |w_i| \sum_{j=1}^d |w_j| = \| \mathbf{w} \|_1^2
\end{align*}\]</span> and <span class="math display">\[\begin{align*}
\| \mathbf{w} \|_1^2 = \sum_{i=1}^d |w_i|^2 \mathbb{1}[w_i \neq 0] \leq \left( \sum_{i=1}^d |w_i|^2 \right) \left( \sum_{j: w_j \neq 0}1 \right) = k \| \mathbf{w} \|_2^2
\end{align*}\]</span> where the inequality follows by Cauchy-Schwarz.</p>
<p>With these tools, we are now ready to prove the <span class="math inline">\(\ell_1\)</span>-minimization theorem.</p>
<p><strong>Proof:</strong> By way of contradiction, we will assume that <span class="math inline">\(\mathbf{x}\)</span> is not the optimal solution. Then there exists some non-zero <span class="math inline">\(\mathbf{\Delta}\)</span> such that <span class="math inline">\(\| \mathbf{x} + \mathbf{\Delta} \|_1 \leq \| \mathbf{x} \|_1\)</span> and <span class="math inline">\(\mathbf{A}(\mathbf{x} + \mathbf{\Delta}) = \mathbf{b}\)</span>. The challenge is that we can not assume that <span class="math inline">\(\mathbf{\Delta}\)</span> is sparse. We will argue that <span class="math inline">\(\mathbf{\Delta}\)</span> is <em>approximately</em> sparse.</p>
<p align="center">
<img src="images/indexingForProof.png" width="200px">
</p>
<p>Let <span class="math inline">\(S\)</span> be the set of <span class="math inline">\(k\)</span> non-zero indices in <span class="math inline">\(\mathbf{x}\)</span>. Let <span class="math inline">\(T_1\)</span> be the set of <span class="math inline">\(2k\)</span> indices not in <span class="math inline">\(S\)</span> with the largest absolute values in <span class="math inline">\(\mathbf{\Delta}\)</span>. Let <span class="math inline">\(T_2\)</span> be the set of <span class="math inline">\(2k\)</span> indices not in <span class="math inline">\(S \cup T_1\)</span> with the next largest magnitudes, et cetera.</p>
<p>We will first show that <span class="math inline">\(\mathbf{\Delta}\)</span> is approximately sparse in the <span class="math inline">\(\ell_1\)</span>-norm. To see this, we can write</p>
<p><span class="math display">\[\begin{align*}
\| \mathbf{x} \|_1 &amp;\geq \| \mathbf{x} + \mathbf{\Delta} \|_1  \\
&amp;= \| \mathbf{x}_S + \mathbf{\Delta}_S \|_1 + \| \mathbf{\Delta}_{\bar{S}} \|_1 \\
&amp;\geq \| \mathbf{x}_S \|_1 - \| \mathbf{\Delta}_S \|_1 + \| \mathbf{\Delta}_{\bar{S}} \|_1
\end{align*}\]</span> where the first inequality follows from assumption, the first equality follows since <span class="math inline">\(\mathbf{x}\)</span> only has non-zero entries on <span class="math inline">\(S\)</span>, and the second inequality follows from Tool 1. Rearranging, we have that <span class="math inline">\(\|\mathbf{\Delta}_S\|_1 \geq \| \mathbf{\Delta}_{\bar{S}} \|_1\)</span>.</p>
<p>Next, we will show that <span class="math inline">\(\mathbf{\Delta}\)</span> is approximately sparse in the <span class="math inline">\(\ell_2\)</span>-norm. To see this, we can write <span class="math display">\[\begin{align*}
\| \mathbf{\Delta}_S\|_2
\geq \frac1{\sqrt{k}} \| \mathbf{\Delta}_S \|_1
\geq \frac1{\sqrt{k}} \| \mathbf{\Delta}_{\bar{S}} \|_1
= \frac1{\sqrt{k}} \sum_{j\geq 1} \| \mathbf{\Delta}_{T_j} \|_1
\end{align*}\]</span> where the first inequality follows from Tool 2 and the second inequality follows from the previous <span class="math inline">\(\ell_1\)</span>-norm sparsity result. Since there are <span class="math inline">\(2k\)</span> indices in <span class="math inline">\(T_j\)</span>, we know that $ | _{T_j} |<em>1 2k (</em>{T_j})$. Simultaneously, we know that <span class="math inline">\(\| \mathbf{\Delta}_{T_{j+1}} \|_2^2 \leq 2k \max(\mathbf{\Delta}_{T_{j+1}})^2\)</span> so <span class="math inline">\(\| \mathbf{\Delta}_{T_{j+1}} \|_2 \leq \sqrt{2k} \max(\mathbf{\Delta}_{T_{j+1}})\)</span>. Combining the last two inequalities, we have that <span class="math display">\[\begin{align*}
\sqrt{2k} \| \mathbf{\Delta}_{T_{j+1}} \|_2
\leq 2k \max(\mathbf{\Delta}_{T_{j+1}})
\leq \| \mathbf{\Delta}_{T_j} \|_1.
\end{align*}\]</span> Plugging this inequality back in, we know that <span class="math display">\[\begin{align*}
\| \mathbf{\Delta}_S \|_2 \geq \sqrt{2} \sum_{j \geq 2} \| \mathbf{\Delta}_{T_j} \|_2.
\end{align*}\]</span></p>
<p>Finally, we will show the contradiction. We know $( + ) = and <span class="math inline">\(\mathbf{A} \mathbf{x} = \mathbf{b}\)</span> so <span class="math inline">\(\mathbf{A} \mathbf{\Delta} = 0\)</span>. By the <span class="math inline">\((3k, \epsilon)\)</span>-RIP, we know that <span class="math display">\[\begin{align*}
0 &amp;= \| \mathbf{A \Delta} \|_2
\\&amp;= \| \mathbf{A \Delta}_{S \cup T_1} \|_2
- \sum_{j \geq 2} \| \mathbf{A \Delta}_{\bar{S}} \|_2
\\&amp;\geq (1-\epsilon) \| \mathbf{\Delta}_{S \cup T_1} \|_2
- (1+\epsilon) \sum_{j \geq 2} \| \mathbf{\Delta}_{T_j} \|_2
\\&amp;\geq (1-\epsilon) \| \mathbf{\Delta}_{S} \|_2
- \frac{(1+\epsilon)}{\sqrt{2}} \| \mathbf{\Delta}_{S} \|_2
\\&amp;= \left( (1-\epsilon) - \frac{(1+\epsilon)}{\sqrt{2}} \right) \| \mathbf{\Delta}_{S} \|_2
\end{align*}\]</span> But this is a contradiction since <span class="math inline">\(\left( (1-\epsilon) - \frac{(1+\epsilon)}{\sqrt{2}} \right) &gt; 0\)</span> for <span class="math inline">\(\epsilon &lt; .17\)</span>.</p>
<p>So we know that a solution to the <span class="math inline">\(\ell_1\)</span>-minimization problem recovers <span class="math inline">\(\mathbf{x}\)</span>. We can solve the <span class="math inline">\(\ell_1\)</span>-minimization problem in polynomial time using linear programming or, alternatively, running gradient descent with an <span class="math inline">\(\ell_1\)</span>-regularization term.</p>
<!--

There are robust versions of the $\ell_0$-minimization theorem.
These are more important in practice because they are more stable to noise.
Here's a flavor of the results:
Suppose $\mathbf{b} = \mathbf{A}(\mathbf{x} + \mathbf{e})$ where $\mathbf{x}$ is $k$-sparse $\mathbf{e}$ is dense but has bounded norm.
Then we can recover some $k$-sparse $\tilde{\mathbf{x}}$ such that $$\| \mathbf{x} - \tilde{\mathbf{x}} \|_2 \leq \|\mathbf{e}\|_2.$$

We will not discuss robustness in detail.
But, along with computational considerations, it is a big part of what has made compressed sensing such an active research area in the last two decades.
There are non-robust compressed sensing results that have been known for a long time.
For example, compressed sensing was discussed in the 1795 paper by Gaspard Riche de Prony titled *Essai experimental et analytique: sur les lois de la dilatabilite des fluides elastique et sur celles de la force expansive de la vapeur de l'eau et de la vapeur de l'alkool, a differentes temperatures.*

In many applications, we can compute measures of the form $\mathbf{Ax} = \mathbf{SFx}$ where $\mathbf{F}$ is the DFT matrix and $\mathbf{S}$ is a subsampling matrix.
Recall that $\mathbf{F}$ decomposes $\mathbf{x}$ into different frequencies which tends to be sparse depending on the application.
Then $\mathbf{Ax}$ is a subset of rows from $\mathbf{F}$ then $\mathbf{Ax}$ is a subset of random frequency components from the DFT of $\mathbf{x}$.
In many scientific applications, we can collect entries of $\mathbf{Fx}$ one at a time for some unobserved data vector $\mathbf{x}$.

Setting $\mathbf{A}$ to contain $m=\left( \frac{k \log^2 k \log n}{\epsilon^2} \right)$ random rows of a discrete Fourier matrix $\mathbf{F}$ yields a matrix that satisfies $(k, \epsilon)$-RIP with high probability.
In spirit, the proof requires similar tools to analyze subsampled Hadamard transforms!
Many brilliant mathematicians have worked on this problem including Guruswami, Bourgain, Veryshinin, and Tao.

Amazingly, we can actually prove a version of this result with only a slightly worse bound.
We'll start with the observation
that the $k$-sparse vectors $\mathbf{x}$ that can be written as $\mathbf{Ax}$ lie in a union of $k$-dimensional subspaces.

-->



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
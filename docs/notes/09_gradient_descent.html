<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Gradient Descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c5a5d5e27fcc88644031c24cff017230.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../favicon.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Spring 2026</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://discord.gg/dES3fSPEeC"> 
<span class="menu-text">Discord</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/1091652"> 
<span class="menu-text">Gradescope</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link active" data-scroll-target="#gradient-descent">Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#calculus-review" id="toc-calculus-review" class="nav-link" data-scroll-target="#calculus-review">Calculus Review</a></li>
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm">Algorithm</a></li>
  <li><a href="#iteration-complexity" id="toc-iteration-complexity" class="nav-link" data-scroll-target="#iteration-complexity">Iteration Complexity</a></li>
  </ul></li>
  <li><a href="#projected-gradient-descent" id="toc-projected-gradient-descent" class="nav-link" data-scroll-target="#projected-gradient-descent">Projected Gradient Descent</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Gradient Descent</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h2>
<p>We will consider the general following problem: Given a function <span class="math inline">\(f: \mathbb{R}^d \to \mathbb{R}\)</span>, we want to find a vector <span class="math inline">\(\hat{\mathbf{x}} \in \mathbb{R}^d\)</span> that approximately minimizes <span class="math inline">\(f\)</span>. That is, we want <span class="math inline">\(\hat{\mathbf{x}}\)</span> such that <span class="math display">\[\begin{align*}
f(\hat{\mathbf{x}}) \leq \min_{\mathbf{x}} f(\mathbf{x}) + \epsilon
\end{align*}\]</span> for some small constant <span class="math inline">\(\epsilon\)</span>.</p>
<p>The problem is at the core of machine learning. Typically, the function <span class="math inline">\(f\)</span> is a loss function that measures how well a model fits the data. The goal is to find parameters <span class="math inline">\(\hat{\mathbf{x}}\)</span> that achieve small loss.</p>
<p><strong>Least Squares Regression Example</strong></p>
<p>As an example, we’ll consider least squares regression where we fit the data with a linear model and the loss function is the <span class="math inline">\(\ell_2\)</span>-norm. Consider <span class="math inline">\(n\)</span> data points <span class="math inline">\(\mathbf{a}^{(1)}, \ldots, \mathbf{a}^{(n)} \in \mathbb{R}^d\)</span> with corresponding labels <span class="math inline">\(y^{(1)}, \ldots, y^{(n)} \in \mathbb{R}\)</span>. We will consider the linear model <span class="math inline">\(\mathbf{x}^\top \mathbf{a}\)</span> where <span class="math inline">\(\mathbf{x}\)</span> is the parameter vector. The loss function is given by <span class="math display">\[\begin{align*}
f(\mathbf{x}) = \sum_{i=1}^n (\mathbf{x}^\top \mathbf{a}^{(i)} - y^{(i)})^2
= \| \mathbf{A x} - \mathbf{y} \|_2^2.
\end{align*}\]</span> In the last expression, we used compact linear algebra notation where the data matrix <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{n \times d}\)</span> has rows <span class="math inline">\(\mathbf{a}^{(i)}\)</span> and the label vector <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> has entries <span class="math inline">\(y^{(i)}\)</span>.</p>
<p>Determining how to find the best parameters <span class="math inline">\(\mathbf{x}\)</span> is a fundamental problem in machine learning. The choice of algorithm depends on the form of the function <span class="math inline">\(f\)</span> (e.g.&nbsp;linear, quadratic, finite sum structure) and additional constraints on the parameters.</p>
<p>Gradient descent is a greedy algorithm for minimizing a function of multiple variables that often works amazingly well in practice. The algorithm iteratively updates the parameters <span class="math inline">\(\mathbf{x}\)</span> by moving in the direction of steepest descent. In this way, the algorithm hopefully decreases the function at each iteration.</p>
<p align="center">
<img src="images/gradient_descent.png" width="200px">
</p>
<p>Before we analyze gradient descent, we’ll review some calculus.</p>
<section id="calculus-review" class="level3">
<h3 class="anchored" data-anchor-id="calculus-review">Calculus Review</h3>
<p>For <span class="math inline">\(i \in \{1, \ldots, d\} = [d]\)</span>, let <span class="math inline">\(x_i\)</span> be the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(\mathbf{x}\)</span>. We will use <span class="math inline">\(\mathbf{e}^{(i)}\)</span> to denote the <span class="math inline">\(i\)</span>th standard basis vector with 0s everywhere except for a 1 in the <span class="math inline">\(i\)</span>th entry.</p>
<p>Recall the partial derivative of a function <span class="math inline">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> with respect to the <span class="math inline">\(i\)</span>th variable is given by</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial x_i}
(\mathbf{x}) = \lim_{t \to 0} \frac{f(\mathbf{x} + t \mathbf{e}^{(i)}) - f(\mathbf{x})}{t}.
\end{align*}\]</span></p>
<p>The directional derivative of <span class="math inline">\(f\)</span> in the direction <span class="math inline">\(\mathbf{v} \in \mathbb{R}^d\)</span> is given by <span class="math display">\[\begin{align*}
D_{\mathbf{v}} f(\mathbf{x}) =
\lim_{t \to 0} \frac{f(\mathbf{x} + t \mathbf{v}) - f(\mathbf{x})}{t}.
\end{align*}\]</span></p>
<p>In this notation, the partial derivative is the directional derivative in the direction of the <span class="math inline">\(i\)</span>th standard basis vector. That is, <span class="math inline">\(\frac{\partial f}{\partial x_i}(\mathbf{x}) = D_{\mathbf{e}^{(i)}} f(\mathbf{x})\)</span>.</p>
<p>We are interested not just in the partial derivative of the function with respect to one variable but the <em>gradient</em> of the function with respect to all variables.</p>
<p>The gradient of <span class="math inline">\(f\)</span> is the vector of partial derivatives <span class="math display">\[\begin{align*}
\nabla f(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f}{\partial x_1}(\mathbf{x}) \\
\vdots \\
\frac{\partial f}{\partial x_d}(\mathbf{x})
\end{bmatrix}.
\end{align*}\]</span></p>
<p>With this notation, we can write the directional derivative as <span class="math display">\[\begin{align*}
D_\mathbf{v} f(\mathbf{x}) = \nabla f(\mathbf{x})^\top \mathbf{v}.
\end{align*}\]</span> While not immediately obvious, the proof is straightforward and can be found <a href="https://math.byu.edu/~bakker/M314F12/M314LectureNotes/M314Lec16.pdf">here</a>.</p>
<p>When we solve the problem of minimizing the function <span class="math inline">\(f\)</span>, we need a way of accessing <span class="math inline">\(f\)</span>. We generally have a <em>function oracle</em> which evaluates <span class="math inline">\(f(\mathbf{x})\)</span> for any <span class="math inline">\(\mathbf{x}\)</span> and a <em>gradient oracle</em> which evaluates <span class="math inline">\(\nabla f(\mathbf{x})\)</span> for any <span class="math inline">\(\mathbf{x}\)</span>. We say access to the function oracle gives zeroth-order information and access to the gradient oracle gives first-order access. We view the implementation of these oracles as a black box. However, computing the function value and gradient can be computationally expensive.</p>
<p><em>Computational Cost for Least Squares Regression</em> Let’s consider the computational cost of evaluating the oracles for the least squares regression problem. When <span class="math inline">\(f(\mathbf{x}) = \| \mathbf{A x} - \mathbf{y} \|_2^2\)</span>, the function oracle requires computing a matrix-vector product and a vector-vector addition. Since <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{n \times d}\)</span> and <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>, the matrix-vector product costs <span class="math inline">\(O(nd)\)</span> operations. Since <span class="math inline">\(\mathbf{A x} \in \mathbb{R}^n\)</span> and <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>, the vector-vector addition costs <span class="math inline">\(O(n)\)</span> operations. So the total cost of the function evaluation is <span class="math inline">\(O(nd)\)</span>.</p>
<p>In order to compute the cost of the gradient, let’s compute the gradient for the least squares regression problem.</p>
<p>Recall we can write</p>
<p><span class="math display">\[\begin{align*}
f(\mathbf{x}) \sum_{i=1}^n (\mathbf{x}^\top \mathbf{a}^{(i)} - y^{(i)})^2
\end{align*}\]</span></p>
<p>so, by the chain rule, the partial derivative is given by</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial x_j} (\mathbf{x})
&amp;= \sum_{i=1}^n 2 (\mathbf{x}^\top \mathbf{a}^{(i)} - y^{(i)}) \frac{\partial}{\partial x_j} (\mathbf{x}^\top \mathbf{a}^{(i)} - y^{(i)}) \\
&amp;= \sum_{i=1}^n 2 a_j^{(i)} (\mathbf{x}^\top \mathbf{a}^{(i)} - y^{(i)}) \\
&amp;= 2 \mathbf{A}_j^\top (\mathbf{A x} - \mathbf{y})
\end{align*}\]</span> where <span class="math inline">\(\mathbf{A}_j\)</span> is the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Then the gradient is given by <span class="math inline">\(\nabla f(\mathbf{x}) = 2 \mathbf{A}^\top (\mathbf{A x} - \mathbf{y})\)</span>. We can compute <span class="math inline">\(\mathbf{A x} - \mathbf{y} \in \mathbb{R}^n\)</span> with <span class="math inline">\(O(nd)\)</span> operations. Then, to get the full gradient, we compute <span class="math inline">\(\mathbf{A}^\top (\mathbf{A x} - \mathbf{y}) \in \mathbb{R}^d\)</span> with <span class="math inline">\(O(n^2 d)\)</span> operations since <span class="math inline">\(\mathbf{A}^\top \in \mathbb{R}^{d \times n}\)</span>. So the total cost of the gradient evaluation is <span class="math inline">\(O(n^2 d)\)</span>.</p>
</section>
<section id="algorithm" class="level3">
<h3 class="anchored" data-anchor-id="algorithm">Algorithm</h3>
<p>Let’s consider the general gradient descent algorithm. The key idea is to iteratively update the parameters <span class="math inline">\(\mathbf{x}\)</span> by making a small adjustment that decreases <span class="math inline">\(f(\mathbf{x})\)</span>. In particular, we will update <span class="math inline">\(\mathbf{x} \gets \mathbf{x} + \eta \mathbf{v}\)</span> where <span class="math inline">\(\eta &gt; 0\)</span> is the <em>step size</em>.</p>
<p>In order to determine how we should set <span class="math inline">\(\mathbf{v}\)</span>, we will consider the case when <span class="math inline">\(\eta\)</span> is small. In this case, we can approximate <span class="math display">\[\begin{align*}
f(\mathbf{x} + \eta \mathbf{v})
- f(\mathbf{x})
\approx D_\mathbf{v} f(\mathbf{x}) \eta
= \nabla f(\mathbf{x})^\top \mathbf{v} \eta.
\end{align*}\]</span> In order to decrease <span class="math inline">\(f(\mathbf{x})\)</span>, we want to choose <span class="math inline">\(\mathbf{v}\)</span> such that <span class="math inline">\(\nabla f(\mathbf{x})^\top \mathbf{v}\)</span> is as negative as possible. Recall the inner product is maximized when the vectors are parallel so we set <span class="math inline">\(\mathbf{v} = -\nabla f(\mathbf{x})\)</span>. Then <span class="math display">\[\begin{align*}
f(\mathbf{x} + \eta \mathbf{v})
- f(\mathbf{x})
\approx -\nabla f(\mathbf{x})^\top \nabla f(\mathbf{x}) \eta = -\| \nabla f(\mathbf{x}) \|_2^2 \eta.
\end{align*}\]</span></p>
<p>Let’s formalize the gradient descent algorithm. We will initialize <span class="math inline">\(\mathbf{x}^{(0)} \in \mathbb{R}^d\)</span> to some initial point. For each iteration <span class="math inline">\(t \in [T]\)</span>, we will compute the gradient <span class="math inline">\(\nabla f(\mathbf{x}^{(t)})\)</span>. Then we will update the parameters by setting <span class="math inline">\(\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \eta \nabla f(\mathbf{x}^{(t)})\)</span>. Finally, we return the best parameters <span class="math inline">\(\hat{\mathbf{x}} = \arg \min_{t \in [T]} \mathbf{x}^{(t)}\)</span>.</p>
<p>Notice we need to choose the step size parameter <span class="math inline">\(\eta\)</span> and the number of iterations <span class="math inline">\(T\)</span>.</p>
<p align="center">
<img src="images/gradient_descent_2.png" width="300px">
</p>
<p>Today, we’ll show that if <span class="math inline">\(f\)</span> is a convex function, then gradient descent converges to a near global minimum when the step size <span class="math inline">\(\eta\)</span> is small and the number of iterations <span class="math inline">\(T\)</span> is large. Formally, <span class="math display">\[\begin{align*}
f(\hat{\mathbf{x}}) \leq f(\mathbf{x}^*) + \epsilon
\end{align*}\]</span> where <span class="math inline">\(\mathbf{x}^*\)</span> is a global minimum of <span class="math inline">\(f\)</span>. Examples of convex functions include least squares reduction, logistic regression, kernel regression, and support vector machines.</p>
<p>Gradient descent also works for non-convex function but the convergence guarantees are not as strong. In particular, gradient descent will converge to a near stationary point. Formally, <span class="math display">\[\begin{align*}
\| \nabla f(\mathbf{x}^{(T)}) \|_2 \leq \epsilon.
\end{align*}\]</span> Examples of non-convex functions include neural networks, matrix completion, and mixture models.</p>
</section>
<section id="iteration-complexity" class="level3">
<h3 class="anchored" data-anchor-id="iteration-complexity">Iteration Complexity</h3>
<p>For machine learning applications, we are primarily interested in how fast gradient descent converges. We will be able to bound the iteration complexity if we make some assumptions on the function <span class="math inline">\(f\)</span>. As usual, the stronger the assumptions we can make, the better bounds we can get.</p>
<p>We will start by assuming that <span class="math inline">\(f\)</span> is convex. A function <span class="math inline">\(f\)</span> is convex if and only if for all <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)</span> and <span class="math inline">\(\lambda \in [0, 1]\)</span>, we have <span class="math display">\[\begin{align*}
(1-\lambda) f(\mathbf{x}) + \lambda f(\mathbf{y}) \geq f((1-\lambda) \mathbf{x} + \lambda \mathbf{y}).
\end{align*}\]</span></p>
<p>In words, every line between two points on the function is above the function.</p>
<p align="center">
<img src="images/convex_function.png" width="300px">
</p>
<p>Equivalently, the tangent line at any point is below the function. In mathematical notation, we can express the tangent line constraint as <span class="math display">\[\begin{align*}
f(\mathbf{x + z}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \mathbf{z}
\end{align*}\]</span> for all <span class="math inline">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\)</span>. By setting <span class="math inline">\(\mathbf{z} = \mathbf{y} - \mathbf{x}\)</span>, we can rewrite the constraint as <span class="math display">\[\begin{align*}
f(\mathbf{x} + \mathbf{y} - \mathbf{x})
&amp;\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) \\
&amp;=f(\mathbf{x}) - \nabla f(\mathbf{x})^\top (\mathbf{x} - \mathbf{y}).
\end{align*}\]</span> Rearranging, we get <span class="math display">\[\begin{align*}
f(\mathbf{x}) - f(\mathbf{y}) \leq \nabla f(\mathbf{x})^\top (\mathbf{x} - \mathbf{y}).
\end{align*}\]</span> We will use this inequality in the analysis of the convergence rate of gradient descent.</p>
<p>Another assumption we will make on the function <span class="math inline">\(f\)</span> is that <span class="math inline">\(G\)</span>-Lipschitz. Formally, <span class="math inline">\(f\)</span> is <span class="math inline">\(G\)</span>-Lipschitz if, for all <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\| \nabla f(\mathbf{x}) \|_2 \leq G\)</span>. This means that the function <span class="math inline">\(f\)</span> cannot change too quickly.</p>
<p>For our analysis of gradient descent, we will assume that <span class="math inline">\(f\)</span> is convex, <span class="math inline">\(f\)</span> is <span class="math inline">\(G\)</span>-Lipschitz, and our starting point <span class="math inline">\(\mathbf{x}^{(0)}\)</span> is within <span class="math inline">\(R\)</span> of the global minimum <span class="math inline">\(\mathbf{x}^*\)</span>.</p>
<p><strong>Gradient Descent Convergence Bound:</strong> If we run gradient with step size <span class="math inline">\(eta =\frac{R}{G \sqrt{T}}\)</span> for <span class="math inline">\(T \geq \frac{R^2 G^2}{\epsilon^2}\)</span> iterations, then <span class="math inline">\(f(\hat{\mathbf{x}}) \leq f(\mathbf{x}^*) + \epsilon\)</span>.</p>
<p>Intuitively, the proof is tricky because the iterates do not necessarily decrease monotonically. We will prove that the <em>average</em> iterate gives a good solution and, therefore, the best iterate must also give a good solution.</p>
<p>We will first prove an intermediate result that if the function value is far from the global minimum, then we take a large step towards the global minimum. This property is intuitively useful because it means that we make good progress towards the global minimum even when we are far way.</p>
<p>Consider the difference between the next iterate and the global minimum <span class="math display">\[\begin{align*}
\| \mathbf{x}^{(t+1)} - \mathbf{x}^* \|_2^2
&amp;= \| \mathbf{x}^{(t)} - \eta \nabla f(\mathbf{x}^{(t)}) - \mathbf{x}^* \|_2^2 \\
&amp;= \| (\mathbf{x}^{(t)} - \mathbf{x}^*) - \eta \nabla f(\mathbf{x}^{(t)}) \|_2^2 \\
&amp;= \| \mathbf{x}^{(t)} - \mathbf{x}^* \|_2^2 - 2 \eta \nabla f(\mathbf{x}^{(t)})^\top (\mathbf{x}^{(t)} - \mathbf{x}^*) + \eta^2 \| \nabla f(\mathbf{x}^{(t)}) \|_2^2 \\
&amp;\leq \| \mathbf{x}^{(t)} - \mathbf{x}^* \|_2^2 - 2 \eta \nabla f(\mathbf{x}^{(t)})^\top (\mathbf{x}^{(t)} - \mathbf{x}^*) + \eta^2 G^2
\end{align*}\]</span> where the third equality followed because the squared <span class="math inline">\(\ell_2\)</span> norm is an inner product and the inequality followed by the <span class="math inline">\(G\)</span>-Lipschitz assumption. Dividing by <span class="math inline">\(2 \eta\)</span> and rearranging, we get <span class="math display">\[\begin{align*}
\nabla f(\mathbf{x}^{(t)})^\top (\mathbf{x}^{(t)} - \mathbf{x}^*)
\leq \frac{\| \mathbf{x}^{(t)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(t+1)} - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2}.
\end{align*}\]</span></p>
<p>By convexity, it follows that <span class="math display">\[\begin{align*}
f(\mathbf{x}^{(t)}) - f(\mathbf{x}^*)
\leq \frac{\| \mathbf{x}^{(t)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(t+1)} - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2}.
\end{align*}\]</span></p>
<p>The next step is to consider the average difference between the iterates and the global minimum <span class="math display">\[\begin{align*}
\sum_{t=0}^{T-1} f(\mathbf{x}^{(t)} - f(\mathbf{x}^*)
&amp;\leq \frac{\| \mathbf{x}^{(0)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(1)} - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2} \\
&amp;+\frac{\| \mathbf{x}^{(1)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(2)} - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2} \\
&amp;+ \ldots + \\
&amp;+ \frac{\| \mathbf{x}^{(T-1)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(T)} - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2}.
\end{align*}\]</span> The expression is a telescoping sum because some terms cancel out. Then we can bound the <em>average</em> as <span class="math display">\[\begin{align*}
\frac1{T} \sum_{t=0}^{T-1} f(\mathbf{x}^{(t)} - f(\mathbf{x}^*)
&amp;\leq \frac{\| \mathbf{x}^{(0)} - \mathbf{x}^* \|_2^2  - \| \mathbf{x}^{(T)} - \mathbf{x}^* \|_2^2}{2 T \eta} + \frac{\eta G^2}{2} \\
&amp;\leq \frac{R^2 - 0}{2 T \eta} + \frac{\eta G^2}{2} \\
&amp;= \frac{R^2 G \sqrt{T} }{2 T R} + \frac{R G^2}{2 G \sqrt{T}} = \frac{R G}{\sqrt{T}} \epsilon^2.
\end{align*}\]</span> The second inequality used that <span class="math inline">\(\mathbf{x}^{(0)}\)</span> is within <span class="math inline">\(R\)</span> of the global minimum <span class="math inline">\(\mathbf{x}^*\)</span> and that <span class="math inline">\(\| \cdot \|_2^2\)</span> is non-negative. The first equality followed by plugging in the step size <span class="math inline">\(\eta = \frac{R}{G \sqrt{T}}\)</span>. The second equality followed by plugging in the number of iterations <span class="math inline">\(T= \frac{R^2 G^2}{\epsilon^2}\)</span>.</p>
<p>For the final step, we will consider the average function values of the iterates. Then we can bound the difference between the average iterate and the global minimum as <span class="math display">\[\begin{align*}
\frac1{T} \sum_{t=0}^{T-1} f(\mathbf{x}^{(t)})
- f(\mathbf{x}^*)
\leq \epsilon.
\end{align*}\]</span> It must be that the best iterate gives a solution that is at least as good as the average. Then <span class="math display">\[\begin{align*}
f(\hat{\mathbf{x}}) \leq \frac1{T} \sum_{t=0}^{T-1} f(\mathbf{x}^{(t)}) \leq f(\mathbf{x}^*) + \epsilon
\end{align*}\]</span> as desired.</p>
<p>It may appear that the bound is not particularly useful because we need to know the right learning rate. In practice, we can run gradient descent several times with different learning rates and choose the best parameters.</p>
</section>
</section>
<section id="projected-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="projected-gradient-descent">Projected Gradient Descent</h2>
<p>We so far assumed that the parameters <span class="math inline">\(\mathbf{x}\)</span> are unconstrained. However, in many applications, we have additional constraints on the parameters. We may have:</p>
<ul>
<li><p>A regularization constraint that <span class="math inline">\(\| \mathbf{x} \|_2\)</span> is small. This is common in machine learning applications where we want to avoid overfitting.</p></li>
<li><p>A positivity constraint that <span class="math inline">\(\mathbf{x} \geq 0\)</span>. This is common in applications where the parameters correspond to real world quantities that cannot be negative.</p></li>
<li><p>A linear constraint that <span class="math inline">\(\mathbf{M x} \leq \mathbf{b}\)</span>. This is common in training support vector machines, industrial optimization, and subroutines in integer programming.</p></li>
</ul>
<p>Today, we will consider the case where the parameters <span class="math inline">\(\mathbf{x}\)</span> are constrained to be inside a convex set <span class="math inline">\(\mathcal{S}\)</span>. Notice we are now using the term convex in two different ways: a function is convex if it satisfies the tangent line constraint and a set is convex if the line between any two points in the set is in the set.</p>
<p>We can check visually that the set <span class="math inline">\(\mathcal{S}\)</span> is convex by drawing a line between any two points in the set and verifying that the line is in the set.</p>
<p align="center">
<img src="images/convex_set.png" width="300px">
</p>
<p>For example, the set on the left is convex but the set on the right is not convex.</p>
<p>Formally, a set <span class="math inline">\(\mathcal{S}\)</span> is convex if for any <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathcal{S}\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>, we have <span class="math display">\[\begin{align*}
(1- \lambda) \mathbf{x} + \lambda \mathbf{y}
\in \mathcal{S}.
\end{align*}\]</span></p>
<p>The challenge for gradient descent is that even if we start with an initial point <span class="math inline">\(\mathbf{x}^{(0)} \in \mathcal{S}\)</span>, there is no guarantee that the next iterate <span class="math inline">\(\mathbf{x}^{(0)} - \eta \nabla f(\mathbf{x}^{(0)})\)</span> will be in <span class="math inline">\(\mathcal{S}\)</span>.</p>
<p>We will consider an extremely simple modification: We force the next iterate to be in <span class="math inline">\(\mathcal{S}\)</span> by projecting it onto <span class="math inline">\(\mathcal{S}\)</span>. In order to do this, we will need a third oracle for projections that returns <span class="math display">\[\begin{align*}
P_{\mathcal{S}} (\mathbf{x})
= \arg \min_{\mathbf{y} \in \mathcal{S}}
\| \mathbf{x} - \mathbf{y} \|_2^2.
\end{align*}\]</span></p>
<p>Projected gradient descent is a simple modification of gradient descent. Instead of setting the next iterate to <span class="math display">\[\begin{align*}
x^{(t+1)} = x^{(t)} - \eta \nabla f(x^{(t)}),,,,
\end{align*}\]</span> we set the next iterate to <span class="math display">\[\begin{align*}
x^{(t+1)} = P_{\mathcal{S}} (x^{(t)} - \eta \nabla f(x^{(t)})).
\end{align*}\]</span></p>
<p>The analysis for projected gradient descent is almost identical to the analysis for gradient descent. We just need one additional claim that, for any <span class="math inline">\(\mathbf{y} \in \mathcal{S}\)</span>, <span class="math display">\[\begin{align*}
\| \mathbf{y} - P_{\mathcal{S}} (\mathbf{x}) \|_2^2
\leq \| \mathbf{y} - \mathbf{x} \|_2^2.
\end{align*}\]</span></p>
<p>We then argue that <span class="math display">\[\begin{align*}
f(\mathbf{x}^{(t)}) - f(\mathbf{x}^*)
&amp;\leq
\frac{\| \mathbf{x}^{(t)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(t)} - \eta \nabla f(\mathbf{x}^{(t)}) - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2} \\
&amp;\leq \frac{\| \mathbf{x}^{(t)} - \mathbf{x}^* \|_2^2 - \| \mathbf{x}^{(t+1)} - \mathbf{x}^* \|_2^2}{2 \eta}  + \frac{\eta G^2}{2}.
\end{align*}\]</span> The rest of the analysis follows as before.</p>
<p>The bounds we showed for gradient descent and projected gradient descent have a <span class="math inline">\(1/\epsilon^2\)</span> dependence. Since we care about the approximation, we would ideally likely a <span class="math inline">\(1/\epsilon\)</span> or even <span class="math inline">\(\log(1/\epsilon)\)</span> dependence. Similarly, we would like to reduce or eliminate the dependence on the Lipschitz constant <span class="math inline">\(G\)</span> and the radius <span class="math inline">\(R\)</span>. Unfortunately, in order to improve the bounds, we need to make stronger assumptions on the function <span class="math inline">\(f\)</span>.</p>



</section>

</main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function () {
  const wordsPerMinute = 200;
  const text = document.body.innerText;
  const words = text.trim().split(/\s+/).length;
  const readingTime = Math.ceil(words / wordsPerMinute);

  const readTimeEl = document.createElement("div");
  readTimeEl.innerText = `⏱️ ${readingTime} min read`;

  // Style it to appear centered
  readTimeEl.style.fontSize = "0.9em";
  readTimeEl.style.margin = "1em auto";
  readTimeEl.style.textAlign = "left";
  readTimeEl.style.width = "100%";

  const title = document.querySelector("h1");
  if (title) {
    title.parentNode.insertBefore(readTimeEl, title.nextSibling);
  }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
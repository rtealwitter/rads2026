<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Gradient Descent with Second Order Conditions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c5a5d5e27fcc88644031c24cff017230.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../favicon.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Spring 2026</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://discord.gg/dES3fSPEeC"> 
<span class="menu-text">Discord</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/1091652"> 
<span class="menu-text">Gradescope</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#first-order-conditions" id="toc-first-order-conditions" class="nav-link active" data-scroll-target="#first-order-conditions">First Order Conditions</a></li>
  <li><a href="#second-order-conditions" id="toc-second-order-conditions" class="nav-link" data-scroll-target="#second-order-conditions">Second Order Conditions</a>
  <ul class="collapse">
  <li><a href="#convergence-for-beta-smooth-functions" id="toc-convergence-for-beta-smooth-functions" class="nav-link" data-scroll-target="#convergence-for-beta-smooth-functions">Convergence for <span class="math inline">\(\beta\)</span>-Smooth Functions</a></li>
  <li><a href="#convergence-for-alpha-strongly-convex-functions" id="toc-convergence-for-alpha-strongly-convex-functions" class="nav-link" data-scroll-target="#convergence-for-alpha-strongly-convex-functions">Convergence for <span class="math inline">\(\alpha\)</span>-Strongly Convex Functions</a></li>
  <li><a href="#linear-regression-loss" id="toc-linear-regression-loss" class="nav-link" data-scroll-target="#linear-regression-loss">Linear Regression Loss</a></li>
  <li><a href="#eigendecomposition" id="toc-eigendecomposition" class="nav-link" data-scroll-target="#eigendecomposition">Eigendecomposition</a></li>
  <li><a href="#condition-number-connection" id="toc-condition-number-connection" class="nav-link" data-scroll-target="#condition-number-connection">Condition Number Connection</a></li>
  <li><a href="#accelerated-gradient-descent" id="toc-accelerated-gradient-descent" class="nav-link" data-scroll-target="#accelerated-gradient-descent">Accelerated Gradient Descent</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Gradient Descent with Second Order Conditions</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="first-order-conditions" class="level2">
<h2 class="anchored" data-anchor-id="first-order-conditions">First Order Conditions</h2>
<p>Recall the problem we considered last class. We are given a function <span class="math inline">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> with a <em>function oracle</em> and a <em>gradient oracle</em>. The function oracle returns <span class="math inline">\(f(\mathbf{x})\)</span> while the gradient oracle returns <span class="math inline">\(\nabla f(\mathbf{x})\)</span> for any <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span> we specify. Given these oracles, we want to find an approximate minimizer <span class="math inline">\(\hat{\mathbf{x}}\)</span> so that</p>
<p><span class="math display">\[\begin{align*}
f(\hat{\mathbf{x}}) \leq \min_{\mathbf{x}} f(\mathbf{x}) + \epsilon.
\end{align*}\]</span></p>
<p>We use the following prototypical gradient descent method. We choose a starting point <span class="math inline">\(\mathbf{x}^{(0)}\)</span>. Then, for <span class="math inline">\(t \in [T]\)</span>, we update the current iterate</p>
<p><span class="math display">\[\begin{align*}
\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \eta \nabla f(\mathbf{x}^{(t)}).
\end{align*}\]</span></p>
<p>Finally, we returned the best iterate <span class="math inline">\(\hat{\mathbf{x}} = \arg \min_{\{\mathbf{x}^{(t)}\}_{t=1}^T} f(\mathbf{x}^{(t)})\)</span>.</p>
<p>Since the gradient is the direction of steepest descent at <span class="math inline">\(\mathbf{x}\)</span>, we expect the function value to decrease for sufficiently small step sizes <span class="math inline">\(\eta\)</span>. However, if <span class="math inline">\(\eta\)</span> is too small, the function value may decrease very slowly. We are interested in how <em>quickly</em> we can reach an approximate minimizer.</p>
<p>For the basic gradient descent analysis we performed last class, we assumed that the function <span class="math inline">\(f\)</span> is convex, the function satisfies a Lipschitz condition <span class="math inline">\(\| \nabla f(\mathbf{x}) \|_2 \leq G\)</span> for all <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>, and the starting point is within a ball of radius <span class="math inline">\(R\)</span> centered at the minimizer <span class="math inline">\(\mathbf{x}^*\)</span>. The last condition ensures that we don’t start too far from the true minimizer.</p>
<p>When we set <span class="math inline">\(\eta = \frac{R}{G \sqrt{T}}\)</span> and run the algorithm for <span class="math inline">\(T \geq \frac{R^2 G^2}{\epsilon^2}\)</span> iterations, we showed that the output of the gradient algorithm satisfied <span class="math inline">\(f(\hat{\mathbf{x}}) \leq f(\mathbf{x}^*) + \epsilon\)</span> where <span class="math inline">\(\mathbf{x}^*\)</span> is the true minimizer of <span class="math inline">\(f\)</span>.</p>
<p>The proof was tricky because <span class="math inline">\(f(\mathbf{x}^{(t)})\)</span> does not necessarily improve monotonically; in particular, it’s possible to overshoot the minimizer especially if the function around the minimizer is very steep.</p>
<p>We also considered the projected gradient descent algorithm where we are optimizing over a convex set <span class="math inline">\(\mathcal{S}\)</span>. In addition to the function and gradient oracles, we used a <em>projection oracle</em> that returns <span class="math inline">\(P_{\mathcal{S}}(\mathbf{x}) = \arg \min_{\mathbf{y} \in \mathcal{S}} \| \mathbf{x} - \mathbf{y}\|_2\)</span>. The projected gradient descent algorithm is similar to the gradient descent algorithm except that we project the update onto the set <span class="math inline">\(\mathcal{S}\)</span>. In particular, <span class="math display">\[\begin{align*}
\mathbf{x}^{(t+1)} = P_{\mathcal{S}} \left( \mathbf{x}^{(t)} - \eta \nabla f(\mathbf{x}^{(t)}) \right).
\end{align*}\]</span></p>
<p>Using similar techniques to the gradient descent bound, we showed the same iteration bound for the projected gradient descent algorithm. The result is somewhat surprising: the projection step does not affect the iteration complexity. That is, projecting onto a convex set does not make the iterate too much worse.</p>
<p>Both the bounds in the gradient descent and projected gradient descent algorithms are actually optimal for convex first order optimization in general. But, in practice, the dependence on <span class="math inline">\(1/\epsilon^2\)</span> is pessimistic: gradient descent typically requires far fewer steps to reach an approximate minimizer.</p>
<p>The gap between theory and practice is in part because the previous bounds only made the very weak first order assumption that the function is Lipschitz. In the real world, many functions satisfy stronger assumptions. Today, we will discuss assumptions that involve the second derivative of the function.</p>
</section>
<section id="second-order-conditions" class="level2">
<h2 class="anchored" data-anchor-id="second-order-conditions">Second Order Conditions</h2>
<p>We will consider the second order conditions <span class="math inline">\(\alpha\)</span>-strong convexity and <span class="math inline">\(\beta\)</span>-smoothness. To define these conditions for multivariate functions, we will need to build some generality for multivariate functions. But, for scalar functions, we can describe them in terms of the second derivative. We say that that <span class="math inline">\(f\)</span> is <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(\beta\)</span>-smooth if for all <span class="math inline">\(x\)</span>, <span class="math display">\[\begin{align*}
\alpha \leq f''(x) \leq \beta.
\end{align*}\]</span></p>
<p>The following table gives the iteration complexity to achieve an <span class="math inline">\(\epsilon\)</span> approximate minimizer under different assumptions.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(G\)</span>-Lipschitz</th>
<th><span class="math inline">\(\beta\)</span>-smooth</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong><span class="math inline">\(R\)</span> bounded start</strong></td>
<td><span class="math inline">\(O\left( \frac{G^2 R^2}{\epsilon^2} \right)\)</span></td>
<td><span class="math inline">\(O\left( \frac{\beta R^2}{\epsilon} \right)\)</span></td>
</tr>
<tr class="even">
<td><strong><span class="math inline">\(\alpha\)</span>-strong convex</strong></td>
<td><span class="math inline">\(O\left( \frac{G^2}{\alpha \epsilon} \right)\)</span></td>
<td><span class="math inline">\(O\left( \frac{\beta}{\alpha} \log(1/\epsilon)\right)\)</span></td>
</tr>
</tbody>
</table>
<p>The takeaway is that having either an upper and lower bound on the second derivative speeds up convergence. In addition, having both an upper and lower speeds up convergence even more. <!--A function must be twice differentiable to be $\alpha$-strongly convex or $\beta$-smooth. On the other hand, gradient descent only requires that a function is differentiable.
--></p>
<p>For scalar functions, <span class="math inline">\(\beta\)</span>-smoothness is equivalent to <span class="math display">\[\begin{align*}
[f(y) - f(x)] - f'(x) (y-x) \leq \frac{\beta}{2} (y-x)^2
\end{align*}\]</span> for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>We’ll prove that <span class="math inline">\(\beta\)</span>-smoothness implies this condition to get a flavor for the proof techniques. We can write <span class="math display">\[\begin{align*}
f(y) - f(x) &amp;= \int_{x}^y f'(t) dt \\
&amp;\leq \int_{x}^y ( f'(x) + (t-x) \beta )dt \\
&amp;= f'(x) (y-x) + \frac{\beta}{2} (t-x)^2 |_x^y \\
&amp;= f'(x) (y-x) + \frac{\beta}{2} (y-x)^2
\end{align*}\]</span> where the inequality follows because <span class="math inline">\(f''(x) \leq \beta\)</span>. In particular, <span class="math inline">\(f'(t)\)</span> is below the tangent line at <span class="math inline">\(x\)</span> given by <span class="math inline">\(f'(x) + (t-x) \beta\)</span> for all <span class="math inline">\(t\)</span>.</p>
<p>Similarly, <span class="math inline">\(\alpha\)</span>-strong convexity is equivalent to <span class="math display">\[\begin{align*}
[f(y) - f(x)] - f'(x) (y-x) \geq \frac{\alpha}{2} (y-x)^2
\end{align*}\]</span> for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>Intuitively, the conditions say that the function is bounded above and below by a quadratic function.</p>
<p>The following figure illustrates the quantities in the conditions.</p>
<p align="center">
<img src="images/second_order.png" width="400px">
</p>
<p>The definitions generalize naturally to multivariate functions. A multivariate function <span class="math inline">\(f\)</span> is <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(\beta\)</span>-smooth if <span class="math display">\[\begin{align*}
\frac{\alpha}{2}\| \mathbf{y} - \mathbf{x} \|_2^2
\leq [f(\mathbf{y}) - f(\mathbf{x})] - \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x})
\leq \frac{\beta}{2}\| \mathbf{y} - \mathbf{x} \|_2^2
\end{align*}\]</span> for all <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
<section id="convergence-for-beta-smooth-functions" class="level3">
<h3 class="anchored" data-anchor-id="convergence-for-beta-smooth-functions">Convergence for <span class="math inline">\(\beta\)</span>-Smooth Functions</h3>
<p><strong><span class="math inline">\(\beta\)</span>-Smooth Convergence:</strong> Consider a multivariate function <span class="math inline">\(f\)</span> that is <span class="math inline">\(\beta\)</span>-smooth. Suppose we run gradient descent on <span class="math inline">\(f\)</span> with learning rate <span class="math inline">\(\eta=\frac1{\beta}\)</span> and suppose the initial point is within a ball of radius <span class="math inline">\(R\)</span> centered at the minimizer <span class="math inline">\(\mathbf{x}^*\)</span>. Then we can find an approximate minimizer in <span class="math inline">\(O \left(\frac{G^2 R^2}{\epsilon^2} \right)\)</span> steps. Compare the bound to the <span class="math inline">\(O \left( \frac{\beta R^2}{\epsilon} \right)\)</span> steps we need if <span class="math inline">\(f\)</span> were <span class="math inline">\(G\)</span>-Lipschitz instead of <span class="math inline">\(\beta\)</span>-smooth.</p>
<p>Why do you think gradient descent might be faster when a function is <span class="math inline">\(\beta\)</span>-smooth? Intuitively, the function is bounded above by a quadratic function so the function value around the minima cannot be too steep.</p>
<p>We won’t show the bound but we will show part of the proof. By <span class="math inline">\(\beta\)</span>-smoothness, we have <span class="math display">\[\begin{align*}
[f(\mathbf{x}^{(t+1)}) - f(\mathbf{x}^{(t)})]
- \nabla f(\mathbf{x}^{(t)})^\top (\mathbf{x}^{(t+1)} - \mathbf{x}^{(t)})
\leq \frac{\beta}{2} \left \| \mathbf{x}^{(t+1)} - \mathbf{x}^{(t)} \right \|_2^2
\end{align*}\]</span> where we set <span class="math inline">\(\mathbf{y} = \mathbf{x}^{(t+1)}\)</span> and <span class="math inline">\(\mathbf{x} = \mathbf{x}^{(t)}\)</span> in the <span class="math inline">\(\beta\)</span>-smoothness condition. By our update, we know that <span class="math inline">\(\mathbf{x}^{(t+1)} - \mathbf{x}^{(t)} = - \frac{1}{\beta} \nabla f(\mathbf{x}^{(t)})\)</span>. Plugging in this observation twice, we get that <span class="math display">\[\begin{align*}
[f(\mathbf{x}^{(t+1)}) - f(\mathbf{x}^{(t)})]
+ \frac{1}{\beta} \left \| \nabla f(\mathbf{x}^{(t)}) \right \|_2^2
\leq \frac{\beta}{2} \left \| \frac{1}{\beta} \nabla f(\mathbf{x}^{(t)}) \right \|_2^2.
\end{align*}\]</span></p>
<p>Rearranging, we have that <span class="math display">\[\begin{align*}
f(\mathbf{x}^{(t)}) - f(\mathbf{x}^{(t+1)})
\geq \frac{1}{2 \beta} \left \| \nabla f(\mathbf{x}^{(t)}) \right \|_2^2.
\end{align*}\]</span></p>
<p>Intuitively, we just used the <span class="math inline">\(\beta\)</span>-smoothness to guarantee that the function decreases by a factor that depends on the norm of the gradient. With this bound, proving the convergence result is not hard but also not obvious. A concise proof can be found on page 15 of these <a href="https://gowerrobert.github.io/pdf/M2_statistique_optimisation/grad_conv.pdf">notes</a>.</p>
<p>While we <em>did</em> use convexity (in the notes on page 15) to prove that the function value converges to an approximate minimizer, we did <em>not</em> use convexity to prove the last inequality that the function value decreases by a factor that depends on the norm of the gradient. In fact, we will use the last inequality to show that non-convex but <span class="math inline">\(\beta\)</span>-smooth functions quickly converge to stationary points (that are not necessarily minimizers). Formally, we say that <span class="math inline">\(\mathbf{x}\)</span> is a stationary point of <span class="math inline">\(f\)</span> if <span class="math inline">\(\nabla f(\mathbf{x}) = \mathbf{0}\)</span>. Stationary points include local minima, global minima, local maxima, global maxima, and saddle points.</p>
<p><strong><span class="math inline">\(\beta\)</span>-Smoooth Stationary Point Convergence:</strong> We will show that for a <span class="math inline">\(\beta\)</span>-smooth function, gradient descent converges to a stationary point in <span class="math inline">\(O \left( \frac{\beta}{\epsilon} \right)\)</span> iterations. Equivalently, after <span class="math inline">\(T\)</span> steps, we can find a point <span class="math inline">\(\hat{\mathbf{x}}\)</span> such that <span class="math display">\[\begin{align*}
\| \nabla f(\hat{\mathbf{x}}) \|_2^2 \leq \frac{2 \beta}{T}
\left( f(\mathbf{x}^{(0)}) - f(\mathbf{x}^*) \right).
\end{align*}\]</span></p>
<p>Using the inequality we just showed, we can use a telescoping sum to write <span class="math display">\[\begin{align*}
\sum_{t=0}^{T-1} \frac1{2\beta} \left\| \nabla f(\mathbf{x}^{(t)}) \right\|_2^2
\leq f(\mathbf{x}^{(0)}) - f(\mathbf{T})
\end{align*}\]</span> and so, after multiplying by <span class="math inline">\(\frac{2\beta}{T}\)</span>, <span class="math display">\[\begin{align*}
\frac{1}{T} \sum_{t=0}^{T-1} \left\| \nabla f(\mathbf{x}^{(t)}) \right\|_2^2
\leq \frac{2\beta}{T} \left( f(\mathbf{x}^{(0)}) - f(\mathbf{x}^*) \right)
\end{align*}\]</span> where we used that <span class="math inline">\(f(\mathbf{x}^{(T)}) \geq f(\mathbf{x}^*)\)</span>.</p>
<p>The claimed complexity result follows by the fact that the average of the gradient norms is at least as large as the minimum of the gradient norms. Non-convex functions are common in practice (e.g.&nbsp;loss functions of neural networks). We just showed that we can find a stationary point of such functions quickly using gradient descent if they are <span class="math inline">\(\beta\)</span>-smooth. Notice that stationary points do not necessarily give minimizers solutions to non-convex functions. In fact, a lot of work in optimizer design is focused on escaping stationary points that are not local minima.</p>
</section>
<section id="convergence-for-alpha-strongly-convex-functions" class="level3">
<h3 class="anchored" data-anchor-id="convergence-for-alpha-strongly-convex-functions">Convergence for <span class="math inline">\(\alpha\)</span>-Strongly Convex Functions</h3>
<p>Let’s return to <span class="math inline">\(\alpha\)</span>-strong convexity. We argued that it is possible for a <span class="math inline">\(\beta\)</span>-smooth function be non-convex. This is not true for <span class="math inline">\(\alpha\)</span>-strong functions: <span class="math inline">\(\alpha\)</span>-strong convexity implies that the function is convex.</p>
<p>We will replace our assumption about the starting point with <span class="math inline">\(\alpha\)</span>-strong convexity. The reason is that <span class="math inline">\(\alpha\)</span>-strong convexity implies that the function is bounded below by a quadratic function. This means that the function can’t be too flat; in particular, we always make good progress when we move towards the minimizer.</p>
<p>For <span class="math inline">\(\alpha\)</span>-strongly convex functions, we <em>adaptively</em> modify the gradient descent learning rate to be <span class="math inline">\(\eta_t = \frac{2}{\alpha (t+1)}\)</span>.</p>
<p><strong><span class="math inline">\(\alpha\)</span>-Strongly Convex Convergence:</strong> If we run gradient descent with this learning rate on an <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(G\)</span>-Lipschitz function for <span class="math inline">\(T\)</span> steps, the output <span class="math inline">\(\hat{\mathbf{x}}\)</span> satisfies <span class="math display">\[\begin{align*}
f(\hat{\mathbf{x}}) - f(\mathbf{x}^*) \leq \frac{2 G^2}{\alpha T}.
\end{align*}\]</span></p>
<p>Equivalently, the output is an <span class="math inline">\(\epsilon\)</span>-approximate minimizer after <span class="math inline">\(T = O\left(\frac{G^2}{\alpha \epsilon} \right)\)</span> iterations.</p>
<p><strong><span class="math inline">\(\alpha\)</span>-Strongly Convex and <span class="math inline">\(\beta\)</span>-Smooth Convergence:</strong> If we run gradient with on an <span class="math inline">\(\alpha\)</span>-strongly convex <em>and</em> <span class="math inline">\(\beta\)</span>-smooth function for <span class="math inline">\(T\)</span> steps, the output <span class="math inline">\(\hat{\mathbf{x}}\)</span> satisfies <span class="math display">\[\begin{align*}
\| \mathbf{x}^{(T)} - \mathbf{x}^* \|_2^2
\leq e^{-T \frac{\alpha}{\beta}} \| \mathbf{x}^{(0)} - \mathbf{x}^* \|_2^2.
\end{align*}\]</span> We call <span class="math inline">\(\kappa = \beta/\alpha\)</span> the <em>condition number</em> of <span class="math inline">\(f\)</span>. Since <span class="math inline">\(\kappa\)</span> is the ratio of the upper and lower bounds on the second derivative, it is a measure of how closely the function is sandwiched between two quadratic functions. As seen in the bound, a smaller condition number implies faster convergence.</p>
<p>The bound we just stated looks slightly different from the bounds we discussed before. We can convert the bound to a more familiar form by plugging in <span class="math inline">\(\mathbf{y} = \mathbf{x}^{(T)}\)</span> and <span class="math inline">\(\mathbf{x} = \mathbf{x}^*\)</span> into the <span class="math inline">\(\beta\)</span>-smoothness and <span class="math inline">\(\alpha\)</span>-strong convexity conditions:</p>
<p><span class="math display">\[\begin{align*}
  {\frac{\alpha}{2}}\|\mathbf{x} - \mathbf{y}\|_2^2 \leq  \left[f(\mathbf{y}) - f(\mathbf{x})\right] - \nabla f(\mathbf{x})^\top(\mathbf{y} - \mathbf{x}) \leq {\frac{\beta}{2}}\|\mathbf{x} - \mathbf{y}\|_2^2.
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(\nabla f(\mathbf{x}^*) = \mathbf{0}\)</span>, we have that</p>
<p><span class="math display">\[\begin{align*}
  \|\mathbf{x}^{(T)} - \mathbf{x}^*\|_2^2 &amp;\geq \frac{2}{\beta} \left[f(\mathbf{x}^{(T)}) - f(\mathbf{x}^*)\right].
\end{align*}\]</span></p>
<p>Now we can restate the prior bound.</p>
<p><strong><span class="math inline">\(\alpha\)</span>-Strongly Convex and <span class="math inline">\(\beta\)</span>-Smooth Convergence:</strong> Let <span class="math inline">\(f\)</span> be a <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex function. If we run gradient descent for <span class="math inline">\(T\)</span> steps with step size <span class="math inline">\(\eta = \frac{1}{\beta}\)</span> we have: <span class="math display">\[\begin{align*}
  f(\mathbf{x}^{(T)}) - f(\mathbf{x}^*)  \leq \frac{\beta}{2} e^{-T\frac{\alpha}{\beta}} \cdot  R^2
\end{align*}\]</span> where the starting point is within a ball of radius <span class="math inline">\(R\)</span> centered at the minimizer <span class="math inline">\(\mathbf{x}^*\)</span>. Equivalently, if <span class="math inline">\(T = O\left(\frac{\beta}{\alpha}\log(R\beta/\epsilon)\right)\)</span> we have, <span class="math display">\[\begin{align*}
  f({\mathbf{x}}^{(T)}) - f(\mathbf{x}^*) \leq \epsilon.
\end{align*}\]</span></p>
</section>
<section id="linear-regression-loss" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-loss">Linear Regression Loss</h3>
<p>We won’t prove this bound in general, but we will prove it for the special case of linear regression loss where <span class="math inline">\(f(\mathbf{x}) = \frac12 \| \mathbf{Ax} - \mathbf{b} \|_2^2\)</span>. Our goal will be to showcase some key ideas, introduce concepts like the Hessian, and demonstrate the connection between conditioning and linear algebra.</p>
<p>Let <span class="math inline">\(f:\mathbb{R}^d \to \mathbb{R}\)</span> be a twice differentiable function. We will define the Hessian <span class="math inline">\(\mathbf{H(x)} = \nabla^2 f(\mathbf{x})\)</span> to contain all partial second derivatives at a point <span class="math inline">\(\mathbf{x}\)</span>. In particular, let the entry in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column of the Hessian be given by <span class="math display">\[\begin{align*}
\left[ \nabla^2 f(\mathbf{x}) \right]_{i,j}
= \frac{\partial^2 f}{\partial x_i, x_j}.
\end{align*}\]</span></p>
<p>For vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> and a small scalar <span class="math inline">\(t\)</span>, we can approximate the gradient with the Hessian. Formally, <span class="math display">\[\begin{align*}
\nabla f(\mathbf{x} + t \mathbf{v})
\approx \nabla f(\mathbf{x}) + t \nabla^2 f(\mathbf{x}) \mathbf{v}.
\end{align*}\]</span></p>
<p>Let’s compute the Hessian for our example function <span class="math display">\[f(\mathbf{x}) = \frac{1}{2}\|\mathbf{A}\mathbf{x} - \mathbf{b}\|_2^2 = \frac{1}{2}\sum_{i=1}^n \left(\mathbf{x}^\top\mathbf{a}^{(i)} - {b}^{(i)}\right)^2\]</span> where <span class="math inline">\(\mathbf{a}^{(i)}\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\({b}^{(i)}\)</span> is the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(\mathbf{b}\)</span>. We have <span class="math display">\[\begin{align*}
  \frac{\partial f}{\partial x_k} &amp;= \frac{1}{2}\sum_{i=1}^n 2\left(\mathbf{x}^\top\mathbf{a}^{(i)} - {b}^{(i)}\right)\cdot a^{(i)}_k \\
  \frac{\partial^2 f}{\partial x_j\partial x_k} &amp;= \sum_{i=1}^n a^{(i)}_j  a^{(i)}_k.
\end{align*}\]</span> Therefore the Hessian is given by <span class="math inline">\(\mathbf{H} = \mathbf{A}^\top\mathbf{A}\)</span>.</p>
<p>We can also see the Hessian more directly. Recall that <span class="math inline">\(\nabla f(\mathbf{x}) = \mathbf{A}^\top (\mathbf{A} \mathbf{x} - \mathbf{b})\)</span>. Then we can write <span class="math display">\[\begin{align*}
\nabla f(\mathbf{x} + t \mathbf{v})
&amp;= \mathbf{A}^\top (\mathbf{A} (\mathbf{x} + t \mathbf{v}) - \mathbf{b}) \\
&amp;= \mathbf{A}^\top (\mathbf{A} \mathbf{x} - \mathbf{b}) + t \mathbf{A}^\top \mathbf{A} \mathbf{v}.
\end{align*}\]</span> By our vector definition of the Hessian, we can see that <span class="math inline">\(\mathbf{H} = \mathbf{A}^\top \mathbf{A}\)</span>.</p>
<p>For scalar functions, we saw that</p>
<ul>
<li><p><span class="math inline">\(f\)</span> is convex if <span class="math inline">\(f''(x) \geq 0\)</span> for all <span class="math inline">\(x\)</span>,</p></li>
<li><p><span class="math inline">\(f\)</span> is <span class="math inline">\(\alpha\)</span>-strongly convex if <span class="math inline">\(f''(x) \geq \alpha\)</span> for all <span class="math inline">\(x\)</span>, and</p></li>
<li><p><span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth if <span class="math inline">\(f''(x) \leq \beta\)</span> for all <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p>We would like to generalize these properties to the case where <span class="math inline">\(f\)</span> is multivariate and the second derivative is a matrix <span class="math inline">\(\mathbf{H}\)</span>.</p>
<p>We will start with a notion of positivity for matrices and relate it to convexity for multivariate functions.</p>
<p><strong>Positive Semidefinite (PSD):</strong> A square, symemtric matrix <span class="math inline">\(\mathbf{H}\)</span> is PSD if <span class="math inline">\(\mathbf{v}^\top \mathbf{H} \mathbf{v} \geq 0\)</span> for all <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>The PSD property is a natural notion of “positivity” for symmetric matrices. To denote that <span class="math inline">\(\mathbf{H}\)</span> is PSD, we write <span class="math inline">\(\mathbf{H} \succeq 0\)</span> where “<span class="math inline">\(\succeq\)</span>” denotes the Loewner order. We can write <span class="math inline">\(\mathbf{B} \succeq \mathbf{A}\)</span> to denote that <span class="math inline">\(\mathbf{B} - \mathbf{A} \succeq 0\)</span>. This gives a partial ordering on matrices (there some matrices that are incomparable under the Loewner order).</p>
<p><strong>Claim:</strong> If <span class="math inline">\(f\)</span> is twice differentiable, then it is convex if and only if <span class="math inline">\(\mathbf{H}\)</span> is positive semidefinite for all <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>We can check that our loss function is convex by showing the Hessian is PSD. In particular, we have that <span class="math display">\[\begin{align*}
\mathbf{v}^\top \mathbf{H} \mathbf{v}
= \mathbf{v}^\top \mathbf{A}^\top \mathbf{A} \mathbf{v}
= \| \mathbf{A} \mathbf{v} \|_2^2 \geq 0
\end{align*}\]</span> for any vector <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>We can also use the Loewner order to generalize the definitions of <span class="math inline">\(\alpha\)</span>-strong convexity and <span class="math inline">\(\beta\)</span>-smoothness. If <span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex, then <span class="math display">\[\begin{align*}
\alpha \mathbf{I} \preceq \mathbf{H} \preceq \beta \mathbf{I}
\end{align*}\]</span> where <span class="math inline">\(\mathbf{I}\)</span> is the <span class="math inline">\(d \times d\)</span> identity matrix.</p>
<p>Notice that this is a natural generalization of the scalar definitions of <span class="math inline">\(\alpha\)</span>-strong convexity and <span class="math inline">\(\beta\)</span>-smoothness where <span class="math display">\[\begin{align*}
\alpha \leq f''(x) \leq \beta.
\end{align*}\]</span></p>
<p>Equivalently, for any <span class="math inline">\(z\)</span>, we have that <span class="math inline">\(\alpha \| \mathbf{z} \|_2^2 \leq \mathbf{z}^\top \mathbf{H} \mathbf{z} \leq \beta \| \mathbf{z} \|_2^2\)</span>.</p>
<p>In order to better understand the Loewner order (and because it’s incredibly useful), we will consider the <em>eigendecomposition</em>.</p>
</section>
<section id="eigendecomposition" class="level3">
<h3 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h3>
<p>Every symmetric matrix <span class="math inline">\(\mathbf{H}\)</span> can be written as <span class="math inline">\(\mathbf{H} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top\)</span> where <span class="math inline">\(\mathbf{V}\)</span> is an orthogonal matrix and <span class="math inline">\(\mathbf{\Lambda}\)</span> is a diagonal matrix. Here, <span class="math inline">\(\mathbf{V}\)</span> is square and orthogonal so <span class="math inline">\(\mathbf{V}^\top \mathbf{V} = \mathbf{I} = \mathbf{V} \mathbf{V}^\top\)</span>.</p>
<p>For the <span class="math inline">\(i\)</span>th column <span class="math inline">\(\mathbf{v}_i\)</span> of <span class="math inline">\(\mathbf{V}\)</span>, we have that <span class="math display">\[\mathbf{H} \mathbf{v}_i = \lambda_i \mathbf{v}_i\]</span> where <span class="math inline">\(\lambda_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal entry of <span class="math inline">\(\mathbf{\Lambda}\)</span>. By definition, <span class="math inline">\(\mathbf{v}_i\)</span> is the <span class="math inline">\(i\)</span>th vector and <span class="math inline">\(\lambda_i\)</span> is the <span class="math inline">\(i\)</span>th eigenvalue of <span class="math inline">\(\mathbf{H}\)</span>.</p>
<p align="center">
<img src="images/eigendecomposition.png" width="400px">
</p>
<p>We can use the eigendecomposition to relate eigenvalues and the PSD property.</p>
<p><strong>Claim:</strong> The matrix <span class="math inline">\(\mathbf{H}\)</span> is PSD if and only if the eigenvalues are all positive i.e.&nbsp;<span class="math inline">\(\lambda_i \geq 0\)</span> for all <span class="math inline">\(i\)</span>.</p>
<p>For the first direction <span class="math inline">\((\Rightarrow)\)</span>, suppose that <span class="math inline">\(\mathbf{H}\)</span> is PSD. Consider an eigenvector <span class="math inline">\(\mathbf{v}_i\)</span>. By the PSD property, we have that <span class="math inline">\(\mathbf{v}_i^\top \mathbf{H} \mathbf{v}_i \geq 0\)</span>. By the eigendecomposition, we have that <span class="math inline">\(\mathbf{v}_i^\top \mathbf{H} \mathbf{v}_i = \lambda_i \mathbf{v}_i^\top \mathbf{v}_i = \lambda_i\)</span>. So <span class="math inline">\(\lambda_i \geq 0\)</span>. For the second direction <span class="math inline">\((\Leftarrow)\)</span>, suppose that <span class="math inline">\(\lambda_i \geq 0\)</span> for all <span class="math inline">\(i\)</span>. Consider any vector <span class="math inline">\(\mathbf{y}\)</span>. Since all the eigenvalues are non-negative, we can write <span class="math inline">\(\mathbf{\Lambda} = \sqrt{\mathbf{\Lambda}} \sqrt{\mathbf{\Lambda}}\)</span> where <span class="math inline">\(\sqrt{\mathbf{\Lambda}}\)</span> is the diagonal matrix with the square root of the eigenvalues on the diagonal. Using the eigendecomposition, we can write <span class="math display">\[\begin{align*}
\mathbf{y}^\top \mathbf{H} \mathbf{y} &amp;= \mathbf{y}^\top \mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top \mathbf{y} \\
&amp;= \mathbf{y}^\top \mathbf{V} \sqrt{\mathbf{\Lambda}} \sqrt{\mathbf{\Lambda}} \mathbf{V}^\top \mathbf{y} \\
&amp;= \|\mathbf{y}^\top \mathbf{V} \sqrt{\mathbf{\Lambda}} \|_2^2 \geq 0.
\end{align*}\]</span> For the last equality, we used that diagonal matrices are symmetric.</p>
<p>We can use this to write our second order assumptions in terms of the Loewner order.</p>
<p><strong>Claim:</strong> <span class="math inline">\(f\)</span> is <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(\beta\)</span>-smooth if and only if <span class="math inline">\(\alpha \leq \lambda_d \leq \ldots \leq \lambda_1 \leq \beta\)</span>. Here, we are assuming that the eigenvalues are sorted in decreasing order.</p>
<p>To see this, we can write <span class="math display">\[\begin{align*}
\beta \mathbf{I} - \mathbf{H} = \beta \mathbf{V} \mathbf{V}^\top
- \mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top
= \mathbf{V} (\beta \mathbf{I} - \mathbf{\Lambda}) \mathbf{V}^\top.
\end{align*}\]</span> With this observation, we can see that the <span class="math inline">\(i\)</span>th eigenvalue of <span class="math inline">\(\beta \mathbf{I} - \mathbf{H}\)</span> is <span class="math inline">\(\beta - \lambda_i\)</span>. Notice that <span class="math inline">\(\beta \mathbf{I} - \mathbf{H} \succeq 0\)</span> if and only if <span class="math inline">\(\beta - \lambda_i \geq 0\)</span> for all <span class="math inline">\(i\)</span>. The <span class="math inline">\(\alpha\)</span>-strongly convex direction is similar.</p>
<p>We will use one more useful property of the eigendecomposition. Since the eigenvectors span all of <span class="math inline">\(\mathbb{R}^d\)</span>, we can write any vector <span class="math inline">\(\mathbf{z}\)</span> as a linear combination of the eigenvectors. In particular, <span class="math display">\[\begin{align*}
\mathbf{z} = \sum_{i=1}^d \alpha_i \mathbf{v}_i
\end{align*}\]</span> for some some coefficients <span class="math inline">\(\alpha_i\)</span>. (We actually know that <span class="math inline">\(\alpha_i = \mathbf{z}^\top \mathbf{v}_i\)</span>.) With this observation, we can see that <span class="math display">\[\begin{align*}
\lambda_\min (\mathbf{H}) \| \mathbf{z} \|_2^2 \leq
\mathbf{z}^\top \mathbf{H} \mathbf{z}
\leq \lambda_\max (\mathbf{H}) \| \mathbf{z} \|_2^2
\end{align*}\]</span> where <span class="math inline">\(\lambda_\min\)</span> and <span class="math inline">\(\lambda_\max\)</span> denote the minimum and maximum eigenvalues of <span class="math inline">\(\mathbf{H}\)</span>.</p>
<p>Then it follows that if the maximum eigenvalue of <span class="math inline">\(\mathbf{H}=\nabla^2 f(\mathbf{x})\)</span> is <span class="math inline">\(\beta\)</span> and the minimum eigenvalue is <span class="math inline">\(\alpha\)</span>, then <span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex.</p>
</section>
<section id="condition-number-connection" class="level3">
<h3 class="anchored" data-anchor-id="condition-number-connection">Condition Number Connection</h3>
<p>With these tools, let’s return to proving the convergence result for the particular function <span class="math display">\[f(\mathbf{x}) = \frac12 \| \mathbf{Ax} - \mathbf{b}\|_2^2.\]</span> Let <span class="math inline">\(\lambda_\max = \lambda_\max(\mathbf{A}^\top \mathbf{A})\)</span> and <span class="math inline">\(\lambda_\min = \lambda_\min(\mathbf{A}^\top \mathbf{A})\)</span>. We will set the step size <span class="math inline">\(\eta = \frac{1}{2\lambda_\max}\)</span>. The gradient descent update is given by <span class="math display">\[\begin{align*}
\mathbf{x}^{(t+1)}
= \mathbf{x}^{(t)} - \frac1{2 \lambda_\max}
2 \mathbf{A}^\top (\mathbf{A} \mathbf{x}^{(t)} - \mathbf{b}).
\end{align*}\]</span></p>
<p>We can view this update as a repeated matrix multiplication. In particular, we have that <span class="math display">\[\begin{align*}
\mathbf{x}^{(t+1)}
&amp;= \mathbf{x}^{(t)} - \frac1{\lambda_\max} \mathbf{A}^\top \left( \mathbf{A} \mathbf{x}^{(t)} - \mathbf{b} \right) \\
&amp;=  \mathbf{x}^{(t)} - \frac1{\lambda_\max}
\mathbf{A}^\top \mathbf{A} \mathbf{x}^{(t)} -
\frac1{\lambda_\max}
\mathbf{A}^\top \mathbf{b}.
\end{align*}\]</span> We will connect the equation to the optimal solution <span class="math inline">\(\mathbf{x}^*\)</span>. Since it is a stationary point, the gradient of the optimal solution is zero and we have that <span class="math inline">\(\mathbf{A}^\top ( \mathbf{A} \mathbf{x}^* - \mathbf{b}) = \mathbf{0}\)</span>. So we can write <span class="math inline">\(\mathbf{A}^\top \mathbf{b} = \mathbf{A}^\top \mathbf{A} \mathbf{x}^*\)</span>. Subtracting the optimal solution <span class="math inline">\(\mathbf{x}^*\)</span> from both sides and substituting our expression for <span class="math inline">\(\mathbf{A}^\top \mathbf{b}\)</span>, we have that <span class="math display">\[\begin{align*}
\mathbf{x}^{(t+1)} - \mathbf{x}^*
&amp;= \mathbf{x}^{(t)}  - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A} \mathbf{x}^{(t)} + \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A} \mathbf{x}^* - \mathbf{x}^* \\
&amp;= \left( \mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A} \right) \left( \mathbf{x}^{(t)} - \mathbf{x}^* \right).
\end{align*}\]</span> By repeatedly applying the equation, we can write <span class="math display">\[\begin{align*}
(\mathbf{x}^{(T)} - \mathbf{x}^*)
= (\mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A})^T (\mathbf{x}^{(0)} - \mathbf{x}^*).
\end{align*}\]</span></p>
<p>We will show that the maximum eigenvalue of <span class="math inline">\((\mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A})\)</span> is small. Therefore the difference between <span class="math inline">\(\mathbf{x}^{(T)}\)</span> and <span class="math inline">\(\mathbf{x}^*\)</span> decreases quickly.</p>
<p>Using the eigendecomposition of <span class="math inline">\(\mathbf{A}^\top \mathbf{A}\)</span>, we can write <span class="math display">\[\begin{align*}
\mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A}
= \mathbf{V} \mathbf{V}^\top - \frac1{\lambda_\max} \mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top
= \mathbf{V} \left( \mathbf{I} - \frac1{\lambda_\max} \mathbf{\Lambda} \right) \mathbf{V}^\top.
\end{align*}\]</span> The eigenvalues are given by <span class="math inline">\(1 - \frac{\lambda_i}{\lambda_\max}\)</span>. The smallest eigenvalue is <span class="math inline">\(1-\frac{\lambda_\max}{\lambda_\max} =0\)</span> while the largest eigenvalue is <span class="math inline">\(1 - \frac{\lambda_\min}{\lambda_\max}\)</span>. Recall that <span class="math inline">\(\lambda_\min = \alpha\)</span> and <span class="math inline">\(\lambda_\max\)</span> so the largest eigenvalue is <span class="math inline">\(1 - \frac{\alpha}{\beta} = 1- \frac1{\kappa}\)</span>.</p>
<p>Notice that repeatedly applying symmetric matrices only modifies the eigenvalues: <span class="math display">\[\begin{align*}
\mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top
\mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top
= \mathbf{V} \mathbf{\Lambda}^2 \mathbf{V}^\top.
\end{align*}\]</span></p>
<p>Using this property, the maximum eigenvalue of <span class="math inline">\((\mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A})^T\)</span> is <span class="math inline">\((1 - \frac1{\kappa})^T\)</span>. We will use the inequality that <span class="math inline">\((1-\frac1{x})^x \leq \frac1{e}\)</span> for all positive <span class="math inline">\(x\)</span> to bound the maximum eigenvalue. <span class="math display">\[\begin{align*}
\left( 1- \frac{1}{\kappa} \right)^T
= \left( \left( 1- \frac{1}{\kappa} \right)^{\kappa} \right)^{\frac{T}{\kappa}}
\leq \frac{1}{e^{\frac{T}{\kappa}}} = e^{-\frac{T}{\kappa}}.
\end{align*}\]</span></p>
<p>Putting everything together, we have that <span class="math display">\[\begin{align*}
\| \mathbf{x}^{(T)} - \mathbf{x}^* \|_2^2
&amp;= \| (\mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A})^T (\mathbf{x}^{(0)} - \mathbf{x}^*) \|_2^2 \\
&amp;\leq \lambda_\max\left((\mathbf{I} - \frac1{\lambda_\max} \mathbf{A}^\top \mathbf{A})^T\right )  \| \mathbf{x}^{(0)} - \mathbf{x}^* \|_2^2 \\
&amp;\leq e^{-2\frac{T}{\kappa}} \| \mathbf{x}^{(0)} - \mathbf{x}^* \|_2^2.
\end{align*}\]</span> The <span class="math inline">\(\alpha\)</span>-strong convexity and <span class="math inline">\(\beta\)</span>-smoothness convergence bound follows when <span class="math inline">\(f\)</span> is the linear regression loss.</p>
</section>
<section id="accelerated-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="accelerated-gradient-descent">Accelerated Gradient Descent</h3>
<p>It turns out that we can actually converge faster for <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(\beta\)</span>-smooth functions. We will use a technique called <em>accelerated gradient descent</em>.</p>
<p>Initialize starting vector <span class="math inline">\(\mathbf{x}^{(0)} = \mathbf{y}^{(1)} = \mathbf{z}^{(1)}\)</span>. For <span class="math inline">\(t = 1,\ldots, T\)</span>, compute</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y}^{(t+1)} = \mathbf{x}^{(t)} - \frac{1}{\beta}\nabla f(\mathbf{x}^{(t)})\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{x}^{(t+1)} = \left(1 + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right) \mathbf{y}^{(t+1)} + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\left(\mathbf{y}^{(t+1)} - \mathbf{y}^{(t)}\right)\)</span>.</p></li>
</ul>
<p><strong>Accelerated Gradient Descent:</strong> Let <span class="math inline">\(f\)</span> be a <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex function. If we run accelerated gradient descent for <span class="math inline">\(T\)</span> steps, the output <span class="math inline">\(\hat{\mathbf{x}}\)</span> satisfies <span class="math display">\[\begin{align*}
f(\mathbf{x}^{(T)}) - f(\mathbf{x}^*) \leq
\kappa e^{-T / \sqrt{\kappa}}
[f(\mathbf{x}^{(0)}) - f(\mathbf{x}^*)].
\end{align*}\]</span> Equivalently, if <span class="math inline">\(T = O(\sqrt{\kappa} \log(\kappa / \epsilon))\)</span> we find an <span class="math inline">\(\epsilon\)</span>-approximate minimizer. Notice the improvement from <span class="math inline">\(\kappa\)</span> to <span class="math inline">\(\sqrt{\kappa}\)</span>.</p>
<p>We won’t show the proof but we will wave at the intuition.</p>
<p align="center">
<img src="images/poor_condition.png" width="400px">
</p>
<p>Standard gradient descent can get stuck backtracking along the valley for functions like the one in the figure. In contrast, accelerated gradient descent maintains its momentum in the direction of descent and avoids oscillating back and forth.</p>
<p>Today, we saw how to gradient descent performs better under second order assumptions. Next time, we’ll discuss how to use gradient descent in online settings and what to do when computing full gradients is too expensive.</p>



</section>
</section>

</main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function () {
  const wordsPerMinute = 200;
  const text = document.body.innerText;
  const words = text.trim().split(/\s+/).length;
  const readingTime = Math.ceil(words / wordsPerMinute);

  const readTimeEl = document.createElement("div");
  readTimeEl.innerText = `⏱️ ${readingTime} min read`;

  // Style it to appear centered
  readTimeEl.style.fontSize = "0.9em";
  readTimeEl.style.margin = "1em auto";
  readTimeEl.style.textAlign = "left";
  readTimeEl.style.width = "100%";

  const title = document.querySelector("h1");
  if (title) {
    title.parentNode.insertBefore(readTimeEl, title.nextSibling);
  }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Similarity Estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c5a5d5e27fcc88644031c24cff017230.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../favicon.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Spring 2026</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://discord.gg/dES3fSPEeC"> 
<span class="menu-text">Discord</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/1091652"> 
<span class="menu-text">Gradescope</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#similarity-estimation" id="toc-similarity-estimation" class="nav-link active" data-scroll-target="#similarity-estimation">Similarity Estimation</a>
  <ul class="collapse">
  <li><a href="#minhash-algorithm" id="toc-minhash-algorithm" class="nav-link" data-scroll-target="#minhash-algorithm">MinHash Algorithm</a></li>
  </ul></li>
  <li><a href="#near-neighbor-search" id="toc-near-neighbor-search" class="nav-link" data-scroll-target="#near-neighbor-search">Near Neighbor Search</a>
  <ul class="collapse">
  <li><a href="#locality-sensitive-hashing" id="toc-locality-sensitive-hashing" class="nav-link" data-scroll-target="#locality-sensitive-hashing">Locality Sensitive Hashing</a></li>
  <li><a href="#simhash" id="toc-simhash" class="nav-link" data-scroll-target="#simhash">SimHash</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Similarity Estimation</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<p>We saw how, without additional structure, we expect that learning in <span class="math inline">\(d\)</span> dimensions requires <span class="math inline">\(n=2^d\)</span> data points. If we really had a data set that large, then the JL lemma would be vacuous since <span class="math inline">\(\log n = d\)</span>.</p>
<p>The JL lemma tells us how we can preserve <span class="math inline">\(\ell_2\)</span>-norm distances between points. Let’s see how we can preserve <em>similarity</em> between points.</p>
<section id="similarity-estimation" class="level2">
<h2 class="anchored" data-anchor-id="similarity-estimation">Similarity Estimation</h2>
<p>Let’s consider the following problem: How do services like Shazam match a song clip against a library of more than 11 million songs in a fraction of a second? To make the problem more challenging, the song clips we match generally have additional noise like background sound in a car.</p>
<p>We know how Shazam does this because they published a <a href="https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf">white paper</a> on their method. The key idea is to use a <em>spectrogram</em> to represent the song clip. A spectrogram is a 2D matrix where the rows correspond to frequencies and the columns correspond to time. We then convert the spectrogram to a <em>fingerprint</em> by taking the peaks in the spectrogram represented as a high-dimensional binary vector.</p>
<p align="center">
<img src="images/shazam.png" width="400">
</p>
<p>The process is a little more challenging because song clips may be slightly off set from the song clips in our database. In order to make the method work, they use anchor points to align the song clips. But, for our purposes, we are primarily interested in how we can compress the high-dimensional binary vector.</p>
<p>Once we have a fingerprint from a song clip, we want to search our database for similar songs. Formally, given a binary vector <span class="math inline">\(q \in \{0,1\}^d\)</span> representing our song clip in <span class="math inline">\(d\)</span> dimensions, we want to find a nearby fingerprint <span class="math inline">\(\mathbf{y} \in \{0,1\}^d\)</span> in our database. The first challenge is that our database is possibly huge with <span class="math inline">\(O(nd)\)</span> bits where <span class="math inline">\(n\)</span> is the number of song clips. The second challenge is that it is expensive to compute the distance between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> with runtime <span class="math inline">\(O(d)\)</span>.</p>
<p>In light of these challenges, our goal is to design a more compact sketch for comparing <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. We ideally want to use <span class="math inline">\(k \ll d\)</span> space and time complexity. We will compute a compression <span class="math inline">\(C(\mathbf{x})\)</span> and <span class="math inline">\(C(\mathbf{y})\)</span> of <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, respectively. As in the JL compressions, we want <span class="math inline">\(C(\mathbf{x})\)</span> and <span class="math inline">\(C(\mathbf{y})\)</span> to be close when <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are similar and far otherwise.</p>
<p>We will use the notion of Jaccard similarity to measure the similarity between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p><strong>Jaccard Similary:</strong> The Jaccard similarity between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is <span class="math display">\[\begin{align*}
J(\mathbf{x}, \mathbf{y})
= \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}
= \frac{\textrm{# non-zero entries in common}}{\textrm{# non-zero entries in total}}.
\end{align*}\]</span></p>
<p>Jaccard similarity can be applied to any data which has a natural binary representation:</p>
<ul>
<li><p>We can use the “bag-of-words” model to represent a document as a binary vector where each entry is 1 if the word is in the document and 0 otherwise. Then the Jaccard similarity is the fraction of words in common between the two documents.</p></li>
<li><p>We can extract features from earthquake data for early detection. The approach is described in <a href="https://www.vldb.org/pvldb/vol11/p1674-rong.pdf">this</a> paper.</p></li>
<li><p>We can compare cached web pages to determine if they are similar and avoid downloading the same content multiple times.</p></li>
</ul>
<section id="minhash-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="minhash-algorithm">MinHash Algorithm</h3>
<p>We’ll use the MinHash algorithm to build the compression function <span class="math inline">\(c:\{0,1\}^d \rightarrow \mathbb{R}^k\)</span>.</p>
<p>Consider an input <span class="math inline">\(\mathbf{x} \in \{0,1\}^d\)</span>. We start by choosing <span class="math inline">\(k\)</span> random hash functions <span class="math inline">\(h_i: \{0, \ldots, d\} \rightarrow [0,1]\)</span> for <span class="math inline">\(i \in [k]\)</span>. For each hash function <span class="math inline">\(h_i\)</span> we’ll compute <span class="math display">\[\begin{align*}
c_i = \min_{j \in \{1, \ldots, d\}: \mathbf{x}_j = 1} h_i(j).
\end{align*}\]</span> In words, we hash each index where <span class="math inline">\(\mathbf{x}\)</span> is non-zero to a random value in <span class="math inline">\([0,1]\)</span>. Then we take the minimum value of the hash function over all the non-zero indices. We call the compression <span class="math inline">\(C(\mathbf{x}) = (c_1, \ldots, c_k)\)</span>. We can see this process represented in the figure below.</p>
<p align="center">
<img src="images/minhash.png" width="400">
</p>
<p>We’ll argue that for all <span class="math inline">\(i\)</span>, <span class="math display">\[\begin{align*}
\Pr \left( c_i(\mathbf{x}) = c_i(\mathbf{y}) \right)
= J(\mathbf{x}, \mathbf{y})
= \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}.
\end{align*}\]</span></p>
<p>Every non-zero index in <span class="math inline">\(\mathbf{x} \cup \mathbf{y}\)</span> is equally likely to produce the lowest hash value. We have <span class="math inline">\(c_i(\mathbf{x}) = c_j(\mathbf{x})\)</span> if and only if the index hashed to the lowest value is 1 in both <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Since there are <span class="math inline">\(|\mathbf{x} \cap \mathbf{y}|\)</span> such indices, the probability that <span class="math inline">\(c_i(\mathbf{x}) = c_j(\mathbf{x})\)</span> is the Jaccard similarity.</p>
<p>Inspired by this observation, the MinHash algorithm returns an estimate for the Jaccard similarity between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>: <span class="math display">\[\begin{align*}
\tilde{J}(\mathbf{x}, \mathbf{y}) = \frac1{k}
\sum_{i=1}^k \mathbb{1}[c_i(\mathbf{x}) = c_i(\mathbf{y})].
\end{align*}\]</span> By linearity of expectation, we have <span class="math display">\[\begin{align*}
\mathbb{E}[\tilde{J}(\mathbf{x}, \mathbf{y})] = \frac1{k} \sum_{i=1}^k \Pr( c_i(\mathbf{x}) = c_i(\mathbf{y}))
= J(\mathbf{x}, \mathbf{y}).
\end{align*}\]</span> We can reduce the variance of the estimator by increasig the number of hash functions <span class="math inline">\(k\)</span>. The variance of the estimator is <span class="math display">\[\begin{align*}
\textrm{Var}( \tilde{J}(\mathbf{x}, \mathbf{y}) )
&amp;= \frac1{k^2} \sum_{i=1}^k \textrm{Var}( \mathbb{1}[c_i(\mathbf{x}) = c_i(\mathbf{y})] ) \\
&amp;= \frac1{k^2} \sum_{i=1}^k J(\mathbf{x}, \mathbf{y}) -  J(\mathbf{x}, \mathbf{y})^2
\leq \frac1{k} J(\mathbf{x}, \mathbf{y})
\leq \frac1{k}.
\end{align*}\]</span></p>
<p>How big should we choose <span class="math inline">\(k\)</span> so that the the estimate is with an additive error <span class="math inline">\(\epsilon\)</span> of its expectation with probability <span class="math inline">\(1-\delta\)</span>?</p>
<p>Chebyshev’s inequality tells us that <span class="math display">\[\begin{align*}
\Pr\left( | J(\mathbf{x}, \mathbf{y}) - \tilde{J}(\mathbf{x}, \mathbf{y}) | \geq \alpha \frac{1}{\sqrt{k}}\right)
\leq \frac1{\alpha^2} = \delta
\end{align*}\]</span></p>
<p>The additive error is <span class="math inline">\(\epsilon= \alpha \frac{1}{\sqrt{k}}\)</span>. Since <span class="math inline">\(\alpha = 1/\sqrt{\delta}\)</span>, we have <span class="math inline">\(k = \frac1{\epsilon^2 \delta}\)</span>.</p>
<p>We have shown that as long as <span class="math inline">\(k=O(\frac1{\epsilon^2 \delta})\)</span>, then with probability <span class="math inline">\(1-\delta\)</span>, <span class="math display">\[\begin{align*}
J(\mathbf{x}, \mathbf{y}) - \epsilon
\leq \tilde{J} (\mathbf{x}, \mathbf{y})
\leq J(\mathbf{x}, \mathbf{y}) + \epsilon.
\end{align*}\]</span> Notice that we only need <span class="math inline">\(O(k)\)</span> time to compute the estimate which is <em>independent</em> of the original fingerprint dimension <span class="math inline">\(d\)</span>.</p>
<p>We can improve the result to have a <span class="math inline">\(\log(1/\delta)\)</span> dependence using a Chernoff bound or the biased coin bound we showed in the concentration inequalities lecture.</p>
<p><strong>Biased Coin Bound:</strong> Let the probability that a coin lands heads be <span class="math inline">\(b\)</span>. Choose <span class="math inline">\(k \geq \frac{3\log(2/\delta)}{\epsilon^2}\)</span>. If we flip a biased coin <span class="math inline">\(k\)</span> times, let <span class="math inline">\(S\)</span> be the number of heads. Notice that <span class="math inline">\(\mu = bk\)</span>. Then <span class="math display">\[
\Pr(| S - b k | \geq \epsilon k) \leq \delta.
\]</span></p>
<p>Think of the indicator random variables <span class="math inline">\(\mathbb{1}[c_i(\mathbf{x}) = c_i(\mathbf{y})]\)</span> as coin flips. The probability the coin lands “heads” is <span class="math inline">\(J(\mathbf{x}, \mathbf{y})\)</span>. Then we can apply the biased coin bound after dividing by <span class="math inline">\(k\)</span>. With probability <span class="math inline">\(1-\delta\)</span>, the number of “heads” is within <span class="math inline">\(\epsilon\)</span> of <span class="math inline">\(J(\mathbf{x}, \mathbf{y})\)</span> if we set <span class="math inline">\(k=O(\frac{\log(1/\delta)}{\epsilon^2})\)</span>.</p>
</section>
</section>
<section id="near-neighbor-search" class="level2">
<h2 class="anchored" data-anchor-id="near-neighbor-search">Near Neighbor Search</h2>
<p>Our prior goal was to preserve distances between points. But the real reason we care about preserving distances is to find nearby points. In this section, we’ll focus on the near neighbor search problem. We want to find vectors in a database <span class="math inline">\(\mathbf{y}_1, \ldots, \mathbf{y}_n \in \mathbb{R}^d\)</span> that are close to an input query vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>.</p>
<p>We showed before how to improve the runtime of linear scans from <span class="math inline">\(O(nd)\)</span> to <span class="math inline">\(O(nk)\)</span> and the space complexity from <span class="math inline">\(O(nd)\)</span> to <span class="math inline">\(O(nk)\)</span>. This can be helpful but we want to go even further to an algorithm which finds nearby vectors with runtime <em>sublinear</em> in <span class="math inline">\(n\)</span>.</p>
<p>To see convince yourself it is possible to find nearest neighbors in sublinear time, consider the problem in one dimension. We know how to solve this problem in <span class="math inline">\(\log_2 n\)</span> time using binary search. We can generalize this approach to higher dimensions with data structures like k-d trees. Unfortunately, the runtime is roughly <span class="math inline">\(O(d \min(n, 2^d))\)</span> which is sublinear only for <span class="math inline">\(d=o(\log n)\)</span>.</p>
<p>High-dimensional vector search is exploding as a research area because of machine-learning multi-model embeddings for images, text, and more. For example, models like CLIP allow us to embed images and text into a common space. Then we can search for images that are similar to a text query or vice versa.</p>
<p align="center">
<img src="images/clip.png" width="400">
</p>
<p>There are many approaches to nearest neighbor search including spectral hashing and vector quantization. Today, we’ll focus on locality sensitive hashing (LSH). The key insight of LSH is to trade worse space complexity for better time complexity. So we’ll use <span class="math inline">\(O(n)\)</span> space or more but we’ll get sublinear time complexity.</p>
<section id="locality-sensitive-hashing" class="level3">
<h3 class="anchored" data-anchor-id="locality-sensitive-hashing">Locality Sensitive Hashing</h3>
<p>Let <span class="math inline">\(h: \mathbb{R}^d \rightarrow \{1, \ldots, m\}\)</span> be a random hash function. Consider a similarity function <span class="math inline">\(s\)</span> such as the Jaccard similarity. We call <span class="math inline">\(h\)</span> <em>locally sensitive</em> if <span class="math inline">\(\Pr(h(\mathbf{x}) = h(\mathbf{y}))\)</span> is high when <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are similar and low when <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are dissimilar.</p>
<p>We will now take a first stab at designing the LSH scheme. Let <span class="math inline">\(c: \{0,1\}^d \rightarrow [0,1]\)</span> be a single instantiation of MinHash. Let <span class="math inline">\(g: [0,1] \rightarrow \{1, \ldots, m\}\)</span> be a uniform random hash function. Then we’ll define the locally sensitive hash function <span class="math inline">\(h(\mathbf{x}) = g(c(\mathbf{x}))\)</span>.</p>
<p>Observe that <span class="math inline">\(h(\mathbf{x}) = h(\mathbf{y})\)</span> when <span class="math inline">\(c(\mathbf{x}) = c(\mathbf{y})\)</span> or when <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> happen to randomly be mapped to the same output. In mathematical notation, we have <span class="math display">\[\begin{align*}
\Pr(h(\mathbf{x}) = h(\mathbf{y}))
&amp;= \Pr(c(\mathbf{x}) = c(\mathbf{y}))
+ \left( 1- \Pr(c(\mathbf{x})) = c(\mathbf{y})) \right) \frac1{m} \\
&amp;\leq J(\mathbf{x}, \mathbf{y}) + \frac1{m}.
\end{align*}\]</span></p>
<p>The basic approach for near neighbor search in a database has two steps. In the preprocessing step, we select a random LSH hash function <span class="math inline">\(h: \{0,1\}^d \rightarrow \{1,\ldots, m\}\)</span>. We create a table with <span class="math inline">\(m=O(n)\)</span> slots. For each vector <span class="math inline">\(\mathbf{y}_i\)</span> with <span class="math inline">\(i \in [n]\)</span>, we compute <span class="math inline">\(h(\mathbf{y}_i)\)</span> and store <span class="math inline">\(\mathbf{y}_i\)</span> in the corresponding slot in the table.</p>
<p>In the query step, we want to find near neighbors of <span class="math inline">\(\mathbf{x}\)</span>. We compute <span class="math inline">\(h(\mathbf{x})\)</span> and look in the corresponding slot of the table. We then scan through all the vectors in the slot and compute the distance to <span class="math inline">\(\mathbf{x}\)</span>. The time required is now the time to compute the distance between the query vector and each vector in the slot.</p>
<p>There are two main considerations that we want to analyze. The first is the false negative rate: What’s the probability we do not find a vector that is close to <span class="math inline">\(\mathbf{x}\)</span>? The second is the false positive rate: What’s the probability that a vector in the slot is not close to <span class="math inline">\(\mathbf{x}\)</span>?</p>
<p>A higher false negative rate means we may miss a vector that is close to <span class="math inline">\(\mathbf{x}\)</span>. A higher false positive rate means we increase the runtime of the query step because we have to scan through more vectors in the slot. Note that the meaning of “close” and “not close” is application dependent. For example, we may want to find anything with Jaccard similarity at least <span class="math inline">\(.4\)</span> but we may not want to compare against anything with similarity less than <span class="math inline">\(.2\)</span>.</p>
<p>We can reduce the false negative rate by using multiple hash functions. In the preprocessing step, we now select <span class="math inline">\(t\)</span> independent LSH hash functions <span class="math inline">\(h_1, \ldots, h_t\)</span>. We create tables <span class="math inline">\(T_1, \ldots, T_t\)</span> each with <span class="math inline">\(m\)</span> slots. For every vector <span class="math inline">\(i \in [n]\)</span> and every table <span class="math inline">\(j\)</span> , we compute <span class="math inline">\(h_1(\mathbf{y}_i), \ldots, h_t(\mathbf{y}_i)\)</span> and store <span class="math inline">\(\mathbf{y}_i\)</span> in the corresponding slots in <span class="math inline">\(T_1, \ldots, T_t\)</span>. In the query step we compute <span class="math inline">\(h_1(\mathbf{x}), \ldots, h_t(\mathbf{x})\)</span> and look in each one of the corresponding slots in <span class="math inline">\(T_1, \ldots, T_t\)</span>.</p>
<p>We can now analyze the probability that we find a nearby vector <span class="math inline">\(\mathbf{y}\)</span> that has Jaccard similarity at least <span class="math inline">\(.4\)</span>. The probability that we find <span class="math inline">\(\mathbf{y}\)</span> is <span class="math display">\[\begin{align*}
1 - \Pr(h_1(\mathbf{x}) \neq h_1(\mathbf{y}))
\cdots
\Pr(h_t(\mathbf{x}) \neq h_t(\mathbf{y}))
= 1-(1-J(\mathbf{x}, \mathbf{y}))^t.
\end{align*}\]</span> If the Jaccard similarity is <span class="math inline">\(.4\)</span> and <span class="math inline">\(t=10\)</span>, then the probability that we find <span class="math inline">\(\mathbf{y}\)</span> is <span class="math inline">\(1-(1-.4)^t \approx .99\)</span>. But now we have a new problem which is that we may find many false positives. If the Jaccard similarity is <span class="math inline">\(.2\)</span> and <span class="math inline">\(t=10\)</span>, then the probability that we find <span class="math inline">\(\mathbf{y}\)</span> is <span class="math inline">\(1-(1-.2)^t \approx .89\)</span>. Now that we reduced the false negative rate, we need to find a way to reduce the false positive rate.</p>
<p>We can reduce the false positive rate by modifying our LSH scheme one more time. Choose a positive integer <span class="math inline">\(r\)</span>. Let <span class="math inline">\(c_1, \ldots, c_r: \{0,1\}^d \rightarrow[0,1]\)</span> be independent instantiations of MinHash. Let <span class="math inline">\(g:[0,1]^r \rightarrow \{1,\ldots,m\}\)</span> be a uniform random hash function. Then define the LSH hash function is <span class="math inline">\(h(\mathbf{x}) = g(c_1(\mathbf{x}), \ldots, c_r(\mathbf{x}))\)</span>. We refer to <span class="math inline">\(r\)</span> as the number of <em>bands</em>. Now the probability that <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are in the same slot in a particular table is <span class="math display">\[\begin{align*}
\Pr(h(\mathbf{x}) = h(\mathbf{y}))
= J(\mathbf{x}, \mathbf{y})^r
+ (1-J(\mathbf{x}, \mathbf{y})^r) \frac1{m}
\end{align*}\]</span></p>
<p>The full LSH scheme now has two parameters: <span class="math inline">\(r\)</span> the number of bands and <span class="math inline">\(t\)</span> the number of tables. Changing <span class="math inline">\(r\)</span> and <span class="math inline">\(t\)</span> changes the false positive and false negative rates. If we increase the number of tables <span class="math inline">\(t\)</span>, we decrease the false negative rate and increase the false positive rate. If we decrease the number of bands <span class="math inline">\(r\)</span>, we increase the false negative rate and decrease the false positive rate.</p>
<p>Intuitively, these parameters are in tension but we can find a sweet spot that works well for each application.</p>
<p align="center">
<img src="images/scurve.png" width="400">
</p>
<p>Increasing both <span class="math inline">\(r\)</span> and <span class="math inline">\(t\)</span> gives a steep curve which is better for search but worse for space complexity.</p>
</section>
<section id="simhash" class="level3">
<h3 class="anchored" data-anchor-id="simhash">SimHash</h3>
<p>We just showed how the MinHash algorithm gives a good LSH scheme for Jaccard similarity. There are also LSH schemes for other similarity functions. We’ll now show how to design a LSH scheme for cosine similarity.</p>
<p>Cosine similarity is a measure of similarity between two vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Formally, <span class="math display">\[\begin{align*}
\cos(\theta(\mathbf{x}, \mathbf{y}))
= \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\|_2 \|\mathbf{y}\|_2}.
\end{align*}\]</span></p>
<p align="center">
<img src="images/cosine_similarity.png" width="400">
</p>
<p>We can think of cosine similarity as a natural “inverse” for Euclidean distance. Suppose <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are unit vectors. Then we have <span class="math display">\[\begin{align*}
\| \mathbf{x} - \mathbf{y} \|_2^2
&amp;= \langle \mathbf{x} - \mathbf{y}, \mathbf{x} - \mathbf{y} \rangle \\
&amp;= \| \mathbf{x} \|_2^2 + \| \mathbf{y} \|_2^2 - 2 \langle \mathbf{x}, \mathbf{y} \rangle
= 2 - 2 \cos(\theta(\mathbf{x}, \mathbf{y})).
\end{align*}\]</span></p>
<p>Let’s design a LSH scheme for cosine similarity with <span class="math inline">\(r\)</span> bands. Let <span class="math inline">\(\mathbf{g}_1, \ldots, \mathbf{g}_r \in \mathbb{R}^d\)</span> be random vectors with each entry drawn from the standard normal distribution <span class="math inline">\(\mathcal{N}(0,1)\)</span>. Let <span class="math inline">\(f: \{-1, 1\}^r \to \{1, \ldots, m\}\)</span> be a uniform random hash function. Then we define the LSH hash function <span class="math inline">\(h: \mathbb{R}^d \to \{1, \ldots, m\}\)</span> as <span class="math display">\[\begin{align*}
h(\mathbf{x}) = f([\textrm{sign}(\langle \mathbf{g}_1, \mathbf{x} \rangle), \ldots,\textrm{sign}(\langle \mathbf{g}_r, \mathbf{x} \rangle) ]).
\end{align*}\]</span></p>
<p>Let <span class="math inline">\(\theta = \theta(\mathbf{x}, \mathbf{y})\)</span>. We will show that <span class="math display">\[\begin{align*}
\Pr( h(\mathbf{x}) = h(\mathbf{y}) )
= (1 - \frac{\theta}{\pi})^r + \frac{1}{m}.
\end{align*}\]</span></p>
<p>As an intermediate result, we will show that <span class="math display">\[\begin{align*}
\Pr( \langle \mathbf{g}, \mathbf{x} \rangle = \langle \mathbf{g}, \mathbf{y} \rangle )
= 1 - \frac{\theta}{\pi}.
\end{align*}\]</span></p>
<p>We will first show the result in one dimension. Consider the random vector <span class="math inline">\(\mathbf{g}\)</span> and its hyperplane. Since it is drawn from the standard normal distribution, the direction of <span class="math inline">\(\mathbf{g}\)</span> is uniformly distributed around the unit circle. Similarly, the hyperplane is also uniformly distributed around the unit circle. The <em>sign</em> of the inner product <span class="math inline">\(\langle \mathbf{g}, \mathbf{x} \rangle\)</span> specifies which side of the hyperplane <span class="math inline">\(\mathbf{x}\)</span> is on. Intuitively, the probability that <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are on the same side of the hyperplane is proportional to their angle. We can make this formal with the following visualization.</p>
<p align="center">
<img src="images/cosine_hyperplane.png" width="400">
</p>
<p>The probability that they lie on <em>different</em> sides of the hyperplane is the probability that the random hyperplane falls between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> which is <span class="math inline">\(\frac{2\theta}{2\pi}\)</span>. Then the probability that they lie on the <em>same</em> side of the hyperplane is <span class="math inline">\(1-\frac{\theta}{\pi}\)</span>.</p>
<p>In higher dimensions, we can use the same intuition. There is always some rotation matrix <span class="math inline">\(\mathbf{U}\)</span> such that <span class="math inline">\(\mathbf{Ux}\)</span> and <span class="math inline">\(\mathbf{Uy}\)</span> are spanned by the first two standard basis vectors and have the same cosine similarity as <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Then we can apply the result in one dimension to <span class="math inline">\(\mathbf{Ux}\)</span> and <span class="math inline">\(\mathbf{Uy}\)</span>.</p>
<p align="center">
<img src="images/high_dim_1.png" width="200"> <img src="images/high_dim_2.png" width="200">
</p>
<p>We have shown how to design LSH schemes that perform well in expectation. But we are also interested in the worst case performance. Such guarantees can be proven and were actually a major driving force in the development of LSH schemes.</p>
<p><strong>Theorem (Indyk and Motwani 1998):</strong> If there exists some vector <span class="math inline">\(\mathbf{y}\)</span> with <span class="math inline">\(\|\mathbf{x} - \mathbf{y}\|_0 \leq R\)</span>, then we can return a vector <span class="math inline">\(\hat{\mathbf{y}}\)</span> with <span class="math inline">\(\|\mathbf{x} - \hat{\mathbf{y}}\|_0 \leq C \cdot R\)</span> in <span class="math inline">\(O(n^{1/C})\)</span> time and <span class="math inline">\(O(n^{1+1/C})\)</span> space.</p>



</section>
</section>

</main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function () {
  const wordsPerMinute = 200;
  const text = document.body.innerText;
  const words = text.trim().split(/\s+/).length;
  const readingTime = Math.ceil(words / wordsPerMinute);

  const readTimeEl = document.createElement("div");
  readTimeEl.innerText = `⏱️ ${readingTime} min read`;

  // Style it to appear centered
  readTimeEl.style.fontSize = "0.9em";
  readTimeEl.style.margin = "1em auto";
  readTimeEl.style.textAlign = "left";
  readTimeEl.style.width = "100%";

  const title = document.querySelector("h1");
  if (title) {
    title.parentNode.insertBefore(readTimeEl, title.nextSibling);
  }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
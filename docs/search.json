[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Description: In this class, we will explore how data science is deployed at scale. The questions we investigate will include: How do services such as Shazam recognize song clips in seconds? In settings with thousands of features, how do we find meaningful patterns? Given a social network, how can we detect clusters? And how can we use vibrations to “see” into the earth? We’ll answer these questions and more by investigating how randomization lets us efficiently leverage limited data and compute. Topics include random estimators, concentration inequalities, dimensionality reduction, singular value decomposition, spectral graph theory, and active learning.\nPrerequisites: Linear Algebra (MATH 60 or CSCI 48 or equivalent), Data Structures and Advanced Programming (CSCI 62 or equivalent).\nStructure: We will meet on Tuesdays and Thursdays from 2:45 to 4pm in Kravis 164.\nResources: This class is based on Chris Musco’s phenomenal algorithmic machine learning and data science course at NYU. While we do not have a textbook, I have prepared typed notes for every lecture; I highly recommend you read the notes before each class.\nElectronic Devices: Phones and computers are distracting to you and your peers. Please do not use them during class.\nCommunication: Please post all your course related questions on discord, either in the appropriate channel or as a direct message to me.\n\nGrading\nYour grade in the class will be based on the number of points \\(P\\) that you earn. You will receive an A if \\(P \\geq 93\\), an A- if \\(93 &gt; P \\geq 90\\), a B+ if \\(90 &gt; P \\geq 87\\), and so on. You may earn points through the following assignments:\n\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of randomly selected classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (5 minutes) and will be graded for correctness.\nExams (50 Points): The two exams will be in-person, and cover the material from the first and second halves of the course, respectively. You may bring a double-sided cheat sheet, but you will not be allowed to use any electronic devices. The exams will be graded for correctness.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: My typed notes are work in progress, and I would love your help improving them! If you find an issue in the notes on the day of the lecture or later, please open an issue on the repo. I will give extra credit to the first person to correct each typo (worth 1/4 point), and mistake (worth 1/2 point).\n\nLate Policy: Most assignments will have a no-questions-asked late policy of 24 hours (refer to Gradescope for details on each specific assignment). I will not accept assignments more than 24 hours late.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments is for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/explainable_ai.html",
    "href": "notes/explainable_ai.html",
    "title": "Explainable AI and Active Learning",
    "section": "",
    "text": "In this course, we have explored how machine learning can be used to solve tasks, from supervised learning with linear models to reinforcement learning with neural networks. The approaches we’ve discussed, especially when we throw lots of compute and data at them, tend to work very well in practice. However, as these models are deployed in the real world, it becomes increasingly important to understand their behavior. Today, we’ll explore one way to gain insights into model behavior.\nUnderstanding a model like linear regression is easy. The output of the model is a weighted linear combination of the inputs: \\[\nf(\\mathbf{x}) = \\sum_{i=1}^{d} w_i x_i.\n\\] So, changing feature \\(i\\) from baseline \\(b_i\\) to \\(x_i\\) will simply increase the output at a rate of the weight \\(w_i\\). Formally, the attribution of the change in output to feature \\(i\\) from \\(x_i\\) to \\(b_i\\) can be expressed as: \\[\n\\phi_i = w_i (x_i - b_i).\n\\] With these attribution values \\(\\phi_i\\) in hand, we can perform several safety checks. For example, it would be concerning if the attribution of race in a mortgage rate model was non-zero, or if the attribution of gender in a hiring model was non-zero.\nGeneral models (think neural networks with many layers) are unfortunately not so easy to understand. The challenge is that there are non-linear interactions between features, making it difficult to attribute changes in the output to specific input features. For example, consider a model which predicts how weather will impact the growth of plants: Precipitation is beneficial for plant growth, but only when the temperature is high enough; if it’s below freezing, precipitation will instead be harmful. Teasing apart the effects of a feature requires considering not just the feature itself, but also all the other features.\nLet \\(\\mathbf{x} \\in \\mathbb{R}^d\\) be the input to a model, and let \\(f(\\mathbf{x})\\) be the output that we’re trying to understand. We’ll consider the impact of feature \\(x_i\\) relative to a baseline \\(\\mathbf{b} \\in \\mathbb{R}^d\\). In order to tease apart this effect, we will consider all possible ways to set the other features. For a subset of features \\(S \\subseteq [d]\\), define \\(\\mathbf{x}^S\\) so that \\[\n\\mathbf{x}^S_j = \\begin{cases}\nx_i & \\text{if } j \\in S \\\\\nb_j & \\text{if } j \\notin S.\n\\end{cases}\n\\] Then, one natural way to examine the effect of \\(x_i\\) is to compare \\(f(\\mathbf{x}^{S \\cup \\{i\\}}) - f(\\mathbf{x}^S)\\) for all sets \\(S \\subseteq [d] \\setminus \\{i\\}\\). For notational convenience, define a function \\(v: 2^{[d]} \\to \\mathbb{R}\\) so that \\(v(S) = f(\\mathbf{x}^{S})\\).\nWe can think of \\(v(S)\\) as defining a value for each vertex of the \\(d\\)-dimensional hypercube. The attribution of the \\(i\\)th element can then be thought of as the average marginal change from \\(v(S)\\) to \\(v(S \\cup \\{i\\})\\) over all subsets \\(S\\). In order to define this average precisely, we need to decide how to weight the different subsets."
  },
  {
    "objectID": "notes/explainable_ai.html#shapley-values",
    "href": "notes/explainable_ai.html#shapley-values",
    "title": "Explainable AI and Active Learning",
    "section": "Shapley Values",
    "text": "Shapley Values\nThere are several desirable properties that we would like our attribution values to satisfy:\n\nNull: If a feature does not affect the output, its attribution should be zero.\nSymmetry: If two features contribute equally to the output, they should receive equal attribution.\nLinearity: If the model is a linear combination of two models, the attribution should be a linear combination of the attributions of the individual models.\nEfficiency: The total attribution across all features should equal the change in output from the baseline.\n\nThese four properties have been studied since the mid-20th century in the context of fairly distributing payouts among players in a cooperative game \\(v: 2^{[d]} \\to \\mathbb{R}\\). The Shapley value, proposed by Nobel laureate Lloyd Shapley, uniquely satisfies all four properties: \\[\n\\begin{align}\n\\phi_i &= \\frac1{d} \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{\\binom{d-1}{|S|}}\n\\\\&=\n\\underbrace{\n\\frac1{d}\n\\sum_{\\ell=0}^{d-1}\n  \\underbrace{\n    \\frac1{\\binom{d-1}{\\ell}}\n    \\sum_{\\substack{S \\subseteq [d] \\setminus \\{i\\} \\\\ |S|=\\ell}}\n      \\big( v(S \\cup \\{i\\}) - v(S) \\big)\n  }_{\\text{average over subsets of size }\\ell}\n}_{\\text{average over all subset sizes}}\n\\end{align}\n\\] Intuitively, the Shapley value is the average contribution of a feature to the model’s output, where the distribution equally weights each set size \\(\\ell\\). While different distributions will satisfy the first three properties, only the Shapley value satisfies all four. Mathematically, efficiency requires that \\[\n\\sum_{i=1}^{d} \\phi_i = v([d]) - v(\\emptyset) = f(\\mathbf{x}) - f(\\mathbf{b}).\n\\] Notably, this allows us to decompose the change in the prediction into contributions from each feature.\n\n\n\nBecause of these desirable properties, Shapley values are the de facto method of feature attribution in modern machine learning. But, because there are an exponential number of subsets, computing Shapley values exactly is computationally challenging.\n\nMonte Carlo Estimation\nOur first attempt at approximation will be with the standard Monte Carlo estimator. Any time we have a summation over many terms, we can estimate the entire sum using only a few of its terms. Consider a sampling distribution \\(\\mathcal{D}\\) over subsets \\(S \\subseteq [d] \\setminus \\{i\\}\\). We will let \\(p_S\\) be the probability of sampling subset \\(S\\) from \\(\\mathcal{D}\\). Draw \\(m\\) subsets \\(S_1, \\ldots, S_m\\) with replacement from \\(\\mathcal{D}\\).\nThe Monte Carlo estimator is \\[\n\\tilde{\\phi}_i^\\text{MC} = \\frac1{m} \\sum_{j=1}^{m} \\frac{v(S_j \\cup \\{i\\}) - v(S_j)}{d \\binom{d-1}{|S_j|} p_{S_j}}.\n\\] Often, we set the sampling probabilities to the weights i.e., \\(p_S = \\frac{1}{d \\binom{d-1}{|S|}}\\) so that the estimator can simply be written as \\[\n\\tilde{\\phi}_i^\\text{MC} = \\frac1{m} \\sum_{j=1}^{m} \\left(v(S_j \\cup \\{i\\}) - v(S_j)\\right).\n\\]\nThe expected value of our estimator, with respect to the randomness of the sampling, is \\[\n\\mathbb{E}[\\tilde{\\phi}_i^\\text{MC}]\n= \\mathbb{E}\\left[\\frac1{m} \\sum_{j=1}^{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S} \\mathbb{1}[S = S_j]\\right]\n= \\frac1{m} \\sum_{j=1}^{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S} \\mathbb{E}[\\mathbb{1}[S = S_j]]\n= \\phi_i,\n\\] where the second equality follows by the linearity of expectation, and the last equality follows because the expectation of an indicator random variable is the probability that the indicator is 1. While it’s important that the estimator is right expectation, we also care about how closely it concentrates around \\(\\phi_i\\). To this end, let’s compute the variance. We’ll use three properties of the variance:\n\nScalar Constant: If \\(c\\) is a constant, then \\(\\text{Var}(cX) = \\mathbb{E}[(cX)^2] - \\mathbb{E}[cX]^2 = c^2 \\text{Var}(X)\\).\nLinearity of Variance: If \\(X\\) and \\(Y\\) are independent random variables, then \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\). Can you prove why?\nVariance of an Indicator Random Variable: Let \\(\\mathbb{1}[A]\\) be an indicator random variable that is 1 if event \\(A\\) occurs and 0 otherwise. Then, \\(\\text{Var}(\\mathbb{1}[A]) = \\mathbb{E}[\\mathbb{1}[A]^2] - \\mathbb{E}[\\mathbb{1}[A]]^2 = \\Pr(A) - \\Pr(A)^2 \\leq \\Pr(A)\\).\n\nWith these in hand, the variance of our estimator is \\[\n\\text{Var}(\\tilde{\\phi}_i^\\text{MC})\n\\leq \\frac1{m^2} \\sum_{j=1}^{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}}\n\\left(\\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S}\\right)^2\n\\Pr(\\mathbb{1}[S_j = S])\n= \\frac1{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}}\n\\left(\\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S}\\right)^2\np_S\n\\]\nWith our choice of \\(p_S = \\frac{1}{d\\binom{d-1}{|S|}}\\), we get \\[\n\\text{Var}(\\tilde{\\phi}_i^\\text{MC})\n\\leq \\frac1{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}}\n\\frac{\\left(v(S \\cup \\{i\\}) - v(S)\\right)^2}{d\\binom{d-1}{|S|}}.\n\\]\nUsing Chebyshev’s inequality, we can bound the probability that our estimator deviates from the true value: \\[\n\\Pr\\left(|\\tilde{\\phi}_i^\\text{MC} - \\phi_i| \\geq \\epsilon\\right) \\leq \\frac{\\text{Var}(\\tilde{\\phi}_i^\\text{MC})}{\\epsilon^2}.\n\\] Setting the failure probability to \\(\\delta\\), we could solve for the number of samples \\(m\\) that we need to achieve an \\(\\epsilon\\) approximation to \\(\\phi_i\\).\nWhile an excellent (and simple) estimator, the naive Monte Carlo approach has an unfortunate drawback: Each sampled pair \\(v(S)\\) and \\(v(S \\cup \\{i\\})\\) can only be used for estimating the \\(i\\)th Shapley value \\(\\phi_i\\). In effect, we only use \\(1/d\\) of our total sample budget for each estimate.\n\n\nMaximum Sample Reuse Estimation\nA more sophisticated approach to estimating Shapley values is to rewrite the Shapley value where the subsets are not paired. Observe that \\[\n\\begin{align}\n\\phi_i &= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\} : i \\in S} \\frac{v(S)}{d \\binom{d-1}{|S|-1}}\n- \\sum_{S \\subseteq [d] \\setminus \\{i\\} : i \\notin S} \\frac{v(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} v(S) \\left(\\frac{\\mathbb{1}[i \\in S]}{d \\binom{d-1}{|S|-1}}\n- \\frac{\\mathbb{1}[i \\notin S]}{d \\binom{d-1}{|S|}}\\right).\n\\end{align}\n\\]\nA natural approach is to use each sampled set \\(S\\) to estimate every Shapley value. This Maximum Sample Reuse (MSR) estimator is\n\\[\n\\begin{align}\n\\tilde{\\phi}_i^\\text{MSR}\n&= \\frac1{m} \\sum_{j=1}^m v(S_j) \\left(\n\\frac{\\mathbb{1}[i \\in S_j]}{d \\binom{d-1}{|S_j|-1}} - \\frac{\\mathbb{1}[i \\notin S_j]}{d \\binom{d-1}{|S_j|}}\n\\right)\n\\frac{1}{p_S}.\n\\end{align}\n\\]\nA similar calculation to before shows that this estimator is also right in expectation. However, its variance depends on \\([v(S)]^2\\) rather than \\([v(S \\cup \\{i\\}) - v(S)]^2\\). In practice, we expect similar inputs \\(\\mathbf{x}^{S \\cup \\{i\\}}\\) and \\(\\mathbf{x}^{S}\\) to yield similar outputs, so the variance of the Monte Carlo estimator is generally much smaller than the variance of the Maximum Sample Reuse estimator."
  },
  {
    "objectID": "notes/explainable_ai.html#active-linear-regression",
    "href": "notes/explainable_ai.html#active-linear-regression",
    "title": "Explainable AI and Active Learning",
    "section": "Active Linear Regression",
    "text": "Active Linear Regression\nShapley values have many wonderful and surprising properties. One of them is that they are the best linear approximation to the function \\(v\\), for a certain weighting of the values \\(v(S)\\).\nFor notational convenience, suppose that \\(v(\\emptyset) =0\\). (If not, we could subtract \\(v(\\emptyset)\\) from all values to center them without change the Shapley values.) We can write the vector of Shapley values as \\[\n\\boldsymbol{\\phi} =\n\\begin{bmatrix}\n\\phi_1 \\\\\n\\phi_2 \\\\\n\\vdots \\\\\n\\phi_d\n\\end{bmatrix}\n= \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d: \\langle \\boldsymbol{\\beta}, \\mathbf{1} \\rangle = v([d])}\n\\sum_{S \\subseteq [d]: 0 &lt; |S| &lt; d}\n\\left( v(S) - \\sum_{i \\in S} \\beta_i \\right)^2 w_S,\n\\] where the weights are \\(w_S = \\frac{1}{\\binom{d}{|S|} |S| (d-|S|)}\\). Notice that the weighting in the regression problem is similar to that of the Shapley values themselves. Further, the constraint that \\(\\boldsymbol{\\beta}\\) sums to \\(v([d])\\) ensures that the efficiency property is satisfied, since we assumed that \\(v(\\emptyset) = 0\\).\nUnfortunately, proving this equality is involved, and not particularly informative. But it will be quite useful for estimating Shapley values efficiently.\nWe’ll first need to convert the constrained regression problem to an unconstrained one. Define the input matrix \\(\\mathbf{A}' \\in \\mathbb{R}^{2^d-2 \\times d}\\) so that each row corresponds to a subset \\(S \\subseteq [d]\\) where \\(0 &lt; |S| &lt; d\\), and each column corresponds to an index \\(i \\in [d]\\). The \\((S,i)\\) entry is given by \\([\\mathbf{A}']_{S,i} = \\mathbb{1}[i \\in S] \\sqrt{w_S}\\). Define the target vector \\(\\mathbf{b}' \\in \\mathbb{R}^{2^d-2}\\) so that the \\(S\\)th entry is given by \\([\\mathbf{b}']_S = v(S) \\sqrt{w_S}\\). Then we can rewrite the regression problem as \\[\n\\begin{align}\n\\boldsymbol{\\phi}\n&= \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d: \\langle \\boldsymbol{\\beta}, \\mathbf{1} \\rangle = v([d])} \\| \\mathbf{A}' \\boldsymbol{\\beta} - \\mathbf{b}' \\|^2\n\\\\& = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d: \\langle \\boldsymbol{\\beta}, \\mathbf{1} \\rangle = 0} \\| \\mathbf{A}' \\boldsymbol{\\beta} + \\mathbf{A}' \\mathbf{1} \\frac{v([d])}{d} - \\mathbf{b}' \\|^2 +  \\mathbf{1} \\frac{v([d])}{d}\n\\\\& = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d} \\| \\mathbf{A} \\boldsymbol{\\beta} - \\mathbf{b} \\|^2 + \\mathbf{1} \\frac{v([d])}{d},\n\\end{align}\n\\] where we define \\(\\mathbf{b} = \\mathbf{b'} - \\mathbf{A' 1} \\frac{v([d])}{d}\\), we define \\(\\mathbf{A}= \\mathbf{A' P}\\), and we define \\(\\mathbf{P} = \\mathbf{I} - \\frac1{d} \\mathbf{1} \\mathbf{1}^\\top\\) as the projection matrix onto the subspace orthogonal to the constant vector \\(\\mathbf{1}\\).\nNow we can apply our favorite linear regression tools.\n\nActive Learning\nLet \\[\n\\boldsymbol{\\beta}^* = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d} \\| \\mathbf{A} \\boldsymbol{\\beta} - \\mathbf{b} \\|^2.\n\\]\nThe challenge in solving this linear regression problem is similar to the challenge of directly computing the Shapley values: the input matrix and target vector are exponentially large in \\(d\\). Luckily, we can sample only a fraction of the rows, and find the best linear approximation to these.\n\n\n\nLet \\(\\boldsymbol{\\Pi} \\in \\mathbb{R}^{m \\times (2^d-2)}\\) be the sampling matrix. Each row of \\(\\boldsymbol{\\Pi}\\) corresponds to a sample. In the \\(j\\)th row, we have \\[\n[\\boldsymbol{\\Pi}]_{j,S} = \\begin{cases}\n\\frac1{\\sqrt{p_S}} & \\text{ if } S_j = S \\\\\n0 & \\text{ else}.\n\\end{cases}\n\\] The least squares solution on the sampled problem is given by: \\[\n\\tilde{\\boldsymbol{\\beta}}=\\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d}\n\\| \\boldsymbol{\\Pi} \\mathbf{A} \\boldsymbol{\\beta} - \\boldsymbol{\\Pi} \\mathbf{b} \\|_2\n= \\left( \\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{A} \\right)^{+} \\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{b}\n\\] Notice that we reweight the sampling matrix so that \\(\\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{A}\\) and \\(\\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{b}\\) are correct in expectation; that is, \\(\\mathbb{E}[ \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi}] = \\mathbf{I}\\). Because of the correlation between samples, this doesn’t actually mean that the model is correct in expectation; that is, \\(\\mathbb{E}[ \\tilde{\\boldsymbol{\\beta}}] \\neq \\boldsymbol{\\beta}^*\\).\nUnlike in most of the problems we’ve seen where sampled points are given to us in a dataset, we get to choose the sampled subsets that we use to solve the regression problem.\n\n\nLeverage Scores\nA particularly powerful tool in active learning is the use of leverage scores. Leverage scores quantify how useful a sample is for fitting the linear regression model. For samples with high leverage, excluding them can deteriorate the quality of the learned model.\n\n\n\nThe idea is to measure how “alone” a fitted sample value could be relative to the other samples. Mathematically, the leverage of a sample \\(S\\) is given by \\[\n\\ell_S = \\max_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d}\n\\frac{([\\boldsymbol{A \\beta}]_S)^2}{\\| \\boldsymbol{A \\beta} \\|_2^2}.\n\\] Notice that \\(\\ell_S\\) is defined independently of the target vector \\(\\mathbf{b}\\): It measures, over all possible learned vectors \\(\\boldsymbol{\\beta}\\), what’s the largest the entry \\([\\boldsymbol{A \\beta}]_S\\) can be relative to the overall norm of the fitted vector \\(\\boldsymbol{A \\beta}\\).\nWhen we sample according to leverage scores, we can get strong guarantees on the performance of our learned function. Roughly, with \\(m=O(d \\log(d) + \\frac{d}{\\epsilon^2})\\) samples, the solution to the approximate regression problem \\(\\boldsymbol{\\beta}\\) satisfies, with high probability, \\[\n\\| \\boldsymbol{A} \\tilde{\\boldsymbol{\\beta}} - \\boldsymbol{b} \\|_2^2 \\leq\n(1+\\epsilon) \\| \\boldsymbol{A} \\boldsymbol{\\beta}^* - \\boldsymbol{b} \\|_2^2.\n\\] That is, the fit of the learned model is close to the fit of the optimal model. We unfortunately won’t cover the proofs of these results here, but the key technical tool is applying matrix concentration inequalities.\n\n\nRegression Adjustment\nOnce we have the learned model \\(\\tilde{\\boldsymbol{\\beta}}\\), we can simply return our estimated Shapley values \\(\\tilde{\\boldsymbol{\\phi}} = \\tilde{\\boldsymbol{\\beta}} + \\mathbf{1} \\frac{v([d])}{d}\\) as our best guess for the Shapley values. While this is generally quite accurate, our linear regression estimates are not correct in expectation (because of the correlated samples when we built the \\(\\boldsymbol{\\beta}\\)).\nIf we wanted to get unbiased estimates with the same accuracy, we could use our estimates to reduce the variance of an unbiased estimator like Monte Carlo or Maximum Sample Reuse. Consider an approximation \\(\\tilde{v}: 2^{[d]} \\to \\mathbb{R}\\) defined by \\[\n\\tilde{v}(S) = \\sum_{i \\in S} \\tilde{\\phi}_i.\n\\] Let \\(\\phi_i(v)\\) be the Shapley value of the \\(i\\)th data point with respect to the function \\(v\\). Since the Shapley values are linear, we can write \\[\n\\begin{align}\n\\phi_i (v) &=\n\\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S) + \\tilde{v}(S \\cup \\{i\\}) - \\tilde{v}(S) - \\tilde{v}(S \\cup \\{i\\}) + \\tilde{v}(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{\\tilde{v}(S \\cup \\{i\\}) - \\tilde{v}(S)}{d \\binom{d-1}{|S|}}\n+ \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S) - \\tilde{v}(S \\cup \\{i\\}) + \\tilde{v}(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\phi_i(\\tilde{v}) + \\phi_i(v - \\tilde{v}).\n\\end{align}\n\\] When \\(\\tilde{v}\\) is a linear function, we can simply read off the Shapley values. It remains to estimate the Shapley values of the residual \\(\\phi_i(v - \\tilde{v})\\), for which we can use any estimator.\nRecall that the Maximum Sample Reuse estimator is both unbiased and sample efficient. The one drawback was that the variance depended on \\([v(S)]^2\\). However, when we use the estimator on the residual \\(v - \\tilde{v}\\), the variance will depend on \\([v(S) - \\tilde{v}(S)]^2\\) instead. As long as \\(\\tilde{v}\\) is a good approximation of \\(v\\), this can lead to much lower variance estimates.\n\n\nA Data Perspective\nWe motivated Shapley values as a way to understand the predictions of models, but their definition and the estimators we discussed can be applied to any function \\(v\\). One particularly relevant choice for \\(v(S)\\) is the quality of a model when trained only on data points in \\(S\\). (Typically, as you may expect, we measure quality via loss on a fixed testing set.) Just like when teasing apart the contributions of individual features, the value of a data point will change in importance depending on the context of the other data points in the training set. With this particular \\(v\\), the Shapley value \\(\\phi_i\\) tells us how valuable the \\(i\\)th data point is to the model performance. As data becomes increasingly important, understanding the contributions of individual data points can help us make more informed decisions about data collection, labeling, and model training. For example, we could use the Shapley values of this data function as a mechanism for assigning value to data owners such as newspapers or artists who created the data."
  },
  {
    "objectID": "notes/01_set_size.html",
    "href": "notes/01_set_size.html",
    "title": "Set Size Estimation",
    "section": "",
    "text": "Powered by repeated innovations in chip manufacturing, computers have grown exponentially more powerful over the last several decades. As a result, we have access to unparalleled computational resources and data. For example, a single NASA satellite collects 20 terabytes of satellite images, more than 8 billion searches are made on Google, and estimates suggest the internet creates more than 300 million terabytes of data every single day. Simultaneously, we are quickly approaching the physical limit of how many transistors can be packed on a single chip. In order to learn from the data we have and continue expanding our computational abilities into the future, fast and efficient algorithms are more important than ever.\nAt first glance, an algorithm that performs only a few operations per item in our data set is efficient. However, these algorithms can be too slow when we have lots and lots of data. Instead, we turn to randomized algorithms that can run even faster. Randomized algorithms typically exploit some source of randomness to run on only a small part of the data set (or use only a small amount of space) while still returning an approximately correct result.\nWe can run randomized algorithms in practice to see how well they work. But we also want to prove that they work and understand why. Today, we will solve a problem using randomized algorithms. Before we get to the problems and algorithms, we’ll build some helpful probability tools.\n\n\nConsider a random variable \\(X\\). For example, \\(X\\) could be the outcome of a fair dice roll and be equal to \\(1,2,3,4,5\\) or \\(6\\), each with probability \\(\\frac{1}{6}\\). Formally, we use \\(\\Pr(X=x)\\) to represent the probability that the random variable \\(X\\) is equal to the outcome \\(x\\). The expectation of a discrete random variable is \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x).\n\\] For example, the expected outcome of a fair dice roll is \\(\\mathbb{E}[X] = 1 \\times \\frac{1}{6} + 2 \\times \\frac{1}{6} + 3 \\times \\frac{1}{6} +\n4 \\times \\frac{1}{6} + 5 \\times \\frac{1}{6} + 6 \\times \\frac{1}{6} = \\frac{21}{6}\\). Note: If the random variable is continuous, we can similarly define its expected value using an integral.\nThe expected value tells us where the random variable is on average but we’re also interested in how closely the random variable concentrates around its expectation. The variance of a random variable is \\[\n\\textrm{Var}[X] = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right].\n\\] Notice that the variance is larger when the random variable is often far from its expectation. In the figure below, can you identify the expected value for each of the three distributions? Which distribution has the largest variance? Which has the smallest?\n\n\n\nThere are a number of useful facts about the expected value and variance. For example,\n\\[\n\\mathbb{E}[\\alpha X] = \\alpha \\mathbb{E}[X]\n\\hspace{1em} \\textrm{and} \\hspace{1em}\n\\textrm{Var}(\\alpha X) = \\alpha^2 \\textrm{Var}(X)\n\\] where \\(\\alpha \\in \\mathbb{R}\\) is a real number. To see this, observe that \\[\n\\mathbb{E}[\\alpha X] = \\sum_{x} \\alpha x \\Pr(X=x)\n= \\alpha \\sum_{x} x \\Pr(X=x) = \\alpha \\mathbb{E}[X]\n\\] and \\[\n\\textrm{Var}(\\alpha X) = \\sum_x (\\alpha x - \\alpha \\mathbb{E}[X])^2 = \\alpha^2 \\sum_x ( x -  \\mathbb{E}[X])^2\n= \\alpha^2 \\textrm{Var}(X).\n\\]\n\n\n\nOnce we have defined random variables, we are often interested in events defined on their outcomes. Let \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that the dice shows \\(1\\) or \\(2\\) while \\(B\\) could be the event that the dice shows an odd number. We use \\(\\Pr(A \\cap B)\\) to denote the probability that events \\(A\\) and \\(B\\) both happen. Often, we have information about one event and want to see how that changes the probability of another event. We use \\(\\Pr(A | B)\\) to denote the conditional probability of event \\(A\\) given that \\(B\\) happened. We define\n\\[\n\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nIf information about event \\(B\\) does not give us information about event \\(A\\), we say that \\(A\\) and \\(B\\) are independent. Formally, events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A|B) = \\Pr(A)\\). By the definition of conditional probability, an equivalent definition of independence is \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\).\nLet’s figure out whether the event \\(A\\) that the dice shows 1 or 2 is independent of the event \\(B\\) that the dice shows an odd number. Well, \\(\\Pr(A \\cap B) = \\frac{1}{6}\\) since the only outcome that satisfy both events is when the dice shows a 1. We also know that \\(\\Pr(A) \\Pr(B) = \\frac{2}{6} \\times \\frac{3}{6} = \\frac{1}{6}\\). So, by the second definition of independence, we can conclude that \\(A\\) and \\(B\\) are independent.\nWe’ve been talking about events defined on random variables, but we’ll also be interested in when random variables are independent. Consider random variables \\(X\\) and \\(Y\\). We say that \\(X\\) and \\(Y\\) are independent if, for all outcomes \\(x\\) and \\(y\\), \\(\\Pr(X=x \\cap Y=y) = \\Pr(X=x) \\Pr(Y=y)\\).\n\n\n\nOne of the most powerful theorems in all of probability is the linearity of expectation.\nTheorem: Let \\(X\\) and \\(Y\\) be random variables. Then \\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\] The result is a powerful tool that requires no assumptions on the random variables.\nProof: Observe that \\[\n\\mathbb{E}[X+Y] = \\sum_{x,y}(x+y) \\Pr(X=x \\cap Y=y)\n\\] Now, we’ll separate the equation into two terms and factor out the \\(x\\) and \\(y\\) terms, respectively. \\[\n= \\sum_x x \\sum_y \\Pr(X=x \\cap Y=y)\n+ \\sum_y y \\sum_x \\Pr(X=x \\cap Y=y)\n\\] Finally, using the law of total probability, we have \\[\n= \\sum_x x \\Pr(X=x) + \\sum_y y \\Pr(Y=y) = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\]\nThere are also several other useful facts about the expected value and variance.\nFact 1: When \\(X\\) and \\(Y\\) are independent, \\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\\).\nProof: Observe that \\[\n\\mathbb{E}[XY] = \\sum_{x,y} xy \\Pr(X=x \\cap Y=y)\n= \\sum_{x,y} xy \\Pr(X=x) \\Pr(Y=y)\n\\]\n\\[\n= \\sum_x x \\Pr(X=x) \\sum_y y \\Pr(Y=y)\n= \\mathbb{E}[X] \\mathbb{E}[Y]\n\\] where the second equality followed by the assumption that \\(X\\) and \\(Y\\) are independent.\nFact 2: Consider a random variable \\(X\\). Then \\(\\textrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\).\nProof: Observe that \\[\n\\textrm{Var}(X) =\n\\mathbb{E}[(X-\\mathbb{E}[X])^2]\n\\] \\[\n= \\mathbb{E}[X^2 - 2 X \\mathbb{E}[X] + \\mathbb{E}[X]^2]\n= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\] where the first equality is by definition, the second equality is by foiling, and the third equality is by linearity of expectation and the observation that \\(\\mathbb{E}[X]\\) is a scaler.\nFact 3: When \\(X\\) and \\(Y\\) are independent, \\(\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y)\\).\nProof: Observe that\n\\[\\begin{align*}\n\\textrm{Var}(X+Y) &= \\mathbb{E}\\left[(X + Y - \\mathbb{E}[X] - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[(X- \\mathbb{E}[X])^2 + 2(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) + (Y-\\mathbb{E})^2\\right] \\\\\n&= \\textrm{Var}(X) + 2\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]+ \\textrm{Var}(Y).\n\\end{align*}\\] Then, when \\(X\\) and \\(Y\\) are independent, \\[\n\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n= \\mathbb{E}[XY - \\mathbb{E}[X]Y - X\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y]] = 0\n\\] where the last equality follows by Fact 1 when \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "notes/01_set_size.html#introduction",
    "href": "notes/01_set_size.html#introduction",
    "title": "Set Size Estimation",
    "section": "",
    "text": "Powered by repeated innovations in chip manufacturing, computers have grown exponentially more powerful over the last several decades. As a result, we have access to unparalleled computational resources and data. For example, a single NASA satellite collects 20 terabytes of satellite images, more than 8 billion searches are made on Google, and estimates suggest the internet creates more than 300 million terabytes of data every single day. Simultaneously, we are quickly approaching the physical limit of how many transistors can be packed on a single chip. In order to learn from the data we have and continue expanding our computational abilities into the future, fast and efficient algorithms are more important than ever.\nAt first glance, an algorithm that performs only a few operations per item in our data set is efficient. However, these algorithms can be too slow when we have lots and lots of data. Instead, we turn to randomized algorithms that can run even faster. Randomized algorithms typically exploit some source of randomness to run on only a small part of the data set (or use only a small amount of space) while still returning an approximately correct result.\nWe can run randomized algorithms in practice to see how well they work. But we also want to prove that they work and understand why. Today, we will solve a problem using randomized algorithms. Before we get to the problems and algorithms, we’ll build some helpful probability tools.\n\n\nConsider a random variable \\(X\\). For example, \\(X\\) could be the outcome of a fair dice roll and be equal to \\(1,2,3,4,5\\) or \\(6\\), each with probability \\(\\frac{1}{6}\\). Formally, we use \\(\\Pr(X=x)\\) to represent the probability that the random variable \\(X\\) is equal to the outcome \\(x\\). The expectation of a discrete random variable is \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x).\n\\] For example, the expected outcome of a fair dice roll is \\(\\mathbb{E}[X] = 1 \\times \\frac{1}{6} + 2 \\times \\frac{1}{6} + 3 \\times \\frac{1}{6} +\n4 \\times \\frac{1}{6} + 5 \\times \\frac{1}{6} + 6 \\times \\frac{1}{6} = \\frac{21}{6}\\). Note: If the random variable is continuous, we can similarly define its expected value using an integral.\nThe expected value tells us where the random variable is on average but we’re also interested in how closely the random variable concentrates around its expectation. The variance of a random variable is \\[\n\\textrm{Var}[X] = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right].\n\\] Notice that the variance is larger when the random variable is often far from its expectation. In the figure below, can you identify the expected value for each of the three distributions? Which distribution has the largest variance? Which has the smallest?\n\n\n\nThere are a number of useful facts about the expected value and variance. For example,\n\\[\n\\mathbb{E}[\\alpha X] = \\alpha \\mathbb{E}[X]\n\\hspace{1em} \\textrm{and} \\hspace{1em}\n\\textrm{Var}(\\alpha X) = \\alpha^2 \\textrm{Var}(X)\n\\] where \\(\\alpha \\in \\mathbb{R}\\) is a real number. To see this, observe that \\[\n\\mathbb{E}[\\alpha X] = \\sum_{x} \\alpha x \\Pr(X=x)\n= \\alpha \\sum_{x} x \\Pr(X=x) = \\alpha \\mathbb{E}[X]\n\\] and \\[\n\\textrm{Var}(\\alpha X) = \\sum_x (\\alpha x - \\alpha \\mathbb{E}[X])^2 = \\alpha^2 \\sum_x ( x -  \\mathbb{E}[X])^2\n= \\alpha^2 \\textrm{Var}(X).\n\\]\n\n\n\nOnce we have defined random variables, we are often interested in events defined on their outcomes. Let \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that the dice shows \\(1\\) or \\(2\\) while \\(B\\) could be the event that the dice shows an odd number. We use \\(\\Pr(A \\cap B)\\) to denote the probability that events \\(A\\) and \\(B\\) both happen. Often, we have information about one event and want to see how that changes the probability of another event. We use \\(\\Pr(A | B)\\) to denote the conditional probability of event \\(A\\) given that \\(B\\) happened. We define\n\\[\n\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nIf information about event \\(B\\) does not give us information about event \\(A\\), we say that \\(A\\) and \\(B\\) are independent. Formally, events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A|B) = \\Pr(A)\\). By the definition of conditional probability, an equivalent definition of independence is \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\).\nLet’s figure out whether the event \\(A\\) that the dice shows 1 or 2 is independent of the event \\(B\\) that the dice shows an odd number. Well, \\(\\Pr(A \\cap B) = \\frac{1}{6}\\) since the only outcome that satisfy both events is when the dice shows a 1. We also know that \\(\\Pr(A) \\Pr(B) = \\frac{2}{6} \\times \\frac{3}{6} = \\frac{1}{6}\\). So, by the second definition of independence, we can conclude that \\(A\\) and \\(B\\) are independent.\nWe’ve been talking about events defined on random variables, but we’ll also be interested in when random variables are independent. Consider random variables \\(X\\) and \\(Y\\). We say that \\(X\\) and \\(Y\\) are independent if, for all outcomes \\(x\\) and \\(y\\), \\(\\Pr(X=x \\cap Y=y) = \\Pr(X=x) \\Pr(Y=y)\\).\n\n\n\nOne of the most powerful theorems in all of probability is the linearity of expectation.\nTheorem: Let \\(X\\) and \\(Y\\) be random variables. Then \\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\] The result is a powerful tool that requires no assumptions on the random variables.\nProof: Observe that \\[\n\\mathbb{E}[X+Y] = \\sum_{x,y}(x+y) \\Pr(X=x \\cap Y=y)\n\\] Now, we’ll separate the equation into two terms and factor out the \\(x\\) and \\(y\\) terms, respectively. \\[\n= \\sum_x x \\sum_y \\Pr(X=x \\cap Y=y)\n+ \\sum_y y \\sum_x \\Pr(X=x \\cap Y=y)\n\\] Finally, using the law of total probability, we have \\[\n= \\sum_x x \\Pr(X=x) + \\sum_y y \\Pr(Y=y) = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\]\nThere are also several other useful facts about the expected value and variance.\nFact 1: When \\(X\\) and \\(Y\\) are independent, \\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\\).\nProof: Observe that \\[\n\\mathbb{E}[XY] = \\sum_{x,y} xy \\Pr(X=x \\cap Y=y)\n= \\sum_{x,y} xy \\Pr(X=x) \\Pr(Y=y)\n\\]\n\\[\n= \\sum_x x \\Pr(X=x) \\sum_y y \\Pr(Y=y)\n= \\mathbb{E}[X] \\mathbb{E}[Y]\n\\] where the second equality followed by the assumption that \\(X\\) and \\(Y\\) are independent.\nFact 2: Consider a random variable \\(X\\). Then \\(\\textrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\).\nProof: Observe that \\[\n\\textrm{Var}(X) =\n\\mathbb{E}[(X-\\mathbb{E}[X])^2]\n\\] \\[\n= \\mathbb{E}[X^2 - 2 X \\mathbb{E}[X] + \\mathbb{E}[X]^2]\n= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\] where the first equality is by definition, the second equality is by foiling, and the third equality is by linearity of expectation and the observation that \\(\\mathbb{E}[X]\\) is a scaler.\nFact 3: When \\(X\\) and \\(Y\\) are independent, \\(\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y)\\).\nProof: Observe that\n\\[\\begin{align*}\n\\textrm{Var}(X+Y) &= \\mathbb{E}\\left[(X + Y - \\mathbb{E}[X] - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[(X- \\mathbb{E}[X])^2 + 2(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) + (Y-\\mathbb{E})^2\\right] \\\\\n&= \\textrm{Var}(X) + 2\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]+ \\textrm{Var}(Y).\n\\end{align*}\\] Then, when \\(X\\) and \\(Y\\) are independent, \\[\n\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n= \\mathbb{E}[XY - \\mathbb{E}[X]Y - X\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y]] = 0\n\\] where the last equality follows by Fact 1 when \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "notes/01_set_size.html#set-size-estimation",
    "href": "notes/01_set_size.html#set-size-estimation",
    "title": "Set Size Estimation",
    "section": "Set Size Estimation",
    "text": "Set Size Estimation\nWe’ll pose a problem that has applications in ecology, social networks, and internet indexing. However, while efficiently solving the problem is useful, our purpose is really to gain familiarity with linearity of expectation and learn Markov’s inequality.\nSuppose you run a website that is considering contracting with a company to provide CAPTCHAs for login verification. The company claims to have a database with \\(n=1000000\\) unique CAPTCHAs. For each API call, they’ll return a CAPTCHA chosen uniformly at random from their database. Here’s our problem: How many queries \\(m\\) do we need to make to their API until we can independently verify that they do in fact have a million CAPTCHAs?\nAn obvious approach is to keep calling ther API until we find a million unique CAPTCHAs. Of course, the issue is that we have to make at least a million API calls. That’s not so good if we care about efficiency, they charge us per call, or the size they claim to have in their database is much bigger than a million.\nA more clever approach is to call their API and count duplicates. Intuitively, the larger their database, the fewer duplicates we expect to see. Define a random variable \\(D_{i,j}\\) which is 1 if the \\(i\\)th and \\(j\\)th calls return the same CAPTCHA and 0 otherwise. (To avoid double counting, we’ll assume \\(i &lt; j\\).) For example, in the figure below, the \\(5\\)th, \\(6\\)th, and \\(7\\)th calls returned the same CAPTCHA so \\(D_{5,6}\\), \\(D_{5,7}\\), and \\(D_{6,7}\\) are all 1.\n\n\n\nWhen a random variable can only be 0 or 1, we call it an indicator random variable. Indicator random variables have the special property that their expected value is the probability they are 1. We can define the total number of duplicates \\(D\\) in terms of our indicator random variables \\(D_{i,j}\\).\n\\[\nD = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} D_{i,j}\n\\]\nWe can calculate the expected number of duplicates using linearity of expectation.\n\\[\n\\mathbb{E}[D] = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} \\mathbb{E}[D_{i,j}]\n\\]\nSince \\(D_{i,j}\\) is an indicator random variable, we know \\(\\mathbb{E}[D_{i,j}]\\) is the probability the \\(i\\)th and \\(j\\)th CAPTCHA are the same. Since each API call is a uniform and independent sample from the database, the probability the \\(j\\)th CAPTCHA is the same as the \\(i\\)th is \\(\\frac{1}{n}\\). With this observation in hand,\n\\[\n\\mathbb{E}[D] = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} \\frac{1}{n}\n= \\binom{m}{2} \\frac{1}{n} = \\frac{m(m-1)}{2n}.\n\\]\nSuppose we take \\(m=1000\\) queries and see \\(D=10\\) duplicates. How does this compare to what we would expect if the database had \\(n=1000000\\) CAPTCHAs?\nWell, the expectation would be \\(\\mathbb{E}[D] = \\frac{1000 \\times 999}{2 \\times 1000000} = .4995\\). Something seems wrong… we observed many more duplicates than we expect. Can we formalize this intuition?\n\nMarkov’s Inequality\nConcentration inequalities are a powerful tool in the analysis of randomized algorithms. They tell us how likely it is that a random variable differs from its expectation.\nThere are many concentration inequalities. Some apply in general and some apply only under special assumptions. The concentration inequalities that apply only under special assumptions tend to give stronger results. We’ll start with one of the most simple and general concentration inequalities.\nTheorem: For any non-negative random variable \\(X\\) and any positive threshold \\(t\\), \\[\n\\Pr(X \\geq t) \\leq \\frac{\\mathbb{E}[X]}{t}.\n\\]\nProof: We’ll prove the inequality directly. By the definition of expectation, we have \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x)\n= \\sum_{\\substack{x \\\\ x \\geq t}} x \\Pr(X=x) +\n\\sum_{\\substack{x \\\\ x &lt; t}} x \\Pr(X=x)\n\\] \\[\n\\geq \\sum_{\\substack{x \\\\ x \\geq t}} t \\Pr(X=x) + 0\n= t \\Pr(X \\geq t).\n\\] Rearranging the above inequality gives Markov’s. Can you see where we used that all outcomes \\(x\\) are non-negative?\nNow let’s apply Markov’s inequality to our set size estimation problem. Since the number of duplicates \\(D\\) is always positive, we satisfy the assumption of the inequality. \\[\n\\Pr(D \\geq 10 ) \\leq \\frac{\\mathbb{E}[D]}{10} = \\frac{.4995}{10} = .04995\n\\] The probability of observing the 10 duplicates is less than \\(5\\%\\)! We should probably start asking the CAPTCHA company some questions.\nIn practice, many of the set size estimation problems are slightly different. Instead of checking a claim about the set size, we want to estimate the set size directly. Notice that we computed \\(\\mathbb{E}[D] = \\frac{m(m-1)}{2n}\\). Rearranging, we see that \\(n = \\frac{m(m-1)}{2\\mathbb{E}[D]}\\). Given \\(m\\) samples, we can naturally build an estimator for the whole set size using the empirical number of duplicates we found in the sample. With a little more work, we can show the following.\nClaim: If we make \\(m \\geq c \\frac{\\sqrt{n}}{\\epsilon}\\) samples for a particular constant \\(c\\), then the estimate \\(\\hat{n} = \\frac{m(m-1)}{2D}\\) satisfies \\((1-\\epsilon) n \\leq \\hat{n} \\leq (1+\\epsilon) n\\) with probability \\(9/10\\)."
  },
  {
    "objectID": "notes/08_similarity_estimation.html",
    "href": "notes/08_similarity_estimation.html",
    "title": "Similarity Estimation",
    "section": "",
    "text": "We saw how, without additional structure, we expect that learning in \\(d\\) dimensions requires \\(n=2^d\\) data points. If we really had a data set that large, then the JL lemma would be vacuous since \\(\\log n = d\\).\nThe JL lemma tells us how we can preserve \\(\\ell_2\\)-norm distances between points. Let’s see how we can preserve similarity between points."
  },
  {
    "objectID": "notes/08_similarity_estimation.html#similarity-estimation",
    "href": "notes/08_similarity_estimation.html#similarity-estimation",
    "title": "Similarity Estimation",
    "section": "Similarity Estimation",
    "text": "Similarity Estimation\nLet’s consider the following problem: How do services like Shazam match a song clip against a library of more than 11 million songs in a fraction of a second? To make the problem more challenging, the song clips we match generally have additional noise like background sound in a car.\nWe know how Shazam does this because they published a white paper on their method. The key idea is to use a spectrogram to represent the song clip. A spectrogram is a 2D matrix where the rows correspond to frequencies and the columns correspond to time. We then convert the spectrogram to a fingerprint by taking the peaks in the spectrogram represented as a high-dimensional binary vector.\n\n\n\nThe process is a little more challenging because song clips may be slightly off set from the song clips in our database. In order to make the method work, they use anchor points to align the song clips. But, for our purposes, we are primarily interested in how we can compress the high-dimensional binary vector.\nOnce we have a fingerprint from a song clip, we want to search our database for similar songs. Formally, given a binary vector \\(q \\in \\{0,1\\}^d\\) representing our song clip in \\(d\\) dimensions, we want to find a nearby fingerprint \\(\\mathbf{y} \\in \\{0,1\\}^d\\) in our database. The first challenge is that our database is possibly huge with \\(O(nd)\\) bits where \\(n\\) is the number of song clips. The second challenge is that it is expensive to compute the distance between \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) with runtime \\(O(d)\\).\nIn light of these challenges, our goal is to design a more compact sketch for comparing \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). We ideally want to use \\(k \\ll d\\) space and time complexity. We will compute a compression \\(C(\\mathbf{x})\\) and \\(C(\\mathbf{y})\\) of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), respectively. As in the JL compressions, we want \\(C(\\mathbf{x})\\) and \\(C(\\mathbf{y})\\) to be close when \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are similar and far otherwise.\nWe will use the notion of Jaccard similarity to measure the similarity between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).\nJaccard Similary: The Jaccard similarity between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is \\[\\begin{align*}\nJ(\\mathbf{x}, \\mathbf{y})\n= \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}\n= \\frac{\\textrm{# non-zero entries in common}}{\\textrm{# non-zero entries in total}}.\n\\end{align*}\\]\nJaccard similarity can be applied to any data which has a natural binary representation:\n\nWe can use the “bag-of-words” model to represent a document as a binary vector where each entry is 1 if the word is in the document and 0 otherwise. Then the Jaccard similarity is the fraction of words in common between the two documents.\nWe can extract features from earthquake data for early detection. The approach is described in this paper.\nWe can compare cached web pages to determine if they are similar and avoid downloading the same content multiple times.\n\n\nMinHash Algorithm\nWe’ll use the MinHash algorithm to build the compression function \\(c:\\{0,1\\}^d \\rightarrow \\mathbb{R}^k\\).\nConsider an input \\(\\mathbf{x} \\in \\{0,1\\}^d\\). We start by choosing \\(k\\) random hash functions \\(h_i: \\{0, \\ldots, d\\} \\rightarrow [0,1]\\) for \\(i \\in [k]\\). For each hash function \\(h_i\\) we’ll compute \\[\\begin{align*}\nc_i = \\min_{j \\in \\{1, \\ldots, d\\}: \\mathbf{x}_j = 1} h_i(j).\n\\end{align*}\\] In words, we hash each index where \\(\\mathbf{x}\\) is non-zero to a random value in \\([0,1]\\). Then we take the minimum value of the hash function over all the non-zero indices. We call the compression \\(C(\\mathbf{x}) = (c_1, \\ldots, c_k)\\). We can see this process represented in the figure below.\n\n\n\nWe’ll argue that for all \\(i\\), \\[\\begin{align*}\n\\Pr \\left( c_i(\\mathbf{x}) = c_i(\\mathbf{y}) \\right)\n= J(\\mathbf{x}, \\mathbf{y})\n= \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n\\end{align*}\\]\nEvery non-zero index in \\(\\mathbf{x} \\cup \\mathbf{y}\\) is equally likely to produce the lowest hash value. We have \\(c_i(\\mathbf{x}) = c_j(\\mathbf{x})\\) if and only if the index hashed to the lowest value is 1 in both \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Since there are \\(|\\mathbf{x} \\cap \\mathbf{y}|\\) such indices, the probability that \\(c_i(\\mathbf{x}) = c_j(\\mathbf{x})\\) is the Jaccard similarity.\nInspired by this observation, the MinHash algorithm returns an estimate for the Jaccard similarity between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\): \\[\\begin{align*}\n\\tilde{J}(\\mathbf{x}, \\mathbf{y}) = \\frac1{k}\n\\sum_{i=1}^k \\mathbb{1}[c_i(\\mathbf{x}) = c_i(\\mathbf{y})].\n\\end{align*}\\] By linearity of expectation, we have \\[\\begin{align*}\n\\mathbb{E}[\\tilde{J}(\\mathbf{x}, \\mathbf{y})] = \\frac1{k} \\sum_{i=1}^k \\Pr( c_i(\\mathbf{x}) = c_i(\\mathbf{y}))\n= J(\\mathbf{x}, \\mathbf{y}).\n\\end{align*}\\] We can reduce the variance of the estimator by increasig the number of hash functions \\(k\\). The variance of the estimator is \\[\\begin{align*}\n\\textrm{Var}( \\tilde{J}(\\mathbf{x}, \\mathbf{y}) )\n&= \\frac1{k^2} \\sum_{i=1}^k \\textrm{Var}( \\mathbb{1}[c_i(\\mathbf{x}) = c_i(\\mathbf{y})] ) \\\\\n&= \\frac1{k^2} \\sum_{i=1}^k J(\\mathbf{x}, \\mathbf{y}) -  J(\\mathbf{x}, \\mathbf{y})^2\n\\leq \\frac1{k} J(\\mathbf{x}, \\mathbf{y})\n\\leq \\frac1{k}.\n\\end{align*}\\]\nHow big should we choose \\(k\\) so that the the estimate is with an additive error \\(\\epsilon\\) of its expectation with probability \\(1-\\delta\\)?\nChebyshev’s inequality tells us that \\[\\begin{align*}\n\\Pr\\left( | J(\\mathbf{x}, \\mathbf{y}) - \\tilde{J}(\\mathbf{x}, \\mathbf{y}) | \\geq \\alpha \\frac{1}{\\sqrt{k}}\\right)\n\\leq \\frac1{\\alpha^2} = \\delta\n\\end{align*}\\]\nThe additive error is \\(\\epsilon= \\alpha \\frac{1}{\\sqrt{k}}\\). Since \\(\\alpha = 1/\\sqrt{\\delta}\\), we have \\(k = \\frac1{\\epsilon^2 \\delta}\\).\nWe have shown that as long as \\(k=O(\\frac1{\\epsilon^2 \\delta})\\), then with probability \\(1-\\delta\\), \\[\\begin{align*}\nJ(\\mathbf{x}, \\mathbf{y}) - \\epsilon\n\\leq \\tilde{J} (\\mathbf{x}, \\mathbf{y})\n\\leq J(\\mathbf{x}, \\mathbf{y}) + \\epsilon.\n\\end{align*}\\] Notice that we only need \\(O(k)\\) time to compute the estimate which is independent of the original fingerprint dimension \\(d\\).\nWe can improve the result to have a \\(\\log(1/\\delta)\\) dependence using a Chernoff bound or the biased coin bound we showed in the concentration inequalities lecture.\nBiased Coin Bound: Let the probability that a coin lands heads be \\(b\\). Choose \\(k \\geq \\frac{3\\log(2/\\delta)}{\\epsilon^2}\\). If we flip a biased coin \\(k\\) times, let \\(S\\) be the number of heads. Notice that \\(\\mu = bk\\). Then \\[\n\\Pr(| S - b k | \\geq \\epsilon k) \\leq \\delta.\n\\]\nThink of the indicator random variables \\(\\mathbb{1}[c_i(\\mathbf{x}) = c_i(\\mathbf{y})]\\) as coin flips. The probability the coin lands “heads” is \\(J(\\mathbf{x}, \\mathbf{y})\\). Then we can apply the biased coin bound after dividing by \\(k\\). With probability \\(1-\\delta\\), the number of “heads” is within \\(\\epsilon\\) of \\(J(\\mathbf{x}, \\mathbf{y})\\) if we set \\(k=O(\\frac{\\log(1/\\delta)}{\\epsilon^2})\\)."
  },
  {
    "objectID": "notes/08_similarity_estimation.html#near-neighbor-search",
    "href": "notes/08_similarity_estimation.html#near-neighbor-search",
    "title": "Similarity Estimation",
    "section": "Near Neighbor Search",
    "text": "Near Neighbor Search\nOur prior goal was to preserve distances between points. But the real reason we care about preserving distances is to find nearby points. In this section, we’ll focus on the near neighbor search problem. We want to find vectors in a database \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n \\in \\mathbb{R}^d\\) that are close to an input query vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\).\nWe showed before how to improve the runtime of linear scans from \\(O(nd)\\) to \\(O(nk)\\) and the space complexity from \\(O(nd)\\) to \\(O(nk)\\). This can be helpful but we want to go even further to an algorithm which finds nearby vectors with runtime sublinear in \\(n\\).\nTo see convince yourself it is possible to find nearest neighbors in sublinear time, consider the problem in one dimension. We know how to solve this problem in \\(\\log_2 n\\) time using binary search. We can generalize this approach to higher dimensions with data structures like k-d trees. Unfortunately, the runtime is roughly \\(O(d \\min(n, 2^d))\\) which is sublinear only for \\(d=o(\\log n)\\).\nHigh-dimensional vector search is exploding as a research area because of machine-learning multi-model embeddings for images, text, and more. For example, models like CLIP allow us to embed images and text into a common space. Then we can search for images that are similar to a text query or vice versa.\n\n\n\nThere are many approaches to nearest neighbor search including spectral hashing and vector quantization. Today, we’ll focus on locality sensitive hashing (LSH). The key insight of LSH is to trade worse space complexity for better time complexity. So we’ll use \\(O(n)\\) space or more but we’ll get sublinear time complexity.\n\nLocality Sensitive Hashing\nLet \\(h: \\mathbb{R}^d \\rightarrow \\{1, \\ldots, m\\}\\) be a random hash function. Consider a similarity function \\(s\\) such as the Jaccard similarity. We call \\(h\\) locally sensitive if \\(\\Pr(h(\\mathbf{x}) = h(\\mathbf{y}))\\) is high when \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are similar and low when \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are dissimilar.\nWe will now take a first stab at designing the LSH scheme. Let \\(c: \\{0,1\\}^d \\rightarrow [0,1]\\) be a single instantiation of MinHash. Let \\(g: [0,1] \\rightarrow \\{1, \\ldots, m\\}\\) be a uniform random hash function. Then we’ll define the locally sensitive hash function \\(h(\\mathbf{x}) = g(c(\\mathbf{x}))\\).\nObserve that \\(h(\\mathbf{x}) = h(\\mathbf{y})\\) when \\(c(\\mathbf{x}) = c(\\mathbf{y})\\) or when \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) happen to randomly be mapped to the same output. In mathematical notation, we have \\[\\begin{align*}\n\\Pr(h(\\mathbf{x}) = h(\\mathbf{y}))\n&= \\Pr(c(\\mathbf{x}) = c(\\mathbf{y}))\n+ \\left( 1- \\Pr(c(\\mathbf{x})) = c(\\mathbf{y})) \\right) \\frac1{m} \\\\\n&\\leq J(\\mathbf{x}, \\mathbf{y}) + \\frac1{m}.\n\\end{align*}\\]\nThe basic approach for near neighbor search in a database has two steps. In the preprocessing step, we select a random LSH hash function \\(h: \\{0,1\\}^d \\rightarrow \\{1,\\ldots, m\\}\\). We create a table with \\(m=O(n)\\) slots. For each vector \\(\\mathbf{y}_i\\) with \\(i \\in [n]\\), we compute \\(h(\\mathbf{y}_i)\\) and store \\(\\mathbf{y}_i\\) in the corresponding slot in the table.\nIn the query step, we want to find near neighbors of \\(\\mathbf{x}\\). We compute \\(h(\\mathbf{x})\\) and look in the corresponding slot of the table. We then scan through all the vectors in the slot and compute the distance to \\(\\mathbf{x}\\). The time required is now the time to compute the distance between the query vector and each vector in the slot.\nThere are two main considerations that we want to analyze. The first is the false negative rate: What’s the probability we do not find a vector that is close to \\(\\mathbf{x}\\)? The second is the false positive rate: What’s the probability that a vector in the slot is not close to \\(\\mathbf{x}\\)?\nA higher false negative rate means we may miss a vector that is close to \\(\\mathbf{x}\\). A higher false positive rate means we increase the runtime of the query step because we have to scan through more vectors in the slot. Note that the meaning of “close” and “not close” is application dependent. For example, we may want to find anything with Jaccard similarity at least \\(.4\\) but we may not want to compare against anything with similarity less than \\(.2\\).\nWe can reduce the false negative rate by using multiple hash functions. In the preprocessing step, we now select \\(t\\) independent LSH hash functions \\(h_1, \\ldots, h_t\\). We create tables \\(T_1, \\ldots, T_t\\) each with \\(m\\) slots. For every vector \\(i \\in [n]\\) and every table \\(j\\) , we compute \\(h_1(\\mathbf{y}_i), \\ldots, h_t(\\mathbf{y}_i)\\) and store \\(\\mathbf{y}_i\\) in the corresponding slots in \\(T_1, \\ldots, T_t\\). In the query step we compute \\(h_1(\\mathbf{x}), \\ldots, h_t(\\mathbf{x})\\) and look in each one of the corresponding slots in \\(T_1, \\ldots, T_t\\).\nWe can now analyze the probability that we find a nearby vector \\(\\mathbf{y}\\) that has Jaccard similarity at least \\(.4\\). The probability that we find \\(\\mathbf{y}\\) is \\[\\begin{align*}\n1 - \\Pr(h_1(\\mathbf{x}) \\neq h_1(\\mathbf{y}))\n\\cdots\n\\Pr(h_t(\\mathbf{x}) \\neq h_t(\\mathbf{y}))\n= 1-(1-J(\\mathbf{x}, \\mathbf{y}))^t.\n\\end{align*}\\] If the Jaccard similarity is \\(.4\\) and \\(t=10\\), then the probability that we find \\(\\mathbf{y}\\) is \\(1-(1-.4)^t \\approx .99\\). But now we have a new problem which is that we may find many false positives. If the Jaccard similarity is \\(.2\\) and \\(t=10\\), then the probability that we find \\(\\mathbf{y}\\) is \\(1-(1-.2)^t \\approx .89\\). Now that we reduced the false negative rate, we need to find a way to reduce the false positive rate.\nWe can reduce the false positive rate by modifying our LSH scheme one more time. Choose a positive integer \\(r\\). Let \\(c_1, \\ldots, c_r: \\{0,1\\}^d \\rightarrow[0,1]\\) be independent instantiations of MinHash. Let \\(g:[0,1]^r \\rightarrow \\{1,\\ldots,m\\}\\) be a uniform random hash function. Then define the LSH hash function is \\(h(\\mathbf{x}) = g(c_1(\\mathbf{x}), \\ldots, c_r(\\mathbf{x}))\\). We refer to \\(r\\) as the number of bands. Now the probability that \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are in the same slot in a particular table is \\[\\begin{align*}\n\\Pr(h(\\mathbf{x}) = h(\\mathbf{y}))\n= J(\\mathbf{x}, \\mathbf{y})^r\n+ (1-J(\\mathbf{x}, \\mathbf{y})^r) \\frac1{m}\n\\end{align*}\\]\nThe full LSH scheme now has two parameters: \\(r\\) the number of bands and \\(t\\) the number of tables. Changing \\(r\\) and \\(t\\) changes the false positive and false negative rates. If we increase the number of tables \\(t\\), we decrease the false negative rate and increase the false positive rate. If we decrease the number of bands \\(r\\), we increase the false negative rate and decrease the false positive rate.\nIntuitively, these parameters are in tension but we can find a sweet spot that works well for each application.\n\n\n\nIncreasing both \\(r\\) and \\(t\\) gives a steep curve which is better for search but worse for space complexity.\n\n\nSimHash\nWe just showed how the MinHash algorithm gives a good LSH scheme for Jaccard similarity. There are also LSH schemes for other similarity functions. We’ll now show how to design a LSH scheme for cosine similarity.\nCosine similarity is a measure of similarity between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Formally, \\[\\begin{align*}\n\\cos(\\theta(\\mathbf{x}, \\mathbf{y}))\n= \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|_2 \\|\\mathbf{y}\\|_2}.\n\\end{align*}\\]\n\n\n\nWe can think of cosine similarity as a natural “inverse” for Euclidean distance. Suppose \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are unit vectors. Then we have \\[\\begin{align*}\n\\| \\mathbf{x} - \\mathbf{y} \\|_2^2\n&= \\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{x} - \\mathbf{y} \\rangle \\\\\n&= \\| \\mathbf{x} \\|_2^2 + \\| \\mathbf{y} \\|_2^2 - 2 \\langle \\mathbf{x}, \\mathbf{y} \\rangle\n= 2 - 2 \\cos(\\theta(\\mathbf{x}, \\mathbf{y})).\n\\end{align*}\\]\nLet’s design a LSH scheme for cosine similarity with \\(r\\) bands. Let \\(\\mathbf{g}_1, \\ldots, \\mathbf{g}_r \\in \\mathbb{R}^d\\) be random vectors with each entry drawn from the standard normal distribution \\(\\mathcal{N}(0,1)\\). Let \\(f: \\{-1, 1\\}^r \\to \\{1, \\ldots, m\\}\\) be a uniform random hash function. Then we define the LSH hash function \\(h: \\mathbb{R}^d \\to \\{1, \\ldots, m\\}\\) as \\[\\begin{align*}\nh(\\mathbf{x}) = f([\\textrm{sign}(\\langle \\mathbf{g}_1, \\mathbf{x} \\rangle), \\ldots,\\textrm{sign}(\\langle \\mathbf{g}_r, \\mathbf{x} \\rangle) ]).\n\\end{align*}\\]\nLet \\(\\theta = \\theta(\\mathbf{x}, \\mathbf{y})\\). We will show that \\[\\begin{align*}\n\\Pr( h(\\mathbf{x}) = h(\\mathbf{y}) )\n= (1 - \\frac{\\theta}{\\pi})^r + \\frac{1}{m}.\n\\end{align*}\\]\nAs an intermediate result, we will show that \\[\\begin{align*}\n\\Pr( \\langle \\mathbf{g}, \\mathbf{x} \\rangle = \\langle \\mathbf{g}, \\mathbf{y} \\rangle )\n= 1 - \\frac{\\theta}{\\pi}.\n\\end{align*}\\]\nWe will first show the result in one dimension. Consider the random vector \\(\\mathbf{g}\\) and its hyperplane. Since it is drawn from the standard normal distribution, the direction of \\(\\mathbf{g}\\) is uniformly distributed around the unit circle. Similarly, the hyperplane is also uniformly distributed around the unit circle. The sign of the inner product \\(\\langle \\mathbf{g}, \\mathbf{x} \\rangle\\) specifies which side of the hyperplane \\(\\mathbf{x}\\) is on. Intuitively, the probability that \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are on the same side of the hyperplane is proportional to their angle. We can make this formal with the following visualization.\n\n\n\nThe probability that they lie on different sides of the hyperplane is the probability that the random hyperplane falls between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) which is \\(\\frac{2\\theta}{2\\pi}\\). Then the probability that they lie on the same side of the hyperplane is \\(1-\\frac{\\theta}{\\pi}\\).\nIn higher dimensions, we can use the same intuition. There is always some rotation matrix \\(\\mathbf{U}\\) such that \\(\\mathbf{Ux}\\) and \\(\\mathbf{Uy}\\) are spanned by the first two standard basis vectors and have the same cosine similarity as \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Then we can apply the result in one dimension to \\(\\mathbf{Ux}\\) and \\(\\mathbf{Uy}\\).\n\n \n\nWe have shown how to design LSH schemes that perform well in expectation. But we are also interested in the worst case performance. Such guarantees can be proven and were actually a major driving force in the development of LSH schemes.\nTheorem (Indyk and Motwani 1998): If there exists some vector \\(\\mathbf{y}\\) with \\(\\|\\mathbf{x} - \\mathbf{y}\\|_0 \\leq R\\), then we can return a vector \\(\\hat{\\mathbf{y}}\\) with \\(\\|\\mathbf{x} - \\hat{\\mathbf{y}}\\|_0 \\leq C \\cdot R\\) in \\(O(n^{1/C})\\) time and \\(O(n^{1+1/C})\\) space."
  },
  {
    "objectID": "notes/13_linear_programs.html",
    "href": "notes/13_linear_programs.html",
    "title": "Linear Programs",
    "section": "",
    "text": "We discussed the ellipsoid method last time. Today, we’ll see an application of the ellipsoid method to solving linear programs.\nLinear programs are one of the most basic convex constrained, convex optimization problems. Let \\(\\mathbf{c} \\in \\mathbb{R}^d\\), \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\), and \\(\\mathbf{b} \\in \\mathbb{R}^n\\) be fixed vectors that define the problem. The goal is to find \\[\\begin{align*}\n\\min_{\\mathbf{x}} f(\\mathbf{x}) = \\mathbf{c}^\\top \\mathbf{x} \\text{ subject to } \\mathbf{Ax} \\geq \\mathbf{b}.\n\\end{align*}\\]\nLike before, we can think of the constraint \\(\\mathbf{Ax} \\geq \\mathbf{b}\\) as a union of half-space constraints \\[\n\\{\\mathbf{x} : \\mathbf{a}_i^\\top \\mathbf{x} \\geq b_i \\}\n\\] for \\(i \\in \\{1, \\ldots, n\\}\\).\nThere are many applications of linear programs including:\nLinear programs were so popular that theorem results were published as front page news.\nTheorem (Khachiyan, 1979): Assume \\(n=d\\). The ellipsoid method solves any linear program with \\(L\\)-bit integer valued constraints exactly in \\(O(n^4 L)\\) time.\nAfter the theorem was proved, the New York Times published an article titled “A Soviety Discovery Rorcks World of Mathematics” on its front page.\nIn the context of the Cold War, the result was a big deal.\nNot to be outdone, a researcher working in the United States proved the following theorem five years later which was again published by the New York Times in an article titled “Breakthrough in Problem Solving” on its front page.\nTheorem (Karmarkar, 1984): Assume \\(n=d\\). The interior point method solves any linear program with \\(L\\)-bit integer valued constraints in \\(O(n^{3.5}L\\) time.\nWe won’t cover the interior point method but you can find lecture notes here.\nThe projected gradient descent optimization algorithm initializes at some point \\(\\mathbf{x}^{(0)}\\) in the interior of the constraint space. Then, at each step \\(t\\), we compute the gradient \\(\\nabla f(\\mathbf{x}^{(t)})\\) and project the gradient onto the constraint space.\nOnce the iterate is along the constraint space, we can get stuck oscillating outside of the constraint space and only slowly converge to the optimal solution.\nThe interior point method is an alternative approach which, like its name suggests, stays in the interior of the constraint space.\nBy staying inside the constraint space, the algorithm can converge much faster.\nBoth the results for the ellipsoid method and the interior point method had a huge impact on the theory of optimization. However, neither of the algorithms are used in practice because the heuristic simplex method is much faster in practice.\nThese days, improved interior point methods compete with and often outperform the simplex method.\nPolynomial time linear programming algorithms have also had a huge impact of combinatorial optimizaiton. They are often the work horse behind approximation algorithms for NP-hard problems.\nWe’ll see an example of a combinatorial optimization problem."
  },
  {
    "objectID": "notes/13_linear_programs.html#vertex-cover",
    "href": "notes/13_linear_programs.html#vertex-cover",
    "title": "Linear Programs",
    "section": "Vertex Cover",
    "text": "Vertex Cover\nConsider a graph \\(G\\) with \\(n\\) nodes and edge set \\(E\\). Each node is assigned a weight \\(w_1, \\ldots, w_n\\).\nThe goal is to select a subset of nodes with minimum total weight that covers all edges.\n\n\n\nIn the simple example in the figure above, the optimal solution is to select the two nodes with weights \\(1\\) and \\(3\\) respectively.\nWe can turn the vertex cover problem into a linear program. Let \\(\\mathbf{x}\\) encode a solution to the vertex cover problem. That is, \\(x_i = 1\\) if node \\(i\\) is selected and \\(x_i = 0\\) otherwise.\nVertex Cover: Then the vertex cover program is \\[\\begin{align*}\n\\min_{\\mathbf{x} \\in \\{0,1\\}^n} \\sum_{i=1}^n x_i w_i\n\\text{ subject to }\nx_{i} + x_j \\geq 1 \\text{ for all }\n(i,j) \\in E.\n\\end{align*}\\]\nIt is NP-hard to solve the vertex cover problem exactly. However, we can use convex optimization to give a 2-approximation in polynomial time. The function is to minimize a linear (convex) function but the constraint is not convex because of the non-convexity of the set \\(\\{0,1\\}^n\\). We can relax the constraint to \\(\\mathbf{x} \\in [0,1]^n\\). Then the constraint is convex and we can efficiently solve the problem but the solution may not be integral.\nRelaxed Vertex Cover: The relaxed vertex cover problem is \\[\\begin{align*}\n\\min_{\\mathbf{x} \\in [0,1]^n} \\sum_{i=1}^n x_i w_i\n\\text{ subject to }\nx_{i} + x_j \\geq 1 \\text{ for all }\n(i,j) \\in E.\n\\end{align*}\\]\nOnce we produce an optimal solution to the relaxed problem, we can round the solution back to the original constraint set.\nNotice that the objective solution to the relaxed problem is always less than or equal to the objective solution to the original problem because we are consider strictly more possible solutions. Formally, let \\(\\bar{\\mathcal{S}} \\supseteq \\mathcal{S}\\) where \\(\\mathcal{S}\\) is the original constraint set and \\(\\bar{\\mathcal{S}}\\) is the relaxed constraint set. Let \\(\\mathbf{x}^* = \\arg \\min_{\\mathbf{x} \\in \\mathcal{S}} f(\\mathbf{x})\\) and \\(\\mathbf{\\bar{x}}^* = \\arg \\min_{\\mathbf{x} \\in \\bar{\\mathcal{S}}} f(\\mathbf{x})\\). We always have that \\(f(\\bar{\\mathbf{x}}^*) \\leq f(\\mathbf{x}^*)\\).\nSo the goal is to round \\(\\bar{\\mathbf{x}}^*\\) to \\(\\mathcal{S}\\) in such a way that we don’t increase the function value too much.\nWe’ll see this approach in the context of the vertex cover problem. Let \\(\\bar{\\mathbf{x}}^*\\) be an optimal solution to the relaxed vertex cover problem. Once we solve the relaxed problem, we will set \\(x_i = 1\\) if \\(\\bar{x}_i^* \\geq 1/2\\) and $x_i = 0 otherwise.\nObservation 1: All edges remain covered. That is, \\(x_i + x_j \\geq 1\\) for all \\((i,j) \\in E\\). To see this, notice that either \\(\\bar{x}_i^* \\geq 1/2\\) or \\(\\bar{x}_j^* \\geq 1/2\\) so \\(x_i + x_j \\geq 1/2 + 1/2 = 1\\).\nObservation 2: We have that \\(f(\\mathbf{x}) \\leq 2 f(\\bar{\\mathbf{x}}^*)\\). To see this, observe that \\[\\begin{align*}\nf(\\mathbf{x}) &= \\sum_{i=1}^n x_i w_i\n= \\sum_{i : \\bar{x}_i^* \\geq 1/2} w_i \\\\\n&\\leq \\sum_{i : \\bar{x}_i^* \\geq 1/2} 2 \\bar{x}_i^* w_i\n\\leq \\sum_{i} 2 \\bar{x}_i^* w_i = f(\\bar{\\mathbf{x}}^*).\n\\end{align*}\\]\nSince \\(f(\\bar{\\mathbf{x}}^*) \\leq f(\\mathbf{x}^*)\\), we have that \\(f(\\mathbf{x}) \\leq 2 f(\\mathbf{x}^*)\\).\nWe just described how a polynomial time algorithm for solving linear programs gives a polynomial time 2-approximation for the vertex cover problem. Recall that exactly solving the vertex cover problem is NP-hard. In addition, solving the vertex cover problem within a \\(\\sqrt{2}\\)-approximation factor is NP-hard by results in Pseudorandom Sets in Grassmann Graph have Near-Perfect Expansion.\nIt is widely believed that doing better than \\(2-\\epsilon\\) is NP-hard for any \\(\\epsilon &gt; 0\\). This is implied by Subhash Khot’s Unique Games Conjecture.\nEven though we don’t think it’s possible to do better than \\(2-\\epsilon\\) in polynomial time, there is an even simpler algorithm that gives a 2-approximation for the vertex cover problem that doesn’t use optimization at all."
  },
  {
    "objectID": "notes/17_sketched_regression.html",
    "href": "notes/17_sketched_regression.html",
    "title": "Sketched Regression",
    "section": "",
    "text": "Consider a matrix \\(\\mathbf{M} \\in \\mathbb{R}^{n \\times n}\\). The main idea of randomized numerical linear algebra is to compress a matrix using a randomized method such as subsampling and then solve the problem we are interested in on the compressed matrix.\nFor example, we can approximately multiply two matrices by subsampling the rows and columns of the matrices.\n\n\n\nWe can also approximately solve regression problems by subsampling the rows of the matrix.\n\n\n\nToday, we will discuss randomized approximate regression using a Johnson-Lindenstrauss matrix for compression.\nThe input is a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and a vector \\(\\mathbf{b} \\in \\mathbb{R}^n\\).\nConsider the optimal solution to the least squares problem \\[\n\\mathbf{x}^* = \\arg \\min_{\\mathbf{x} \\in \\mathbb{R}^d} \\| \\mathbf{A} \\mathbf{x} - \\mathbf{b} \\|_2^2\n\\] and the approximate solution \\[\n\\tilde{\\mathbf{x}} = \\arg \\min_{\\mathbf{x} \\in \\mathbb{R}^d} \\| \\mathbf{\\Pi} \\mathbf{A} \\mathbf{x} - \\mathbf{\\Pi} \\mathbf{b} \\|_2^2\n\\]\nwhere \\(\\mathbf{\\Pi} \\in \\mathbb{R}^{m \\times n}\\) is a Johnson-Lindenstrauss matrix with \\(m \\ll n\\) rows.\nThe goal is to show that the quality of the approximate solution is close to the quality of the optimal solution.\nRandomized Linear Regression: Let \\(\\mathbf{\\Pi}\\) be a properly scaled JL matrix (random Gaussian, random sign, sparse matrix, etc.) with \\(m = O(d/\\epsilon^2)\\) rows. Then, with probability \\(9/10\\), for any \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{b} \\in \\mathbb{R}^n\\), \\[\n\\| \\mathbf{A} \\tilde{\\mathbf{x}} - \\mathbf{b} \\|_2^2\n\\leq\n(1+\\epsilon)\n\\| \\mathbf{A} \\mathbf{x}^* - \\mathbf{b} \\|_2^2.\n\\]\nWe will prove the theorem using an \\(\\epsilon\\)-net argument. The technique is a popular approach for applying our standard concentration inequality and union bound argument to an infinite number of events. While the result we consider today is interesting, our real motivation is to explore this kind of argument because it appears in many other contexts in theoretical computer science and machine learning.\nIn order to prove the theorem, we will prove the following claim.\nClaim: For all \\(\\mathbf{x} \\in \\mathbb{R}^d\\), \\[\\begin{align*}\n(1-\\epsilon) \\| \\mathbf{A} \\mathbf{x} - \\mathbf{b} \\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{A} \\mathbf{x} - \\mathbf{\\Pi} \\mathbf{b} \\|_2^2\n\\leq (1+\\epsilon) \\| \\mathbf{A} \\mathbf{x} - \\mathbf{b} \\|_2^2.\n\\end{align*}\\]\nWith the claim, we can prove the theorem since \\[\\begin{align*}\n\\| \\mathbf{A} \\tilde{\\mathbf{x}} - \\mathbf{b} \\|_2^2\n&\\leq \\frac1{1-\\epsilon} \\| \\mathbf{\\Pi} \\mathbf{A} \\tilde{\\mathbf{x}} - \\mathbf{\\Pi} \\mathbf{b} \\|_2^2 \\\\\n& \\leq \\frac1{1-\\epsilon}  \\| \\mathbf{\\Pi A} \\mathbf{x}^* - \\mathbf{\\Pi b} \\|_2^2 \\\\\n& \\leq \\frac{1+\\epsilon}{1-\\epsilon} \\| \\mathbf{A} \\mathbf{x}^* - \\mathbf{b} \\|_2^2\n\\end{align*}\\] where the first and third inequalities are by the claim and the second inequality is by the optimality of \\(\\tilde{\\mathbf{x}}\\) for the compressed regression problem.\nRecall the following lemma that we previously proved.\nDistributional JL Lemma: If \\(\\mathbf{\\Pi}\\) is a properly scaled JL matrix with \\(m = O(\\log(1/\\delta) /\\epsilon^2)\\) rows then for any fixed \\(\\mathbf{y}\\), \\[\n(1-\\epsilon) \\| \\mathbf{y} \\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{y} \\|_2^2\n\\leq (1+\\epsilon) \\| \\mathbf{y} \\|_2^2\n\\] with probabiity \\(1-\\delta\\).\nThe challenge in going from the distributional JL lemma to the claim is that the JL lemma only holds for a fixed vector \\(\\mathbf{y}\\) whereas we need it to hold for an infinite number of vectors.\nNote that all vectors of the form \\(\\mathbf{Ax - b}\\) lie in a low dimensional space spanned by \\(d+1\\) vectors where \\(d\\) is the number of columns in \\(\\mathbf{A}\\). So even though the set is infinite, it is “simple” in some sense.\nOur goal is to prove the following theorem.\nSubspace Embedding Theorem: Let \\(\\mathcal{U} \\subset \\mathbb{R}^n\\) be a \\(d\\)-dimensional subspace. If \\(\\mathbf{\\Pi}\\) is chosen from any distribution satisfying the distributional JL Lemma, then with probability \\(1-\\delta\\), \\[\n(1-\\epsilon) \\| \\mathbf{y} \\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{y} \\|_2^2\n\\leq (1+\\epsilon) \\| \\mathbf{y} \\|_2^2\n\\] for all \\(\\mathbf{y} \\in \\mathcal{U}\\) as long as \\(m = O \\left( \\frac{d \\log(1/\\epsilon) + \\log(1/\\delta)}{\\epsilon^2} \\right)\\).\nIt is possible to obtain a slightly tighter bound of \\(O \\left( \\frac{d + \\log(1/\\delta)}{\\epsilon^2} \\right)\\) by using a more careful analysis.\nIf we can prove the subspace embedding theorem, the following corollary will follow.\nCorollary: If we choose \\(\\mathbf{\\Pi}\\) and properly scale, then with \\(O(d/\\epsilon^2)\\) rows, \\[\n(1-\\epsilon) \\| \\mathbf{Ax - b} \\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{Ax - \\Pi b} \\|_2^2\n\\leq (1+\\epsilon) \\| \\mathbf{Ax - b} \\|_2^2\n\\] for all \\(\\mathbf{x}\\) and therefore \\[\n\\| \\mathbf{A} \\tilde{\\mathbf{x}} - \\mathbf{b} \\|_2^2\n\\leq (1+O(\\epsilon)) \\| \\mathbf{A} \\mathbf{x}^* - \\mathbf{b} \\|_2^2.\n\\]\nWe can prove the corollary by applying the subspace embedding theorem to the \\(d+1\\) dimensional subspace spanned by the \\(d\\) columns of \\(\\mathbf{A}\\) and \\(\\mathbf{b}\\). Every vector \\(\\mathbf{A x -b}\\) lies in this space.\nThe first observation in proving the subspace embedding theorem is that it suffices to prove it for all vectors \\(\\mathbf{w}\\) on the unit sphere in \\(\\mathcal{U}\\). This observation follows from linearity: Any point \\(\\mathbf{v} \\in \\mathcal{U}\\) can be written as \\(c \\mathbf{w}\\) for some sclar \\(c\\) and some point \\(\\mathbf{w}\\) the sphere \\(S_{\\mathcal{U}} = \\{ \\mathbf{w} \\in \\mathcal{U} : \\| \\mathbf{w} \\|_2 = 1 \\}\\). If \\[\\begin{align*}\n(1-\\epsilon) \\| \\mathbf{w} \\|_2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{w} \\|_2\n\\leq (1+\\epsilon) \\| \\mathbf{w} \\|_2\n\\end{align*}\\] then \\[\\begin{align*}\nc(1-\\epsilon) \\| \\mathbf{w} \\|_2\n\\leq c \\| \\mathbf{\\Pi} \\mathbf{w} \\|_2\n\\leq c(1+\\epsilon) \\| \\mathbf{w} \\|_2\n\\end{align*}\\] and therefore \\[\\begin{align*}\n(1-\\epsilon) \\| c \\mathbf{w} \\|_2\n\\leq \\| \\mathbf{\\Pi} c \\mathbf{w} \\|_2\n\\leq (1+\\epsilon) \\| c \\mathbf{w} \\|_2.\n\\end{align*}\\]\nNow that we have restricted our problem to proving the theorem on the unit sphere, our intuiton is that there are not too many “different” points on a \\(d\\)-dimensional sphere.\n\n\n\nWe call \\(\\mathcal{N}_\\epsilon\\) the \\(\\epsilon\\)-net that includes a representative set of points. If we can prove that \\[\\begin{align*}\n(1-\\epsilon) \\| \\mathbf{w} \\|_2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{w} \\|_2\n\\leq (1+\\epsilon) \\| \\mathbf{w} \\|_2\n\\end{align*}\\] for all points \\(\\mathbf{w} \\in \\mathcal{N}_\\epsilon\\), we can hopefully extend the result to all points on the sphere.\nLemma (Epsilon Net): For any \\(\\epsilon \\leq 1\\), there exists a set \\(\\mathcal{N}_\\epsilon \\subset S_\\mathcal{U}\\) with \\(| \\mathcal{N}_\\epsilon | \\leq \\left( \\frac{4}{\\epsilon} \\right)^d\\) such that for all \\(\\mathbf{v} \\in \\mathcal{S}\\), \\[\n\\min_{\\mathbf{w} \\in \\mathcal{N}_\\epsilon} \\| \\mathbf{v} - \\mathbf{w} \\|_2 \\leq \\epsilon.\n\\]\nProof:\nWe will construct the \\(\\epsilon\\)-net by a greedy algorithm. Initially, the epsilon \\(\\mathcal{N}_\\epsilon\\) is empty. While there is a point \\(\\mathbf{v}\\) on the sphere that is not within \\(\\epsilon\\) of any point in \\(\\mathcal{N}_\\epsilon\\), we will add \\(\\mathbf{v}\\) to \\(\\mathcal{N}_\\epsilon\\). After running the algorithm, we will have a set \\(\\mathcal{N}_\\epsilon\\) such that \\[\n\\min_{\\mathbf{w} \\in \\mathcal{N}_\\epsilon} \\| \\mathbf{v} - \\mathbf{w} \\|_2 \\leq \\epsilon.\n\\] (Otherwise, the procedure would not have terminated.) It remains to show that the \\(\\epsilon\\)-net is not too large.\n\n\n\nWe can count the number of points in the \\(\\epsilon\\)-net by considering the volume of the sphere. Each point in the net is at least \\(\\epsilon\\) away from every other point in the net (otherwise, the point would not have been added). So we can imagine a ball of radius \\(\\frac{\\epsilon}{2}\\) around each point in the net that does not overlap with any other ball. All of these balls live in a larger ball of radius \\(1+\\frac{\\epsilon}{2}\\). Let’s count the number of non-overlapping balls of radius \\(\\frac{\\epsilon}{2}\\) that fit in the larger ball of radius \\(1+\\frac{\\epsilon}{2}\\). We know that the volume of a ball with radius \\(r\\) in \\(d\\) dimensions is \\[\n\\text{vol}(d,r) = c \\cdot r^d\n\\] where \\(c\\) is a constant that depends on the dimension but not on the radius.\nBecause all the small balls are non-overlapping and live in the larger ball, we know \\[\\begin{align*}\n\\text{vol}\\left(d,\\frac{\\epsilon}{2} \\right) | \\mathcal{N}_\\epsilon |\n&\\leq \\text{vol}(d,1+\\frac{\\epsilon}{2}) \\\\\n| \\mathcal{N}_\\epsilon | &\\leq \\frac{\\text{vol}(d,1+\\frac{\\epsilon}{2})}{\\text{vol}(d,\\frac{\\epsilon}{2})} \\\\\n&\\leq \\frac{(1+\\frac{\\epsilon}{2})^d}{(\\frac{\\epsilon}{2})^d} \\leq \\left( \\frac{3}{\\epsilon} \\right)^d\n\\end{align*}\\]\nWith this claim in hand, we will first consider how to preserve the norms of all points in the net \\(\\mathcal{N}_\\epsilon\\). Set \\(\\delta' = \\frac1{|\\mathcal{N}_\\epsilon|} \\delta\n= \\left(\\frac{\\epsilon}{4} \\right)^d \\delta\\). As long as \\(\\mathbf{\\Pi}\\) has\n\\[O \\left( \\frac{\\log(1/\\delta')}{\\epsilon^2} \\right) =\nO \\left( \\frac{d \\log(1/\\epsilon) + \\log(1/\\delta)}{\\epsilon^2} \\right)\\] rows, then by a union bound, \\[\n(1-\\epsilon) \\| \\mathbf{w} \\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{w} \\|_2^2\n\\leq (1+\\epsilon) \\| \\mathbf{w} \\|_2^2\n\\] for all \\(\\mathbf{w} \\in \\mathcal{N}_\\epsilon\\) with probability \\(1-\\delta\\).\nNext, we will extend the result to all points on the sphere. For some set of points in the net \\(\\mathbf{w}_0, \\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\in \\mathcal{N}_\\epsilon\\), any point on the sphere \\(\\mathbf{v} \\in S_\\mathcal{U}\\) can be written as \\[\n\\mathbf{v} = \\mathbf{w}_0 + c_1 \\mathbf{w}_1 + c_2 \\mathbf{w}_2 + \\ldots\n\\] for constants \\(c_1, c_2, \\ldots\\) where \\(|c_i| \\leq \\epsilon^i\\).\nWe can prove this through a greedy construction and the \\(\\epsilon\\)-net property. \\[\\begin{align*}\n\\mathbf{w}_0 &= \\arg \\min_{\\mathbf{w} \\in \\mathcal{N}_\\epsilon} \\| \\mathbf{v} - \\mathbf{w} \\|_2\n\\qquad \\mathbf{r}_0 = \\mathbf{v} - \\mathbf{w}_0 \\qquad c_1 = \\| \\mathbf{r_0} \\|_2 \\\\\n\\mathbf{w}_1 &= \\arg \\min_{\\mathbf{w} \\in \\mathcal{N}_\\epsilon} \\| \\frac{\\mathbf{r}_0}{c_1} - \\mathbf{w} \\|_2\n\\qquad \\mathbf{r}_1 = \\mathbf{v} - \\mathbf{w}_0 - c_1 \\mathbf{w}_1\n\\qquad c_2 = \\| \\mathbf{r}_1 \\|_2  \\\\\n\\mathbf{w}_2 &= \\arg \\min_{\\mathbf{w} \\in \\mathcal{N}_\\epsilon} \\| \\frac{\\mathbf{r}_1}{c_2} - \\mathbf{w} \\|_2\n\\qquad \\mathbf{r}_2 = \\mathbf{v} - \\mathbf{w}_0 - c_1 \\mathbf{w}_1 - c_2 \\mathbf{w}_2\n\\qquad c_3 = \\| \\mathbf{r}_2 \\|_2 \\\\\n&\\vdots\n\\end{align*}\\] We will inductively prove that \\(\\| \\mathbf{r}_i \\|_2 \\leq \\epsilon^i\\). To see this, observe that \\[\\begin{align*}\n\\left\\| \\frac{\\mathbf{r}_{i-1}}{c_{i}} - \\mathbf{w}_i \\right\\|_2\n\\leq \\epsilon\n\\end{align*}\\] by the property of the \\(\\epsilon\\)-net. Multiplying by \\(\\| \\mathbf{r}_{i-1} \\|_2\\) gives \\[\\begin{align*}\n\\| \\mathbf{r}_i \\|_2 =\n\\| \\mathbf{r}_{i-1} - c_i \\mathbf{w}_i \\|_2\n\\leq \\epsilon c_i = \\epsilon \\| \\mathbf{r}_{i-1} \\|_2\n\\end{align*}\\] The inductive claim follows.\nApplying triangle inequality, we have that \\[\\begin{align*}\n\\| \\mathbf{\\Pi v} \\|_2 &=\n\\| \\mathbf{\\Pi w}_0 + c_1 \\mathbf{\\Pi w}_1 + c_2 \\mathbf{\\Pi w}_2 + \\ldots \\|_2 \\\\\n&\\leq \\| \\mathbf{\\Pi w}_0 \\|_2 + c_1 \\| \\mathbf{\\Pi w}_1 \\|_2 + c_2 \\| \\mathbf{\\Pi w}_2 \\|_2 + \\ldots \\\\\n&\\leq \\| \\mathbf{\\Pi w}_0 \\|_2 + \\epsilon \\| \\mathbf{\\Pi w}_1 \\|_2 + \\epsilon^2 \\| \\mathbf{\\Pi w}_2 \\|_2 + \\ldots \\\\\n&\\leq (1+\\epsilon) \\| \\mathbf{w}_0 \\|_2 + \\epsilon (1+\\epsilon) \\| \\mathbf{w}_1 \\|_2 + \\epsilon^2 (1+\\epsilon) \\| \\mathbf{w}_2 \\|_2 + \\ldots \\\\\n&= 1 + 2 \\epsilon + 2 \\epsilon^2 + 2 \\epsilon^3 \\ldots \\\\\n&\\leq \\frac{1}{1-2\\epsilon} \\leq 1 + 4\\epsilon.\n\\end{align*}\\]\nFor the other direction, we will use that \\(\\| \\mathbf{a} + \\mathbf{b} \\|_2 \\geq \\| \\mathbf{a} \\|_2 - \\| \\mathbf{b} \\|_2\\). To see this, observe that \\(\\| \\mathbf{a} \\|_2 \\leq \\| \\mathbf{a} + \\mathbf{b} \\|_2 + \\| \\mathbf{b} \\|_2\\) by the triangle inequality applied to the vectors \\(\\mathbf{a} + \\mathbf{b}\\) and \\(-\\mathbf{b}\\). By repeatedly applying this inequality, we have that \\[\\begin{align*}\n\\| \\mathbf{\\Pi v} \\|_2 &=\n\\| \\mathbf{\\Pi w}_0 + c_1 \\mathbf{\\Pi w}_1 + c_2 \\mathbf{\\Pi w}_2 + \\ldots \\|_2 \\\\\n&\\geq \\| \\mathbf{\\Pi w}_0 \\|_2 - c_1 \\| \\mathbf{\\Pi w}_1 \\|_2 - c_2 \\| \\mathbf{\\Pi w}_2 \\|_2 - \\ldots \\\\\n&\\geq \\| \\mathbf{\\Pi w}_0 \\|_2 - \\epsilon \\| \\mathbf{\\Pi w}_1 \\|_2 - \\epsilon^2 \\| \\mathbf{\\Pi w}_2 \\|_2 - \\ldots \\\\\n&\\geq (1-\\epsilon) \\| \\mathbf{w}_0 \\|_2 - \\epsilon (1-\\epsilon) \\| \\mathbf{w}_1 \\|_2 - \\epsilon^2 (1-\\epsilon) \\| \\mathbf{w}_2 \\|_2 - \\ldots \\\\\n&= 1 - 2 \\epsilon + 2 \\epsilon^2 - 2 \\epsilon^3 \\ldots \\\\\n&\\geq \\frac{1}{1+2\\epsilon} \\geq 1 - 4\\epsilon.\n\\end{align*}\\]\nSo we have proven that \\[\\begin{align*}\n(1-O(\\epsilon)) \\| \\mathbf{v} \\|_2\n\\leq \\| \\mathbf{\\Pi v} \\|_2\n\\leq (1+O(\\epsilon)) \\| \\mathbf{v} \\|_2\n\\end{align*}\\] for all vectors \\(\\mathbf{v}\\) on the sphere. Adjusting \\(\\epsilon\\) gives the subspace embedding theorem."
  },
  {
    "objectID": "notes/12_ellipsoid_method.html",
    "href": "notes/12_ellipsoid_method.html",
    "title": "Ellipsoid Method",
    "section": "",
    "text": "When we analyzed first order optimization, we were able to get a bound on the iteration complexity that did not depend on the dimension \\(d\\). However, each iteration required us to compute the gradient which is linear in the dimension.\nAlternatively, we can get much better bounds on the number of oracle calls if we are willing to depend on the problem dimension. We actually can already see how to do this in the case of linear regression.\nLinear Regression Example: Consider the function \\(f(\\mathbf{x}) = \\frac12 \\| \\mathbf{Ax} - \\mathbf{b} \\|_2^2\\). We know the gradient is given by \\[\\nabla f(\\mathbf{x}) = \\mathbf{A}^\\top (\\mathbf{Ax} - \\mathbf{b}) = \\mathbf{A}^\\top \\mathbf{Ax } - \\mathbf{A}^\\top \\mathbf{b}\\] and the optimal solution is given by \\[\n\\mathbf{x}^* = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}.\n\\]\nWe can find the optimal solution with \\(d+1\\) calls to the gradient oracle. For the first call, query \\(\\nabla f(\\mathbf{0}) = - \\mathbf{A}^\\top \\mathbf{b}\\). Then for the remaining calls \\(i \\in \\{1, \\ldots, d\\}\\), query the standard basis vector \\(\\mathbf{e}_i\\). We can then recover the \\(i\\)th column of \\(\\mathbf{A}^\\top \\mathbf{A}\\) since \\[\nf(\\mathbf{e}_i) + \\mathbf{A}^\\top \\mathbf{b}\n= \\mathbf{A}^\\top \\mathbf{A} \\mathbf{e}_i.\n\\]\nOnce we have \\(\\mathbf{A}^\\top \\mathbf{A}\\), we can invert it (in \\(O(d^3)\\) time but without any oracle calls) and multiply it by \\(\\mathbf{A}^\\top \\mathbf{b}\\) to recover the optimal solution \\(\\mathbf{x}^*\\).\nWe will now analyze a general algorithm for convex optimization that depends on the dimension \\(d\\).\nDimension Dependent Convex Optimization: Consider a function \\(f\\) bounded between \\([-B, B]\\) on a convex set \\(\\mathcal{S}\\). The center of gravity algorithm outputs a solution \\(\\hat{\\mathbf{x}}\\) such that \\[\nf(\\hat{\\mathbf{x}}) \\leq  f(\\mathbf{x}^*) + \\epsilon\n\\] using \\(O(d \\log(B/\\epsilon))\\) calls to a function and gradient oracle for convex function \\(f\\).\nOne caveat of the algorithm is that we need some representation of \\(\\mathcal{S}\\), not just a projection oracle.\nThe center of gravity algorithm is a natural cutting plane method. Interestingly, it was developed simultaneously on opposite sides of the iron curtain by A. Y. Levin in the Soviet Union and D. J. Newman in the United States both in 1975. The algorithm is not used in practice (we will discuss why) but the basic idea underlies many popular algorithms.\nThe center of gravity algorithm uses two basic ingredients. First, the center of gravity of a convex set \\(\\mathcal{S}\\) is defined as \\[\nc = \\frac{\\int_{x \\in \\mathcal{S}} x \\, dx}{\\text{vol}(\\mathcal{S})}\n= \\frac{\\int_{x \\in \\mathcal{S}} x \\, dx}{\\int_{x \\in \\mathcal{S}} 1 dx}.\n\\] Second, for two convex sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\), the intersection \\(\\mathcal{A} \\cap \\mathcal{B}\\) is also convex. To see this, consider two points \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{A} \\cap \\mathcal{B}\\). Then for any \\(\\lambda \\in (0,1)\\), we have \\[\n\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y} \\in \\mathcal{A}\n\\] because \\(\\mathbf{x,y}\\) are in \\(\\mathcal{A}\\) and \\[\n\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y} \\in \\mathcal{B}\n\\] because \\(\\mathbf{x,y}\\) are in \\(\\mathcal{B}\\).\nThe center of gravity algorithm is as follows. Instantiate \\(\\mathcal{S}_1 = \\mathcal{S}\\). For each step \\(t=1, \\ldots, T\\), let \\(\\mathbf{c}_t\\) be the center of gravity of \\(\\mathcal{S}_t\\). We will then compute \\(\\nabla f(\\mathbf{c}_t)\\). Now define \\[\n\\mathcal{H} = \\{ \\mathbf{x} \\in \\mathbb{R}^d : \\langle \\mathbf{x} - \\mathbf{c}_t, \\nabla f(\\mathbf{c}_t) \\rangle \\leq 0 \\}.\n\\] Then update \\(\\mathcal{S}_{t+1} = \\mathcal{S}_t \\cap \\mathcal{H}\\). Finally, return the best solution \\(\\hat{\\mathbf{x}} = \\arg \\min_{t} f(\\mathbf{c}_t)\\).\nAn iteration is described graphically below. The entire blue area is the convex set \\(\\mathcal{S}_1\\). The center of gravity of \\(\\mathcal{S}_1\\) is \\(\\mathbf{c}_1\\). We compute the gradient of \\(f\\) with at \\(\\mathbf{c}_1\\) and define the halfspace \\(\\mathcal{H}\\) as everything in the direction of descent. Then the next convex set \\(\\mathcal{S}_2\\) is the intersection of \\(\\mathcal{S}_1\\) and \\(\\mathcal{H}\\).\nBy convexity, \\[\nf(\\mathbf{y}) \\geq f(\\mathbf{c}_t) + \\langle \\mathbf{y} - \\mathbf{c}_t, \\nabla f(\\mathbf{c}_t) \\rangle.\n\\] If \\(\\mathbf{y} \\notin \\{ \\mathcal{S}_t \\cap \\mathcal{H} \\}\\), then \\(\\langle \\nabla f(\\mathbf{c}_t), \\mathbf{y} - \\mathbf{c}_t \\rangle\\) is positive so \\(f(\\mathbf{y}) &gt; f(\\mathbf{c}_t)\\).\nCenter of Gravity Convergence: Let \\(f\\) be a convex function with values in \\([-B, B]\\) on a convex set \\(\\mathcal{S}\\). Then the center of gravity algorithm outputs a solution \\(\\hat{\\mathbf{x}}\\) after \\(T\\) iterations such that\n\\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*)\n\\leq 2B \\left( 1- \\frac1{e} \\right)^{T/d}\n\\leq 2B \\exp \\left( - \\frac{T}{3d} \\right).\n\\end{align*}\\] If we set \\(T = 3d \\log(B/\\epsilon)\\), then we have \\(f(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*) \\leq \\epsilon\\).\nWe want to argue that, at every step of the algorithm, we “cut off” a large portion of the convex set we are searching over.\nGrunbaum’s Theorem: For any convex set \\(\\mathcal{S}\\) with center of gravity \\(\\mathbf{c}\\) and any halfspace \\(\\mathcal{Z}= \\{\\mathbf{x} | \\langle \\mathbf{a}, \\mathbf{x} - \\mathbf{c} \\rangle \\leq 0 \\}\\), we have\n\\[\n\\frac{\\text{vol}(\\mathcal{S} \\cap \\mathcal{Z})}{\\text{vol}(\\mathcal{S})}\n\\geq \\frac{1}{e}.\n\\]\nThe extreme case of the inequality is given by a triangle.\nLet \\(\\mathcal{Z}\\) be the complement of \\(\\mathcal{H}\\) from the algorithm. Then, by Grunbaum’s Theorem, we cut off at least a \\(1/\\epsilon\\) fraction of the convex body on every iteration. After \\(t\\) steps, we have \\[\n\\text{vol}(\\mathcal{S}_t) \\leq \\left(1-\\frac1{\\epsilon}\\right)^t\n\\text{vol}(\\mathcal{S}).\n\\]\nTo connect the result to the proof of the convergence bound, let \\(\\delta &gt; 0\\) be a small parameter to be set later. Now define \\(\\mathcal{S}^\\delta = \\{ (1-\\delta) \\mathbf{x}^* + \\delta \\mathbf{x} : \\mathbf{x} \\in \\mathcal{S} \\}\\).\nClaim: Every point \\(\\mathbf{y}\\) in \\(\\mathcal{S}^\\delta\\) has good function value. To see this, observe that we have \\[\\begin{align*}\nf(\\mathbf{y}) &= f((1-\\delta) \\mathbf{x}^* + \\delta \\mathbf{x}) \\\\\n&\\leq (1-\\delta) f(\\mathbf{x}^*) + \\delta f(\\mathbf{x}) \\\\\n&\\leq f(\\mathbf{x}^*) - \\delta f(\\mathbf{x}^*) + \\delta f(\\mathbf{x}) \\\\\n& \\leq f(\\mathbf{x}^*) + 2B\\delta\n\\end{align*}\\] where the first inequality follows by Jensen’s inequality.\nWe also have that \\(\\text{vol}(\\mathcal{S}^\\delta) = \\delta^d \\text{vol}(\\mathcal{S})\\). Set \\(\\delta = (1-\\frac1{e})^{T/d}\\). After \\(T\\) steps, we have \\[\\begin{align*}\n\\text{vol}(\\mathcal{S}_t) \\leq (1-\\frac1{e})^T \\text{vol}(\\mathcal{S}).\n\\end{align*}\\]\nEither:\nIf we have chopped off at least one point in \\(\\mathbf{y} \\in \\mathcal{S}^\\delta\\) by the time we reach step \\(T\\), then we have for some centroid \\(t\\), \\(f(\\mathbf{c}_t) &lt; 2B \\delta\\).\nTo see this, observe that \\[\n2 B \\delta \\geq f(\\mathbf{y}) \\geq f(\\mathbf{c}_t) + \\langle \\nabla f(\\mathbf{c}_t), \\mathbf{y} - \\mathbf{c}_t \\rangle &gt; f(\\mathbf{c}_t)\n\\] since we require that every point \\(\\mathcal{y}\\) in the next convex set \\(\\mathcal{S}_{t+1}\\) satisfies \\(\\langle \\nabla f(\\mathbf{c}_t), \\mathbf{y} - \\mathbf{c}_t \\rangle \\leq 0\\).\nIn terms of gradient oracle complexity, the algorithm is essentially optimal. However, the algorithm is not widely used in practice. The reason is that computing the centroid is hard. In fact, computing the centroid is \\(\\#\\)P-hard even when \\(\\mathcal{S}\\) is an intersection of halfspaces.\nEven if the problem isn’t hard for the starting convex body \\(\\mathcal{S}\\), it likely will become hard for the convex body \\(\\mathcal{S} \\cap \\mathcal{H}_1 \\cap \\mathcal{H}_2 \\cap \\ldots \\cap \\mathcal{H}_T\\).\nWe will now discuss how to obtain a computationally efficient version of the center of gravity algorithm called the ellipsoid method. The ellipsoid method is most famous for giving the first polynomial time algorithm for linear programming."
  },
  {
    "objectID": "notes/12_ellipsoid_method.html#ellipsoid-method",
    "href": "notes/12_ellipsoid_method.html#ellipsoid-method",
    "title": "Ellipsoid Method",
    "section": "Ellipsoid Method",
    "text": "Ellipsoid Method\nWe’ll consider a slightly more general problem. Given a convex set \\(\\mathcal{K}\\) via access to a separation oracle \\(S_\\mathcal{K}\\) for the set, we want to determine if \\(\\mathcal{K}\\) is empty or, otherwise, return any p oint \\(\\mathbf{x} \\in \\mathcal{K}\\).\nFormally, the separation oracle returns \\[\nS_\\mathcal{K}(\\mathbf{y}) =\n\\begin{cases}\n\\emptyset & \\text{if } \\mathbf{y} \\in \\mathcal{K} \\\\\n\\text{separating hyperplane} \\mathcal{H} & \\text{if } \\mathbf{y} \\notin \\mathcal{K}.\n\\end{cases}\n\\]\nThe hyperplane is parameterized by a normal vector \\(\\mathbf{a}\\) and offset \\(c\\) so \\(\\mathcal{H} = \\{ \\mathbf{x} : \\langle \\mathbf{a}, \\mathbf{x} \\rangle \\leq c \\}\\).\n\n\n\nLet’s consider an example.\nSeparation Oracle Example: How would we implement a separation oracle for a polytope \\(\\{ \\mathbf{x} : \\mathbf{Ax} \\geq \\mathbf{b} \\}\\)? Here, \\(\\mathbf{x} \\in \\mathbb{R}^d\\), \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times d}\\), and \\(\\mathbf{b} \\in \\mathbb{R}^m\\).\nThe polytope is specified by a sequence of equations \\[\\begin{align*}\n\\mathbf{a}_1^\\top \\mathbf{x} &\\geq b_1 \\\\\n\\mathbf{a}_2^\\top \\mathbf{x} &\\geq b_2 \\\\\n\\vdots \\\\\n\\mathbf{a}_m^\\top \\mathbf{x} &\\geq b_m\n\\end{align*}\\] where \\(\\mathbf{a}_i\\) is the \\(i\\)th row of \\(\\mathbf{A}\\) and \\(b_i\\) is the \\(i\\)th entry of \\(\\mathbf{b}\\). We can check each equation to see if it is satisfied by \\(\\mathbf{x}\\) in \\(O(d)\\) time per equation. If all the equations are satisfied, we return \\(\\emptyset\\). If there is some equation \\(i\\) that is not satisfied, we return the corresponding hyperplane \\(\\mathcal{H} = \\{ \\mathbf{x} : \\mathbf{a}_i^\\top \\mathbf{x} = b_i \\}\\).\nRecall our original problem is to find \\[\n\\min_\\mathbf{x} f(\\mathbf{x}) \\text{ subject to } \\mathbf{x} \\in \\mathcal{K}.\n\\]\n\n\n\nHow can we reduce the original problem to determining if a convex set \\(\\mathcal{K}\\) is empty or not?\nThe answer is binary search! For a convex function \\(f\\), the set \\(\\{ \\mathbf{x} : f(\\mathbf{x}) \\leq c \\}\\) is convex. In addition, we can get a separation oracle via the gradient of \\(f\\). We start with an upper and lower bound \\(u\\) and \\(l\\) on the optimal solution. We will check if the convex set \\[\n\\mathcal{S} \\cap \\{ \\mathbf{x} : f(\\mathbf{x}) \\leq (u+l)/2 \\}\n\\] contains a point. We will update \\(u = (u+l)/2\\) if it does and \\(l = (u+l)/2\\) if it does not. We continue until \\(u-l \\leq \\epsilon\\).\nThe goal of the ellipsoid method is to determine whether a convex set \\(\\mathcal{K}\\) is empty or not given a separation oracle. We will assume that:\n\n\\(\\mathcal{K}\\) is bounded by a ball of radius \\(R\\) centered at some point \\(\\mathbf{c}_R\\).\nIf \\(\\mathcal{K}\\) is nonempty, then it contains a ball of radius \\(r\\) centered at some point \\(\\mathbf{c}_r\\) for some \\(r &lt; R\\).\n\n\n\n\nWe can apply the solution to the original problem with the following observation: For a convex function \\(f\\) such that \\(\\| \\nabla f(\\mathbf{x}) \\|_2 \\leq G\\) for all \\(\\mathbf{x}\\), it can be checked that the convex set \\(\\{\\mathbf{x} : f(\\mathbf{x}) \\leq \\epsilon \\}\\) contains a ball ofo radius \\(\\epsilon/G\\).\nWe’ll now sketch the ellipsoid method. The ellipsoid method is an iterative algorithm that is similar to the center of gravity algorithm. We will:\n\nCheck if the center \\(\\mathbf{c}_R\\) of the ball with radius \\(R\\) is in the current ellipsoid is in \\(\\mathcal{K}\\).\nIf it is, then we are done.\nIf it is not, then we cut the search space in half using the separating hyperplane.\n\nThe key insight of the ellipsoid method is that we can approximate the new search region with a shape that we can easily compute the centroid of. Specifically, we’ll use an ellipse! The advantage is that we can compute the centroid of an ellipse efficiently. The disadvantage is that the volume of the space we’re searching over does not decrease as quickly.\n\n\n\nIn this way, we’ll produce a sequence of ellipses that always contain \\(\\mathcal{K}\\) and decrease in volume. Once we get to an ellipse with less than the ball with center \\(\\mathbf{c}_r\\) and radius \\(r\\), we know that \\(\\mathcal{K}\\) must be empty.\nAn ellipse is a convex set of the form: \\[\n\\{ \\mathbf{ x : \\| \\mathbf{A(x-c)} \\|_2^2 \\leq \\alpha \\} }\n\\] for some constant \\(\\alpha\\) and matrix \\(\\mathbf{A}\\). The center of mass is \\(\\mathbf{c}\\).\n\n\n\nOften, we re-parameterize to say that the ellipse is \\[\n\\{\\mathbf{x} : (\\mathbf{x} - \\mathbf{c})^\\top \\mathbf{Q}^{-1} (\\mathbf{x} - \\mathbf{c}) \\leq 1 \\}.\n\\]\nLuckily, there is a closed form solution for the equation of the smallest ellipse containing a given half-ellipse. That is, let ellipse \\(\\mathbf{E}_i\\) have parameters \\(\\mathbf{Q}_i\\) and \\(\\mathbf{c}_i\\). Consider the half-ellipse: \\[\n\\mathbf{E}_i \\cap \\{ \\mathbf{x} : \\langle \\mathbf{a}_i, \\mathbf{x} \\rangle \\leq \\langle \\mathbf{a}_i, \\mathbf{c}_i \\rangle \\}.\n\\] Then the next ellipse \\(\\mathbf{E}_{i+1}\\) is the ellipse with parameters \\[\\begin{align*}\n\\mathbf{Q}_{i+1} &= \\frac{d^2}{d^2-1}\n\\left( \\mathbf{Q}_i - \\frac2{d+1} \\mathbf{hh}^\\top \\right) \\\\\n\\mathbf{c}_{i+1} = \\mathbf{c}_i - \\frac1{n+1}\\mathbf{h}\n\\end{align*}\\] where \\(\\mathbf{h} = \\sqrt{\\mathbf{a}_i^\\top \\mathbf{Q}_i \\mathbf{a}_i} \\cdot \\mathbf{a}_i\\). Importantly, computing the new ellipse takes \\(O(d^2)\\) time.\nThe next claim describes how the volume of the next ellipse decreases.\nClaim: \\[\n\\text{vol}(\\mathbf{E}_{i+1})\n\\leq \\left( 1- \\frac1{2d} \\right)\n\\text{vol}(\\mathbf{E}_i).\n\\]\nWe can prove the claim via a reduction to the “isotropic case”. You can find a proof here.\nThe result is not as good as the \\((1-\\frac1{\\epsilon})\\) constant factor volume reduction we got from the center of gravity algorithm.\nHowever, after \\(O(d)\\) iterations, we reduce the volume by a constant. In total, we require \\(O(d^2 \\log(R/r))\\) iterations to solve the problem. Formally, if we run for \\(T= d^2 \\log(B/r)\\) steps, then \\[\n\\text{vol}(\\mathbf{E}_T) \\leq \\left( \\frac{r}{R} \\right)^{d^2} \\text{vol}(\\mathbf{E}_1).\n\\] Then, since the volume \\(\\text{vol}(\\mathbf{E}_T)\\) is less than the volume of the ball with radius \\(r\\), we know that \\(\\mathcal{K}\\) is empty by our assumption.\nWe’ll see an application of the ellipsoid method to solving linear programs."
  },
  {
    "objectID": "notes/10_second_order_conditions.html",
    "href": "notes/10_second_order_conditions.html",
    "title": "Gradient Descent with Second Order Conditions",
    "section": "",
    "text": "Recall the problem we considered last class. We are given a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) with a function oracle and a gradient oracle. The function oracle returns \\(f(\\mathbf{x})\\) while the gradient oracle returns \\(\\nabla f(\\mathbf{x})\\) for any \\(\\mathbf{x} \\in \\mathbb{R}^d\\) we specify. Given these oracles, we want to find an approximate minimizer \\(\\hat{\\mathbf{x}}\\) so that\n\\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq \\min_{\\mathbf{x}} f(\\mathbf{x}) + \\epsilon.\n\\end{align*}\\]\nWe use the following prototypical gradient descent method. We choose a starting point \\(\\mathbf{x}^{(0)}\\). Then, for \\(t \\in [T]\\), we update the current iterate\n\\[\\begin{align*}\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}).\n\\end{align*}\\]\nFinally, we returned the best iterate \\(\\hat{\\mathbf{x}} = \\arg \\min_{\\{\\mathbf{x}^{(t)}\\}_{t=1}^T} f(\\mathbf{x}^{(t)})\\).\nSince the gradient is the direction of steepest descent at \\(\\mathbf{x}\\), we expect the function value to decrease for sufficiently small step sizes \\(\\eta\\). However, if \\(\\eta\\) is too small, the function value may decrease very slowly. We are interested in how quickly we can reach an approximate minimizer.\nFor the basic gradient descent analysis we performed last class, we assumed that the function \\(f\\) is convex, the function satisfies a Lipschitz condition \\(\\| \\nabla f(\\mathbf{x}) \\|_2 \\leq G\\) for all \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and the starting point is within a ball of radius \\(R\\) centered at the minimizer \\(\\mathbf{x}^*\\). The last condition ensures that we don’t start too far from the true minimizer.\nWhen we set \\(\\eta = \\frac{R}{G \\sqrt{T}}\\) and run the algorithm for \\(T \\geq \\frac{R^2 G^2}{\\epsilon^2}\\) iterations, we showed that the output of the gradient algorithm satisfied \\(f(\\hat{\\mathbf{x}}) \\leq f(\\mathbf{x}^*) + \\epsilon\\) where \\(\\mathbf{x}^*\\) is the true minimizer of \\(f\\).\nThe proof was tricky because \\(f(\\mathbf{x}^{(t)})\\) does not necessarily improve monotonically; in particular, it’s possible to overshoot the minimizer especially if the function around the minimizer is very steep.\nWe also considered the projected gradient descent algorithm where we are optimizing over a convex set \\(\\mathcal{S}\\). In addition to the function and gradient oracles, we used a projection oracle that returns \\(P_{\\mathcal{S}}(\\mathbf{x}) = \\arg \\min_{\\mathbf{y} \\in \\mathcal{S}} \\| \\mathbf{x} - \\mathbf{y}\\|_2\\). The projected gradient descent algorithm is similar to the gradient descent algorithm except that we project the update onto the set \\(\\mathcal{S}\\). In particular, \\[\\begin{align*}\n\\mathbf{x}^{(t+1)} = P_{\\mathcal{S}} \\left( \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}) \\right).\n\\end{align*}\\]\nUsing similar techniques to the gradient descent bound, we showed the same iteration bound for the projected gradient descent algorithm. The result is somewhat surprising: the projection step does not affect the iteration complexity. That is, projecting onto a convex set does not make the iterate too much worse.\nBoth the bounds in the gradient descent and projected gradient descent algorithms are actually optimal for convex first order optimization in general. But, in practice, the dependence on \\(1/\\epsilon^2\\) is pessimistic: gradient descent typically requires far fewer steps to reach an approximate minimizer.\nThe gap between theory and practice is in part because the previous bounds only made the very weak first order assumption that the function is Lipschitz. In the real world, many functions satisfy stronger assumptions. Today, we will discuss assumptions that involve the second derivative of the function."
  },
  {
    "objectID": "notes/10_second_order_conditions.html#first-order-conditions",
    "href": "notes/10_second_order_conditions.html#first-order-conditions",
    "title": "Gradient Descent with Second Order Conditions",
    "section": "",
    "text": "Recall the problem we considered last class. We are given a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) with a function oracle and a gradient oracle. The function oracle returns \\(f(\\mathbf{x})\\) while the gradient oracle returns \\(\\nabla f(\\mathbf{x})\\) for any \\(\\mathbf{x} \\in \\mathbb{R}^d\\) we specify. Given these oracles, we want to find an approximate minimizer \\(\\hat{\\mathbf{x}}\\) so that\n\\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq \\min_{\\mathbf{x}} f(\\mathbf{x}) + \\epsilon.\n\\end{align*}\\]\nWe use the following prototypical gradient descent method. We choose a starting point \\(\\mathbf{x}^{(0)}\\). Then, for \\(t \\in [T]\\), we update the current iterate\n\\[\\begin{align*}\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}).\n\\end{align*}\\]\nFinally, we returned the best iterate \\(\\hat{\\mathbf{x}} = \\arg \\min_{\\{\\mathbf{x}^{(t)}\\}_{t=1}^T} f(\\mathbf{x}^{(t)})\\).\nSince the gradient is the direction of steepest descent at \\(\\mathbf{x}\\), we expect the function value to decrease for sufficiently small step sizes \\(\\eta\\). However, if \\(\\eta\\) is too small, the function value may decrease very slowly. We are interested in how quickly we can reach an approximate minimizer.\nFor the basic gradient descent analysis we performed last class, we assumed that the function \\(f\\) is convex, the function satisfies a Lipschitz condition \\(\\| \\nabla f(\\mathbf{x}) \\|_2 \\leq G\\) for all \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and the starting point is within a ball of radius \\(R\\) centered at the minimizer \\(\\mathbf{x}^*\\). The last condition ensures that we don’t start too far from the true minimizer.\nWhen we set \\(\\eta = \\frac{R}{G \\sqrt{T}}\\) and run the algorithm for \\(T \\geq \\frac{R^2 G^2}{\\epsilon^2}\\) iterations, we showed that the output of the gradient algorithm satisfied \\(f(\\hat{\\mathbf{x}}) \\leq f(\\mathbf{x}^*) + \\epsilon\\) where \\(\\mathbf{x}^*\\) is the true minimizer of \\(f\\).\nThe proof was tricky because \\(f(\\mathbf{x}^{(t)})\\) does not necessarily improve monotonically; in particular, it’s possible to overshoot the minimizer especially if the function around the minimizer is very steep.\nWe also considered the projected gradient descent algorithm where we are optimizing over a convex set \\(\\mathcal{S}\\). In addition to the function and gradient oracles, we used a projection oracle that returns \\(P_{\\mathcal{S}}(\\mathbf{x}) = \\arg \\min_{\\mathbf{y} \\in \\mathcal{S}} \\| \\mathbf{x} - \\mathbf{y}\\|_2\\). The projected gradient descent algorithm is similar to the gradient descent algorithm except that we project the update onto the set \\(\\mathcal{S}\\). In particular, \\[\\begin{align*}\n\\mathbf{x}^{(t+1)} = P_{\\mathcal{S}} \\left( \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}) \\right).\n\\end{align*}\\]\nUsing similar techniques to the gradient descent bound, we showed the same iteration bound for the projected gradient descent algorithm. The result is somewhat surprising: the projection step does not affect the iteration complexity. That is, projecting onto a convex set does not make the iterate too much worse.\nBoth the bounds in the gradient descent and projected gradient descent algorithms are actually optimal for convex first order optimization in general. But, in practice, the dependence on \\(1/\\epsilon^2\\) is pessimistic: gradient descent typically requires far fewer steps to reach an approximate minimizer.\nThe gap between theory and practice is in part because the previous bounds only made the very weak first order assumption that the function is Lipschitz. In the real world, many functions satisfy stronger assumptions. Today, we will discuss assumptions that involve the second derivative of the function."
  },
  {
    "objectID": "notes/10_second_order_conditions.html#second-order-conditions",
    "href": "notes/10_second_order_conditions.html#second-order-conditions",
    "title": "Gradient Descent with Second Order Conditions",
    "section": "Second Order Conditions",
    "text": "Second Order Conditions\nWe will consider the second order conditions \\(\\alpha\\)-strong convexity and \\(\\beta\\)-smoothness. To define these conditions for multivariate functions, we will need to build some generality for multivariate functions. But, for scalar functions, we can describe them in terms of the second derivative. We say that that \\(f\\) is \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth if for all \\(x\\), \\[\\begin{align*}\n\\alpha \\leq f''(x) \\leq \\beta.\n\\end{align*}\\]\nThe following table gives the iteration complexity to achieve an \\(\\epsilon\\) approximate minimizer under different assumptions.\n\n\n\n\n\n\n\n\n\n\\(G\\)-Lipschitz\n\\(\\beta\\)-smooth\n\n\n\n\n\\(R\\) bounded start\n\\(O\\left( \\frac{G^2 R^2}{\\epsilon^2} \\right)\\)\n\\(O\\left( \\frac{\\beta R^2}{\\epsilon} \\right)\\)\n\n\n\\(\\alpha\\)-strong convex\n\\(O\\left( \\frac{G^2}{\\alpha \\epsilon} \\right)\\)\n\\(O\\left( \\frac{\\beta}{\\alpha} \\log(1/\\epsilon)\\right)\\)\n\n\n\nThe takeaway is that having either an upper and lower bound on the second derivative speeds up convergence. In addition, having both an upper and lower speeds up convergence even more. \nFor scalar functions, \\(\\beta\\)-smoothness is equivalent to \\[\\begin{align*}\n[f(y) - f(x)] - f'(x) (y-x) \\leq \\frac{\\beta}{2} (y-x)^2\n\\end{align*}\\] for all \\(x\\) and \\(y\\).\nWe’ll prove that \\(\\beta\\)-smoothness implies this condition to get a flavor for the proof techniques. We can write \\[\\begin{align*}\nf(y) - f(x) &= \\int_{x}^y f'(t) dt \\\\\n&\\leq \\int_{x}^y ( f'(x) + (t-x) \\beta )dt \\\\\n&= f'(x) (y-x) + \\frac{\\beta}{2} (t-x)^2 |_x^y \\\\\n&= f'(x) (y-x) + \\frac{\\beta}{2} (y-x)^2\n\\end{align*}\\] where the inequality follows because \\(f''(x) \\leq \\beta\\). In particular, \\(f'(t)\\) is below the tangent line at \\(x\\) given by \\(f'(x) + (t-x) \\beta\\) for all \\(t\\).\nSimilarly, \\(\\alpha\\)-strong convexity is equivalent to \\[\\begin{align*}\n[f(y) - f(x)] - f'(x) (y-x) \\geq \\frac{\\alpha}{2} (y-x)^2\n\\end{align*}\\] for all \\(x\\) and \\(y\\).\nIntuitively, the conditions say that the function is bounded above and below by a quadratic function.\nThe following figure illustrates the quantities in the conditions.\n\n\n\nThe definitions generalize naturally to multivariate functions. A multivariate function \\(f\\) is \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth if \\[\\begin{align*}\n\\frac{\\alpha}{2}\\| \\mathbf{y} - \\mathbf{x} \\|_2^2\n\\leq [f(\\mathbf{y}) - f(\\mathbf{x})] - \\nabla f(\\mathbf{x})^\\top (\\mathbf{y} - \\mathbf{x})\n\\leq \\frac{\\beta}{2}\\| \\mathbf{y} - \\mathbf{x} \\|_2^2\n\\end{align*}\\] for all \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).\n\nConvergence for \\(\\beta\\)-Smooth Functions\n\\(\\beta\\)-Smooth Convergence: Consider a multivariate function \\(f\\) that is \\(\\beta\\)-smooth. Suppose we run gradient descent on \\(f\\) with learning rate \\(\\eta=\\frac1{\\beta}\\) and suppose the initial point is within a ball of radius \\(R\\) centered at the minimizer \\(\\mathbf{x}^*\\). Then we can find an approximate minimizer in \\(O \\left(\\frac{G^2 R^2}{\\epsilon^2} \\right)\\) steps. Compare the bound to the \\(O \\left( \\frac{\\beta R^2}{\\epsilon} \\right)\\) steps we need if \\(f\\) were \\(G\\)-Lipschitz instead of \\(\\beta\\)-smooth.\nWhy do you think gradient descent might be faster when a function is \\(\\beta\\)-smooth? Intuitively, the function is bounded above by a quadratic function so the function value around the minima cannot be too steep.\nWe won’t show the bound but we will show part of the proof. By \\(\\beta\\)-smoothness, we have \\[\\begin{align*}\n[f(\\mathbf{x}^{(t+1)}) - f(\\mathbf{x}^{(t)})]\n- \\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t+1)} - \\mathbf{x}^{(t)})\n\\leq \\frac{\\beta}{2} \\left \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^{(t)} \\right \\|_2^2\n\\end{align*}\\] where we set \\(\\mathbf{y} = \\mathbf{x}^{(t+1)}\\) and \\(\\mathbf{x} = \\mathbf{x}^{(t)}\\) in the \\(\\beta\\)-smoothness condition. By our update, we know that \\(\\mathbf{x}^{(t+1)} - \\mathbf{x}^{(t)} = - \\frac{1}{\\beta} \\nabla f(\\mathbf{x}^{(t)})\\). Plugging in this observation twice, we get that \\[\\begin{align*}\n[f(\\mathbf{x}^{(t+1)}) - f(\\mathbf{x}^{(t)})]\n+ \\frac{1}{\\beta} \\left \\| \\nabla f(\\mathbf{x}^{(t)}) \\right \\|_2^2\n\\leq \\frac{\\beta}{2} \\left \\| \\frac{1}{\\beta} \\nabla f(\\mathbf{x}^{(t)}) \\right \\|_2^2.\n\\end{align*}\\]\nRearranging, we have that \\[\\begin{align*}\nf(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^{(t+1)})\n\\geq \\frac{1}{2 \\beta} \\left \\| \\nabla f(\\mathbf{x}^{(t)}) \\right \\|_2^2.\n\\end{align*}\\]\nIntuitively, we just used the \\(\\beta\\)-smoothness to guarantee that the function decreases by a factor that depends on the norm of the gradient. With this bound, proving the convergence result is not hard but also not obvious. A concise proof can be found on page 15 of these notes.\nWhile we did use convexity (in the notes on page 15) to prove that the function value converges to an approximate minimizer, we did not use convexity to prove the last inequality that the function value decreases by a factor that depends on the norm of the gradient. In fact, we will use the last inequality to show that non-convex but \\(\\beta\\)-smooth functions quickly converge to stationary points (that are not necessarily minimizers). Formally, we say that \\(\\mathbf{x}\\) is a stationary point of \\(f\\) if \\(\\nabla f(\\mathbf{x}) = \\mathbf{0}\\). Stationary points include local minima, global minima, local maxima, global maxima, and saddle points.\n\\(\\beta\\)-Smoooth Stationary Point Convergence: We will show that for a \\(\\beta\\)-smooth function, gradient descent converges to a stationary point in \\(O \\left( \\frac{\\beta}{\\epsilon} \\right)\\) iterations. Equivalently, after \\(T\\) steps, we can find a point \\(\\hat{\\mathbf{x}}\\) such that \\[\\begin{align*}\n\\| \\nabla f(\\hat{\\mathbf{x}}) \\|_2^2 \\leq \\frac{2 \\beta}{T}\n\\left( f(\\mathbf{x}^{(0)}) - f(\\mathbf{x}^*) \\right).\n\\end{align*}\\]\nUsing the inequality we just showed, we can use a telescoping sum to write \\[\\begin{align*}\n\\sum_{t=0}^{T-1} \\frac1{2\\beta} \\left\\| \\nabla f(\\mathbf{x}^{(t)}) \\right\\|_2^2\n\\leq f(\\mathbf{x}^{(0)}) - f(\\mathbf{T})\n\\end{align*}\\] and so, after multiplying by \\(\\frac{2\\beta}{T}\\), \\[\\begin{align*}\n\\frac{1}{T} \\sum_{t=0}^{T-1} \\left\\| \\nabla f(\\mathbf{x}^{(t)}) \\right\\|_2^2\n\\leq \\frac{2\\beta}{T} \\left( f(\\mathbf{x}^{(0)}) - f(\\mathbf{x}^*) \\right)\n\\end{align*}\\] where we used that \\(f(\\mathbf{x}^{(T)}) \\geq f(\\mathbf{x}^*)\\).\nThe claimed complexity result follows by the fact that the average of the gradient norms is at least as large as the minimum of the gradient norms. Non-convex functions are common in practice (e.g. loss functions of neural networks). We just showed that we can find a stationary point of such functions quickly using gradient descent if they are \\(\\beta\\)-smooth. Notice that stationary points do not necessarily give minimizers solutions to non-convex functions. In fact, a lot of work in optimizer design is focused on escaping stationary points that are not local minima.\n\n\nConvergence for \\(\\alpha\\)-Strongly Convex Functions\nLet’s return to \\(\\alpha\\)-strong convexity. We argued that it is possible for a \\(\\beta\\)-smooth function be non-convex. This is not true for \\(\\alpha\\)-strong functions: \\(\\alpha\\)-strong convexity implies that the function is convex.\nWe will replace our assumption about the starting point with \\(\\alpha\\)-strong convexity. The reason is that \\(\\alpha\\)-strong convexity implies that the function is bounded below by a quadratic function. This means that the function can’t be too flat; in particular, we always make good progress when we move towards the minimizer.\nFor \\(\\alpha\\)-strongly convex functions, we adaptively modify the gradient descent learning rate to be \\(\\eta_t = \\frac{2}{\\alpha (t+1)}\\).\n\\(\\alpha\\)-Strongly Convex Convergence: If we run gradient descent with this learning rate on an \\(\\alpha\\)-strongly convex and \\(G\\)-Lipschitz function for \\(T\\) steps, the output \\(\\hat{\\mathbf{x}}\\) satisfies \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*) \\leq \\frac{2 G^2}{\\alpha T}.\n\\end{align*}\\]\nEquivalently, the output is an \\(\\epsilon\\)-approximate minimizer after \\(T = O\\left(\\frac{G^2}{\\alpha \\epsilon} \\right)\\) iterations.\n\\(\\alpha\\)-Strongly Convex and \\(\\beta\\)-Smooth Convergence: If we run gradient with on an \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth function for \\(T\\) steps, the output \\(\\hat{\\mathbf{x}}\\) satisfies \\[\\begin{align*}\n\\| \\mathbf{x}^{(T)} - \\mathbf{x}^* \\|_2^2\n\\leq e^{-T \\frac{\\alpha}{\\beta}} \\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2.\n\\end{align*}\\] We call \\(\\kappa = \\beta/\\alpha\\) the condition number of \\(f\\). Since \\(\\kappa\\) is the ratio of the upper and lower bounds on the second derivative, it is a measure of how closely the function is sandwiched between two quadratic functions. As seen in the bound, a smaller condition number implies faster convergence.\nThe bound we just stated looks slightly different from the bounds we discussed before. We can convert the bound to a more familiar form by plugging in \\(\\mathbf{y} = \\mathbf{x}^{(T)}\\) and \\(\\mathbf{x} = \\mathbf{x}^*\\) into the \\(\\beta\\)-smoothness and \\(\\alpha\\)-strong convexity conditions:\n\\[\\begin{align*}\n  {\\frac{\\alpha}{2}}\\|\\mathbf{x} - \\mathbf{y}\\|_2^2 \\leq  \\left[f(\\mathbf{y}) - f(\\mathbf{x})\\right] - \\nabla f(\\mathbf{x})^\\top(\\mathbf{y} - \\mathbf{x}) \\leq {\\frac{\\beta}{2}}\\|\\mathbf{x} - \\mathbf{y}\\|_2^2.\n\\end{align*}\\]\nSince \\(\\nabla f(\\mathbf{x}^*) = \\mathbf{0}\\), we have that\n\\[\\begin{align*}\n  \\|\\mathbf{x}^{(T)} - \\mathbf{x}^*\\|_2^2 &\\geq \\frac{2}{\\beta} \\left[f(\\mathbf{x}^{(T)}) - f(\\mathbf{x}^*)\\right].\n\\end{align*}\\]\nNow we can restate the prior bound.\n\\(\\alpha\\)-Strongly Convex and \\(\\beta\\)-Smooth Convergence: Let \\(f\\) be a \\(\\beta\\)-smooth and \\(\\alpha\\)-strongly convex function. If we run gradient descent for \\(T\\) steps with step size \\(\\eta = \\frac{1}{\\beta}\\) we have: \\[\\begin{align*}\n  f(\\mathbf{x}^{(T)}) - f(\\mathbf{x}^*)  \\leq \\frac{\\beta}{2} e^{-T\\frac{\\alpha}{\\beta}} \\cdot  R^2\n\\end{align*}\\] where the starting point is within a ball of radius \\(R\\) centered at the minimizer \\(\\mathbf{x}^*\\). Equivalently, if \\(T = O\\left(\\frac{\\beta}{\\alpha}\\log(R\\beta/\\epsilon)\\right)\\) we have, \\[\\begin{align*}\n  f({\\mathbf{x}}^{(T)}) - f(\\mathbf{x}^*) \\leq \\epsilon.\n\\end{align*}\\]\n\n\nLinear Regression Loss\nWe won’t prove this bound in general, but we will prove it for the special case of linear regression loss where \\(f(\\mathbf{x}) = \\frac12 \\| \\mathbf{Ax} - \\mathbf{b} \\|_2^2\\). Our goal will be to showcase some key ideas, introduce concepts like the Hessian, and demonstrate the connection between conditioning and linear algebra.\nLet \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) be a twice differentiable function. We will define the Hessian \\(\\mathbf{H(x)} = \\nabla^2 f(\\mathbf{x})\\) to contain all partial second derivatives at a point \\(\\mathbf{x}\\). In particular, let the entry in the \\(i\\)th row and \\(j\\)th column of the Hessian be given by \\[\\begin{align*}\n\\left[ \\nabla^2 f(\\mathbf{x}) \\right]_{i,j}\n= \\frac{\\partial^2 f}{\\partial x_i, x_j}.\n\\end{align*}\\]\nFor vectors \\(\\mathbf{x}\\) and \\(\\mathbf{v}\\) and a small scalar \\(t\\), we can approximate the gradient with the Hessian. Formally, \\[\\begin{align*}\n\\nabla f(\\mathbf{x} + t \\mathbf{v})\n\\approx \\nabla f(\\mathbf{x}) + t \\nabla^2 f(\\mathbf{x}) \\mathbf{v}.\n\\end{align*}\\]\nLet’s compute the Hessian for our example function \\[f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{b}\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n \\left(\\mathbf{x}^\\top\\mathbf{a}^{(i)} - {b}^{(i)}\\right)^2\\] where \\(\\mathbf{a}^{(i)}\\) is the \\(i\\)th row of \\(\\mathbf{A}\\) and \\({b}^{(i)}\\) is the \\(i\\)th entry of \\(\\mathbf{b}\\). We have \\[\\begin{align*}\n  \\frac{\\partial f}{\\partial x_k} &= \\frac{1}{2}\\sum_{i=1}^n 2\\left(\\mathbf{x}^\\top\\mathbf{a}^{(i)} - {b}^{(i)}\\right)\\cdot a^{(i)}_k \\\\\n  \\frac{\\partial^2 f}{\\partial x_j\\partial x_k} &= \\sum_{i=1}^n a^{(i)}_j  a^{(i)}_k.\n\\end{align*}\\] Therefore the Hessian is given by \\(\\mathbf{H} = \\mathbf{A}^\\top\\mathbf{A}\\).\nWe can also see the Hessian more directly. Recall that \\(\\nabla f(\\mathbf{x}) = \\mathbf{A}^\\top (\\mathbf{A} \\mathbf{x} - \\mathbf{b})\\). Then we can write \\[\\begin{align*}\n\\nabla f(\\mathbf{x} + t \\mathbf{v})\n&= \\mathbf{A}^\\top (\\mathbf{A} (\\mathbf{x} + t \\mathbf{v}) - \\mathbf{b}) \\\\\n&= \\mathbf{A}^\\top (\\mathbf{A} \\mathbf{x} - \\mathbf{b}) + t \\mathbf{A}^\\top \\mathbf{A} \\mathbf{v}.\n\\end{align*}\\] By our vector definition of the Hessian, we can see that \\(\\mathbf{H} = \\mathbf{A}^\\top \\mathbf{A}\\).\nFor scalar functions, we saw that\n\n\\(f\\) is convex if \\(f''(x) \\geq 0\\) for all \\(x\\),\n\\(f\\) is \\(\\alpha\\)-strongly convex if \\(f''(x) \\geq \\alpha\\) for all \\(x\\), and\n\\(f\\) is \\(\\beta\\)-smooth if \\(f''(x) \\leq \\beta\\) for all \\(x\\).\n\nWe would like to generalize these properties to the case where \\(f\\) is multivariate and the second derivative is a matrix \\(\\mathbf{H}\\).\nWe will start with a notion of positivity for matrices and relate it to convexity for multivariate functions.\nPositive Semidefinite (PSD): A square, symemtric matrix \\(\\mathbf{H}\\) is PSD if \\(\\mathbf{v}^\\top \\mathbf{H} \\mathbf{v} \\geq 0\\) for all \\(\\mathbf{v}\\).\nThe PSD property is a natural notion of “positivity” for symmetric matrices. To denote that \\(\\mathbf{H}\\) is PSD, we write \\(\\mathbf{H} \\succeq 0\\) where “\\(\\succeq\\)” denotes the Loewner order. We can write \\(\\mathbf{B} \\succeq \\mathbf{A}\\) to denote that \\(\\mathbf{B} - \\mathbf{A} \\succeq 0\\). This gives a partial ordering on matrices (there some matrices that are incomparable under the Loewner order).\nClaim: If \\(f\\) is twice differentiable, then it is convex if and only if \\(\\mathbf{H}\\) is positive semidefinite for all \\(\\mathbf{x}\\).\nWe can check that our loss function is convex by showing the Hessian is PSD. In particular, we have that \\[\\begin{align*}\n\\mathbf{v}^\\top \\mathbf{H} \\mathbf{v}\n= \\mathbf{v}^\\top \\mathbf{A}^\\top \\mathbf{A} \\mathbf{v}\n= \\| \\mathbf{A} \\mathbf{v} \\|_2^2 \\geq 0\n\\end{align*}\\] for any vector \\(\\mathbf{v}\\).\nWe can also use the Loewner order to generalize the definitions of \\(\\alpha\\)-strong convexity and \\(\\beta\\)-smoothness. If \\(f\\) is \\(\\beta\\)-smooth and \\(\\alpha\\)-strongly convex, then \\[\\begin{align*}\n\\alpha \\mathbf{I} \\preceq \\mathbf{H} \\preceq \\beta \\mathbf{I}\n\\end{align*}\\] where \\(\\mathbf{I}\\) is the \\(d \\times d\\) identity matrix.\nNotice that this is a natural generalization of the scalar definitions of \\(\\alpha\\)-strong convexity and \\(\\beta\\)-smoothness where \\[\\begin{align*}\n\\alpha \\leq f''(x) \\leq \\beta.\n\\end{align*}\\]\nEquivalently, for any \\(z\\), we have that \\(\\alpha \\| \\mathbf{z} \\|_2^2 \\leq \\mathbf{z}^\\top \\mathbf{H} \\mathbf{z} \\leq \\beta \\| \\mathbf{z} \\|_2^2\\).\nIn order to better understand the Loewner order (and because it’s incredibly useful), we will consider the eigendecomposition.\n\n\nEigendecomposition\nEvery symmetric matrix \\(\\mathbf{H}\\) can be written as \\(\\mathbf{H} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\) where \\(\\mathbf{V}\\) is an orthogonal matrix and \\(\\mathbf{\\Lambda}\\) is a diagonal matrix. Here, \\(\\mathbf{V}\\) is square and orthogonal so \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I} = \\mathbf{V} \\mathbf{V}^\\top\\).\nFor the \\(i\\)th column \\(\\mathbf{v}_i\\) of \\(\\mathbf{V}\\), we have that \\[\\mathbf{H} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\] where \\(\\lambda_i\\) is the \\(i\\)th diagonal entry of \\(\\mathbf{\\Lambda}\\). By definition, \\(\\mathbf{v}_i\\) is the \\(i\\)th vector and \\(\\lambda_i\\) is the \\(i\\)th eigenvalue of \\(\\mathbf{H}\\).\n\n\n\nWe can use the eigendecomposition to relate eigenvalues and the PSD property.\nClaim: The matrix \\(\\mathbf{H}\\) is PSD if and only if the eigenvalues are all positive i.e. \\(\\lambda_i \\geq 0\\) for all \\(i\\).\nFor the first direction \\((\\Rightarrow)\\), suppose that \\(\\mathbf{H}\\) is PSD. Consider an eigenvector \\(\\mathbf{v}_i\\). By the PSD property, we have that \\(\\mathbf{v}_i^\\top \\mathbf{H} \\mathbf{v}_i \\geq 0\\). By the eigendecomposition, we have that \\(\\mathbf{v}_i^\\top \\mathbf{H} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i^\\top \\mathbf{v}_i = \\lambda_i\\). So \\(\\lambda_i \\geq 0\\). For the second direction \\((\\Leftarrow)\\), suppose that \\(\\lambda_i \\geq 0\\) for all \\(i\\). Consider any vector \\(\\mathbf{y}\\). Since all the eigenvalues are non-negative, we can write \\(\\mathbf{\\Lambda} = \\sqrt{\\mathbf{\\Lambda}} \\sqrt{\\mathbf{\\Lambda}}\\) where \\(\\sqrt{\\mathbf{\\Lambda}}\\) is the diagonal matrix with the square root of the eigenvalues on the diagonal. Using the eigendecomposition, we can write \\[\\begin{align*}\n\\mathbf{y}^\\top \\mathbf{H} \\mathbf{y} &= \\mathbf{y}^\\top \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top \\mathbf{y} \\\\\n&= \\mathbf{y}^\\top \\mathbf{V} \\sqrt{\\mathbf{\\Lambda}} \\sqrt{\\mathbf{\\Lambda}} \\mathbf{V}^\\top \\mathbf{y} \\\\\n&= \\|\\mathbf{y}^\\top \\mathbf{V} \\sqrt{\\mathbf{\\Lambda}} \\|_2^2 \\geq 0.\n\\end{align*}\\] For the last equality, we used that diagonal matrices are symmetric.\nWe can use this to write our second order assumptions in terms of the Loewner order.\nClaim: \\(f\\) is \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth if and only if \\(\\alpha \\leq \\lambda_d \\leq \\ldots \\leq \\lambda_1 \\leq \\beta\\). Here, we are assuming that the eigenvalues are sorted in decreasing order.\nTo see this, we can write \\[\\begin{align*}\n\\beta \\mathbf{I} - \\mathbf{H} = \\beta \\mathbf{V} \\mathbf{V}^\\top\n- \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\n= \\mathbf{V} (\\beta \\mathbf{I} - \\mathbf{\\Lambda}) \\mathbf{V}^\\top.\n\\end{align*}\\] With this observation, we can see that the \\(i\\)th eigenvalue of \\(\\beta \\mathbf{I} - \\mathbf{H}\\) is \\(\\beta - \\lambda_i\\). Notice that \\(\\beta \\mathbf{I} - \\mathbf{H} \\succeq 0\\) if and only if \\(\\beta - \\lambda_i \\geq 0\\) for all \\(i\\). The \\(\\alpha\\)-strongly convex direction is similar.\nWe will use one more useful property of the eigendecomposition. Since the eigenvectors span all of \\(\\mathbb{R}^d\\), we can write any vector \\(\\mathbf{z}\\) as a linear combination of the eigenvectors. In particular, \\[\\begin{align*}\n\\mathbf{z} = \\sum_{i=1}^d \\alpha_i \\mathbf{v}_i\n\\end{align*}\\] for some some coefficients \\(\\alpha_i\\). (We actually know that \\(\\alpha_i = \\mathbf{z}^\\top \\mathbf{v}_i\\).) With this observation, we can see that \\[\\begin{align*}\n\\lambda_\\min (\\mathbf{H}) \\| \\mathbf{z} \\|_2^2 \\leq\n\\mathbf{z}^\\top \\mathbf{H} \\mathbf{z}\n\\leq \\lambda_\\max (\\mathbf{H}) \\| \\mathbf{z} \\|_2^2\n\\end{align*}\\] where \\(\\lambda_\\min\\) and \\(\\lambda_\\max\\) denote the minimum and maximum eigenvalues of \\(\\mathbf{H}\\).\nThen it follows that if the maximum eigenvalue of \\(\\mathbf{H}=\\nabla^2 f(\\mathbf{x})\\) is \\(\\beta\\) and the minimum eigenvalue is \\(\\alpha\\), then \\(f\\) is \\(\\beta\\)-smooth and \\(\\alpha\\)-strongly convex.\n\n\nCondition Number Connection\nWith these tools, let’s return to proving the convergence result for the particular function \\[f(\\mathbf{x}) = \\frac12 \\| \\mathbf{Ax} - \\mathbf{b}\\|_2^2.\\] Let \\(\\lambda_\\max = \\lambda_\\max(\\mathbf{A}^\\top \\mathbf{A})\\) and \\(\\lambda_\\min = \\lambda_\\min(\\mathbf{A}^\\top \\mathbf{A})\\). We will set the step size \\(\\eta = \\frac{1}{2\\lambda_\\max}\\). The gradient descent update is given by \\[\\begin{align*}\n\\mathbf{x}^{(t+1)}\n= \\mathbf{x}^{(t)} - \\frac1{2 \\lambda_\\max}\n2 \\mathbf{A}^\\top (\\mathbf{A} \\mathbf{x}^{(t)} - \\mathbf{b}).\n\\end{align*}\\]\nWe can view this update as a repeated matrix multiplication. In particular, we have that \\[\\begin{align*}\n\\mathbf{x}^{(t+1)}\n&= \\mathbf{x}^{(t)} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\left( \\mathbf{A} \\mathbf{x}^{(t)} - \\mathbf{b} \\right) \\\\\n&=  \\mathbf{x}^{(t)} - \\frac1{\\lambda_\\max}\n\\mathbf{A}^\\top \\mathbf{A} \\mathbf{x}^{(t)} -\n\\frac1{\\lambda_\\max}\n\\mathbf{A}^\\top \\mathbf{b}.\n\\end{align*}\\] We will connect the equation to the optimal solution \\(\\mathbf{x}^*\\). Since it is a stationary point, the gradient of the optimal solution is zero and we have that \\(\\mathbf{A}^\\top ( \\mathbf{A} \\mathbf{x}^* - \\mathbf{b}) = \\mathbf{0}\\). So we can write \\(\\mathbf{A}^\\top \\mathbf{b} = \\mathbf{A}^\\top \\mathbf{A} \\mathbf{x}^*\\). Subtracting the optimal solution \\(\\mathbf{x}^*\\) from both sides and substituting our expression for \\(\\mathbf{A}^\\top \\mathbf{b}\\), we have that \\[\\begin{align*}\n\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\n&= \\mathbf{x}^{(t)}  - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A} \\mathbf{x}^{(t)} + \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A} \\mathbf{x}^* - \\mathbf{x}^* \\\\\n&= \\left( \\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A} \\right) \\left( \\mathbf{x}^{(t)} - \\mathbf{x}^* \\right).\n\\end{align*}\\] By repeatedly applying the equation, we can write \\[\\begin{align*}\n(\\mathbf{x}^{(T)} - \\mathbf{x}^*)\n= (\\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A})^T (\\mathbf{x}^{(0)} - \\mathbf{x}^*).\n\\end{align*}\\]\nWe will show that the maximum eigenvalue of \\((\\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A})\\) is small. Therefore the difference between \\(\\mathbf{x}^{(T)}\\) and \\(\\mathbf{x}^*\\) decreases quickly.\nUsing the eigendecomposition of \\(\\mathbf{A}^\\top \\mathbf{A}\\), we can write \\[\\begin{align*}\n\\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A}\n= \\mathbf{V} \\mathbf{V}^\\top - \\frac1{\\lambda_\\max} \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\n= \\mathbf{V} \\left( \\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{\\Lambda} \\right) \\mathbf{V}^\\top.\n\\end{align*}\\] The eigenvalues are given by \\(1 - \\frac{\\lambda_i}{\\lambda_\\max}\\). The smallest eigenvalue is \\(1-\\frac{\\lambda_\\max}{\\lambda_\\max} =0\\) while the largest eigenvalue is \\(1 - \\frac{\\lambda_\\min}{\\lambda_\\max}\\). Recall that \\(\\lambda_\\min = \\alpha\\) and \\(\\lambda_\\max\\) so the largest eigenvalue is \\(1 - \\frac{\\alpha}{\\beta} = 1- \\frac1{\\kappa}\\).\nNotice that repeatedly applying symmetric matrices only modifies the eigenvalues: \\[\\begin{align*}\n\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\n\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\n= \\mathbf{V} \\mathbf{\\Lambda}^2 \\mathbf{V}^\\top.\n\\end{align*}\\]\nUsing this property, the maximum eigenvalue of \\((\\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A})^T\\) is \\((1 - \\frac1{\\kappa})^T\\). We will use the inequality that \\((1-\\frac1{x})^x \\leq \\frac1{e}\\) for all positive \\(x\\) to bound the maximum eigenvalue. \\[\\begin{align*}\n\\left( 1- \\frac{1}{\\kappa} \\right)^T\n= \\left( \\left( 1- \\frac{1}{\\kappa} \\right)^{\\kappa} \\right)^{\\frac{T}{\\kappa}}\n\\leq \\frac{1}{e^{\\frac{T}{\\kappa}}} = e^{-\\frac{T}{\\kappa}}.\n\\end{align*}\\]\nPutting everything together, we have that \\[\\begin{align*}\n\\| \\mathbf{x}^{(T)} - \\mathbf{x}^* \\|_2^2\n&= \\| (\\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A})^T (\\mathbf{x}^{(0)} - \\mathbf{x}^*) \\|_2^2 \\\\\n&\\leq \\lambda_\\max\\left((\\mathbf{I} - \\frac1{\\lambda_\\max} \\mathbf{A}^\\top \\mathbf{A})^T\\right )  \\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2 \\\\\n&\\leq e^{-2\\frac{T}{\\kappa}} \\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2.\n\\end{align*}\\] The \\(\\alpha\\)-strong convexity and \\(\\beta\\)-smoothness convergence bound follows when \\(f\\) is the linear regression loss.\n\n\nAccelerated Gradient Descent\nIt turns out that we can actually converge faster for \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth functions. We will use a technique called accelerated gradient descent.\nInitialize starting vector \\(\\mathbf{x}^{(0)} = \\mathbf{y}^{(1)} = \\mathbf{z}^{(1)}\\). For \\(t = 1,\\ldots, T\\), compute\n\n\\(\\mathbf{y}^{(t+1)} = \\mathbf{x}^{(t)} - \\frac{1}{\\beta}\\nabla f(\\mathbf{x}^{(t)})\\)\n\\(\\mathbf{x}^{(t+1)} = \\left(1 + \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right) \\mathbf{y}^{(t+1)} + \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\left(\\mathbf{y}^{(t+1)} - \\mathbf{y}^{(t)}\\right)\\).\n\nAccelerated Gradient Descent: Let \\(f\\) be a \\(\\beta\\)-smooth and \\(\\alpha\\)-strongly convex function. If we run accelerated gradient descent for \\(T\\) steps, the output \\(\\hat{\\mathbf{x}}\\) satisfies \\[\\begin{align*}\nf(\\mathbf{x}^{(T)}) - f(\\mathbf{x}^*) \\leq\n\\kappa e^{-T / \\sqrt{\\kappa}}\n[f(\\mathbf{x}^{(0)}) - f(\\mathbf{x}^*)].\n\\end{align*}\\] Equivalently, if \\(T = O(\\sqrt{\\kappa} \\log(\\kappa / \\epsilon))\\) we find an \\(\\epsilon\\)-approximate minimizer. Notice the improvement from \\(\\kappa\\) to \\(\\sqrt{\\kappa}\\).\nWe won’t show the proof but we will wave at the intuition.\n\n\n\nStandard gradient descent can get stuck backtracking along the valley for functions like the one in the figure. In contrast, accelerated gradient descent maintains its momentum in the direction of descent and avoids oscillating back and forth.\nToday, we saw how to gradient descent performs better under second order assumptions. Next time, we’ll discuss how to use gradient descent in online settings and what to do when computing full gradients is too expensive."
  },
  {
    "objectID": "notes/19_sparse_recovery.html",
    "href": "notes/19_sparse_recovery.html",
    "title": "Sparse Recovery",
    "section": "",
    "text": "When we considered regression and the Fast JL Transform, we wanted to solve the regression problem by finding a vector \\(\\mathbf{x}\\) that minimized the mean squared error \\[\\begin{align*}\n\\|\\mathbf{Ax} - \\mathbf{b} \\|_2^2\n\\end{align*}\\] where \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) is a feature matrix and \\(\\mathbf{b} \\in \\mathbb{R}^n\\) is a target vector. In particular, we considered the setting where \\(n \\gg d\\) and we could only hope to return a solution that made the mean squared error small.\nIn sparse recovery, we consider the same problem from a different perspective and we assume that \\(n \\ll d\\). Now, the the idea is that \\(\\mathbf{x}\\) is a vector that is hidden from us and we want to recover it by multiplying it by a matrix \\(\\mathbf{A}\\) and observe the result \\(\\mathbf{b}\\).\n\n\n\nBecause we can control so many coefficients in \\(\\mathbf{x}\\), there are likely many solutions that make the mean squared error zero. We will make the problem more interesting by assuming that \\(\\mathbf{x}\\) has a special structure. In particular, we will assume that \\(\\mathbf{x}\\) is \\(k\\)-sparse i.e., there are at most \\(k\\) non-zero entries in \\(\\mathbf{x}\\). Typically, \\(k \\ll d\\).\nSince we know we can recover \\(\\mathbf{x}\\), the question becomes how many measurements i.e., rows in \\(\\mathbf{A}\\) do we need to recover it?\nWe’ll start with a trivial solution. Notice that if we make each row of \\(\\mathbf{A}\\) a standard basis vector, then we can recover each entry of \\(\\mathbf{x}\\) by observing the corresponding entry of \\(\\mathbf{b}\\). Unfortunately, this approach requires that we have as many rows in \\(\\mathbf{A}\\) as entries in \\(\\mathbf{x}\\) which requires \\(O(d)\\) measurements and \\(O(d^2)\\) space to even store \\(\\mathbf{A}\\).\nToday, we will do better by coming up with an algorithm which makes \\(O(k \\log k)\\) measurements. This should be surprising because, while we know that \\(\\mathbf{x}\\) is \\(k\\)-sparse, we don’t know which \\(k\\) entries are non-zero.\n\n\nPhotography: Typically, cameras acquire images by measuring the intensity of light with one sensor per pixel. We could instead imagine a camera that measures the intensity of light with just one sensor. If the pixel intensities are \\(\\mathbf{x} \\in \\mathbb{R}^n\\), then the single pixel returns \\[\nb = \\frac1{n} \\sum_{i=1}^n x_i\n= \\begin{bmatrix} \\frac1{n} & \\frac1{n} & \\cdots & \\frac1{n} \\end{bmatrix} \\mathbf{x}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\] which is not very much information about the image. But we can get more information from other linear measurements via masking.\n\n\n\nLet \\[\nb_i = \\frac1{n} \\sum_{j \\in S_i} x_j\n= \\begin{bmatrix} 0 & \\frac1{n} & \\cdots & 0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}.\n\\] If we take enough measurements \\(b_i\\), then we can recover the whole image.\nThere are applications of this approach in imaging outside of the visible spectrum, microscopy, and other scientific imaging. The theory we will discuss does not exactly describe these problems but it has been very valuable in modeling them.\nMedical Imaging (MRI): How do we measure entries of the Fourier transform \\(\\mathbf{Fx}\\) for an MRI? We blast the body with sound waves of varying frequency. If we can reduce the number of measurements, we can reduce the time the patient spends in the machine and the energy use of the procedure.\nGeophysics: How do we measure entries of the Fourier transform \\(\\mathbf{Fx}\\) for geophysical analysis? We blast the ground with sound waves of varying frequency (e.g.,using airguns, controlled explosions, and vibrations from drilling). If we can reduce the number of measurements, we can make the data acquisition process cheaper and less disruptive.\n\n\n\nThe main roadblock to directly applying the sparse recovery problem to the examples we described is that the data we measure is not necessarily sparse. For example, an image is not sparse if we represent as a vector of pixel intensities. However, we can often represent the data in a different basis where it is sparse.\nThe Fourier transform is a linear transformation that decomposes data into frequencies. By the Uncertainty Principle in harmonic analysis, a vector and its Fourier transform cannot both be too dense. The figure below gives an example. On the left we have data with many non-zero entries and, on the left, we have the Fourier transform of the data which is (close to) sparse.\n\n \n\nGenerally, we can take dense data and make it sparse by applying the Fourier transform. The Fourier transform is closely related to the Hadamard matrix we analyzed for the Fast JL Transform algorithm. The discrete Fourier transform (DFT) \\(\\mathbf{F} \\in \\mathbb{C}^{n \\times n}\\) is defined on complex numbers \\[\n\\mathbf{F}_{j,k} = e^{-2\\pi i \\frac{j k}{n}}\n\\] where \\(i\\) is the imaginary unit. The DFT is a unitary matrix so \\(\\mathbf{F}^* \\mathbf{F} = \\mathbf{I}\\) where \\(\\mathbf{F}^*\\) is the conjugate transpose of \\(\\mathbf{F}\\).\nUsing the same divide-and-conquer algorithm as the Hadamard matrix, we can compute \\(\\mathbf{Fy}\\) the DFT of the vector \\(\\mathbf{y}\\) in \\(O(n \\log n)\\) time.\nThe real part of the \\(j,k\\) entry is \\(\\cos(2\\pi j k)\\) so the \\(j\\)th row of \\(\\mathbf{F}\\) looks like a cosine wave with frequency \\(j\\). Computing \\(\\mathbf{Fy}\\) computes inner products of \\(\\mathbf{y}\\) with many different frequencies, which can be used to decompose the vector into a sum of those frequencies.\n\n\n\nAs we saw before, sampling does not preserve norms when \\(\\mathbf{y}\\) has a few large entries. Before, the hard case was when \\(\\mathbf{y}\\) had a few large entries and now the hard case is when \\(\\mathbf{y}\\) has too many non-zero entries. Taking the Fourier transform, just like taking the Hadamard transform, eliminates the hard case without changing the norm. Because of this property, the Fourier transform is one of the central tools in sparse recovery (also sometimes called compressed sensing).\nThe goal in sparse recover is to recover a vector \\(\\mathbf{x}\\) from linear measurements. We can choose \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) with \\(m &lt; n\\). Assume we can access the measurement \\(\\mathbf{b} = \\mathbf{Ax}\\). We will try to recover \\(\\mathbf{x}\\) from \\(\\mathbf{b}\\).\nSince \\(m &lt; n\\), there are infinitely many vectors \\(\\mathbf{x}\\) that satisfy \\(\\mathbf{Ax} = \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/19_sparse_recovery.html#motivation",
    "href": "notes/19_sparse_recovery.html#motivation",
    "title": "Sparse Recovery",
    "section": "",
    "text": "When we considered regression and the Fast JL Transform, we wanted to solve the regression problem by finding a vector \\(\\mathbf{x}\\) that minimized the mean squared error \\[\\begin{align*}\n\\|\\mathbf{Ax} - \\mathbf{b} \\|_2^2\n\\end{align*}\\] where \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) is a feature matrix and \\(\\mathbf{b} \\in \\mathbb{R}^n\\) is a target vector. In particular, we considered the setting where \\(n \\gg d\\) and we could only hope to return a solution that made the mean squared error small.\nIn sparse recovery, we consider the same problem from a different perspective and we assume that \\(n \\ll d\\). Now, the the idea is that \\(\\mathbf{x}\\) is a vector that is hidden from us and we want to recover it by multiplying it by a matrix \\(\\mathbf{A}\\) and observe the result \\(\\mathbf{b}\\).\n\n\n\nBecause we can control so many coefficients in \\(\\mathbf{x}\\), there are likely many solutions that make the mean squared error zero. We will make the problem more interesting by assuming that \\(\\mathbf{x}\\) has a special structure. In particular, we will assume that \\(\\mathbf{x}\\) is \\(k\\)-sparse i.e., there are at most \\(k\\) non-zero entries in \\(\\mathbf{x}\\). Typically, \\(k \\ll d\\).\nSince we know we can recover \\(\\mathbf{x}\\), the question becomes how many measurements i.e., rows in \\(\\mathbf{A}\\) do we need to recover it?\nWe’ll start with a trivial solution. Notice that if we make each row of \\(\\mathbf{A}\\) a standard basis vector, then we can recover each entry of \\(\\mathbf{x}\\) by observing the corresponding entry of \\(\\mathbf{b}\\). Unfortunately, this approach requires that we have as many rows in \\(\\mathbf{A}\\) as entries in \\(\\mathbf{x}\\) which requires \\(O(d)\\) measurements and \\(O(d^2)\\) space to even store \\(\\mathbf{A}\\).\nToday, we will do better by coming up with an algorithm which makes \\(O(k \\log k)\\) measurements. This should be surprising because, while we know that \\(\\mathbf{x}\\) is \\(k\\)-sparse, we don’t know which \\(k\\) entries are non-zero.\n\n\nPhotography: Typically, cameras acquire images by measuring the intensity of light with one sensor per pixel. We could instead imagine a camera that measures the intensity of light with just one sensor. If the pixel intensities are \\(\\mathbf{x} \\in \\mathbb{R}^n\\), then the single pixel returns \\[\nb = \\frac1{n} \\sum_{i=1}^n x_i\n= \\begin{bmatrix} \\frac1{n} & \\frac1{n} & \\cdots & \\frac1{n} \\end{bmatrix} \\mathbf{x}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\] which is not very much information about the image. But we can get more information from other linear measurements via masking.\n\n\n\nLet \\[\nb_i = \\frac1{n} \\sum_{j \\in S_i} x_j\n= \\begin{bmatrix} 0 & \\frac1{n} & \\cdots & 0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}.\n\\] If we take enough measurements \\(b_i\\), then we can recover the whole image.\nThere are applications of this approach in imaging outside of the visible spectrum, microscopy, and other scientific imaging. The theory we will discuss does not exactly describe these problems but it has been very valuable in modeling them.\nMedical Imaging (MRI): How do we measure entries of the Fourier transform \\(\\mathbf{Fx}\\) for an MRI? We blast the body with sound waves of varying frequency. If we can reduce the number of measurements, we can reduce the time the patient spends in the machine and the energy use of the procedure.\nGeophysics: How do we measure entries of the Fourier transform \\(\\mathbf{Fx}\\) for geophysical analysis? We blast the ground with sound waves of varying frequency (e.g.,using airguns, controlled explosions, and vibrations from drilling). If we can reduce the number of measurements, we can make the data acquisition process cheaper and less disruptive.\n\n\n\nThe main roadblock to directly applying the sparse recovery problem to the examples we described is that the data we measure is not necessarily sparse. For example, an image is not sparse if we represent as a vector of pixel intensities. However, we can often represent the data in a different basis where it is sparse.\nThe Fourier transform is a linear transformation that decomposes data into frequencies. By the Uncertainty Principle in harmonic analysis, a vector and its Fourier transform cannot both be too dense. The figure below gives an example. On the left we have data with many non-zero entries and, on the left, we have the Fourier transform of the data which is (close to) sparse.\n\n \n\nGenerally, we can take dense data and make it sparse by applying the Fourier transform. The Fourier transform is closely related to the Hadamard matrix we analyzed for the Fast JL Transform algorithm. The discrete Fourier transform (DFT) \\(\\mathbf{F} \\in \\mathbb{C}^{n \\times n}\\) is defined on complex numbers \\[\n\\mathbf{F}_{j,k} = e^{-2\\pi i \\frac{j k}{n}}\n\\] where \\(i\\) is the imaginary unit. The DFT is a unitary matrix so \\(\\mathbf{F}^* \\mathbf{F} = \\mathbf{I}\\) where \\(\\mathbf{F}^*\\) is the conjugate transpose of \\(\\mathbf{F}\\).\nUsing the same divide-and-conquer algorithm as the Hadamard matrix, we can compute \\(\\mathbf{Fy}\\) the DFT of the vector \\(\\mathbf{y}\\) in \\(O(n \\log n)\\) time.\nThe real part of the \\(j,k\\) entry is \\(\\cos(2\\pi j k)\\) so the \\(j\\)th row of \\(\\mathbf{F}\\) looks like a cosine wave with frequency \\(j\\). Computing \\(\\mathbf{Fy}\\) computes inner products of \\(\\mathbf{y}\\) with many different frequencies, which can be used to decompose the vector into a sum of those frequencies.\n\n\n\nAs we saw before, sampling does not preserve norms when \\(\\mathbf{y}\\) has a few large entries. Before, the hard case was when \\(\\mathbf{y}\\) had a few large entries and now the hard case is when \\(\\mathbf{y}\\) has too many non-zero entries. Taking the Fourier transform, just like taking the Hadamard transform, eliminates the hard case without changing the norm. Because of this property, the Fourier transform is one of the central tools in sparse recovery (also sometimes called compressed sensing).\nThe goal in sparse recover is to recover a vector \\(\\mathbf{x}\\) from linear measurements. We can choose \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) with \\(m &lt; n\\). Assume we can access the measurement \\(\\mathbf{b} = \\mathbf{Ax}\\). We will try to recover \\(\\mathbf{x}\\) from \\(\\mathbf{b}\\).\nSince \\(m &lt; n\\), there are infinitely many vectors \\(\\mathbf{x}\\) that satisfy \\(\\mathbf{Ax} = \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/19_sparse_recovery.html#sparse-recovery",
    "href": "notes/19_sparse_recovery.html#sparse-recovery",
    "title": "Sparse Recovery",
    "section": "Sparse Recovery",
    "text": "Sparse Recovery\nNow that we have a sense of the problem, we will discuss how to solve it. We want to recover \\(\\mathbf{x}\\) from \\(\\mathbf{b} = \\mathbf{Ax}\\) under the assumption that \\(\\mathbf{x}\\) is \\(k\\)-sparse i.e., it has at most \\(k\\) non-zero entries.\nIn this problem, we get to choose the matrix \\(\\mathbf{A}\\) that we will use to measure \\(\\mathbf{x}\\). Let’s consider the properties of \\(\\mathbf{A}\\) that would allow us to recover \\(\\mathbf{x}\\).\nOne property that would not allow us to recover \\(\\mathbf{x}\\) is if \\(\\mathbf{A}\\) has repeated columns. Then we cannot distinguish between the entries of \\(\\mathbf{x}\\) that correspond to those columns.\n\n\n\nThere are several ways to formalize the property that \\(\\mathbf{A}\\) allows us to recover \\(\\mathbf{x}\\):\n\n\\(\\mathbf{A}\\) has Kruskal rank \\(r\\) if all sets of \\(r\\) columns are linearly independent.\n\\(\\mathbf{A}\\) is \\(\\mu\\)-incoherent if \\(|\\mathbf{A}_{i}^\\top \\mathbf{A}_j | \\leq \\mu \\|\\mathbf{A}_i \\|_2 \\| \\mathbf{A}_j \\|_2\\) for all distinct columns \\(i,j\\).\n\nToday, we will consider matrices that satisfy the RIP.\nRestricted Isometry Property (RIP): \\(\\mathbf{A}\\) satisfies the \\((q,\\epsilon)\\)-Restricted Isometry Property (RIP) if \\((1-\\epsilon) \\| \\mathbf{x} \\|_2^2 \\leq \\| \\mathbf{Ax} \\|_2^2 \\leq (1+\\epsilon) \\| \\mathbf{x} \\|_2^2\\) for all \\(q\\)-sparse vectors \\(\\mathbf{x}\\).\nNotice that RIP is a Johnson-Lindenstrauss type condition. However, it is not quite the same as the JL Lemma because it applies to all \\(q\\)-sparse vectors instead of a discrete set of vectors or all vectors in a subspace.\nA natural question is whether we can find matrices that satisfy RIP and how difficult it is to find them.\n\nRandom Matrices\nMatrices that are satisfy RIP are not too rare. In fact, random Johnson-Lindenstrauss matrices (e.g., Gaussian, sign, sparse) with \\(m=O\\left(\\frac{k \\log(n/k}{\\epsilon^2}\\right)\\) rows are \\((q, \\epsilon)\\)-RIP.\nWe will show this by applying the JL subspace embedding theorem to a collection of subspaces.\nWe’ll use the following subspace embedding theorem that we proved previously.\nSubspace Embedding Theorem Let \\(\\mathcal{U} \\subset \\mathbb{R}^n\\) be a \\(k\\)-dimensional subspace. If \\(\\mathbf{\\Pi}\\) is chosen from any distribution satisfying the distributional JL Lemma, then with probability \\(1-\\delta\\), \\[\n(1-\\epsilon) \\| \\mathbf{y} \\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{y} \\|_2^2\n\\leq (1+\\epsilon) \\| \\mathbf{y} \\|_2^2\n\\] for all \\(\\mathbf{y} \\in \\mathcal{U}\\) as long as \\(m = O \\left( \\frac{k + \\log(1/\\delta)}{\\epsilon^2} \\right)\\).\nWe will use union bound to apply this theorem to a collection of linear subspaces. Let \\(\\mathcal{S}_k = \\{ \\mathbf{x} \\in \\mathbb{R}^n : \\|\\mathbf{x}\\|_0 \\leq k \\}\\) be the set of all \\(k\\)-sparse vectors. We can write \\(\\mathcal{S}_k\\) as a union of disjoint \\(k\\)-dimensional subspaces \\(\\mathcal{S}_k = \\mathcal{U}_1 \\cup \\ldots \\mathcal{U}_T\\). The number of subspaces \\(T\\) is the number of ways to choose \\(k\\) coordinates from \\(n\\) coordinates, which is \\(T = \\binom{n}{k}\\). Consider all \\(k\\)-sparse vectors \\(\\mathbf{x} \\in \\mathbb{R}^n\\). We can partition the set by which coordinates are non-zero. For one set of non-zero coordinates, we only need to preserve the basis vectors for that set of coordinates. Therefore \\(T\\) is the number of ways to choose \\(q\\) coordinates from \\(n\\) coordinates, which is \\(T = \\binom{n}{k}\\).\n\n \n\nWe’ll apply union bound to all \\(T\\) subspaces. In particular, we’ll set \\(\\delta' = \\frac{\\delta}{T}\\) and apply the subspace embedding theorem with failure probability \\(\\delta'\\). It remains to analyze \\[\\begin{align*}\n\\log(1/\\delta') &= \\log(1/\\delta) + \\log(T) \\\\\n&= \\log(1/\\delta) + \\log \\left( \\frac{n!}{k!(n-k)!} \\right) \\\\\n&\\approx \\log(1/\\delta) + k \\log \\left( \\frac{n}{k} \\right).\n\\end{align*}\\] We won’t prove the approximate equality but you can check that \\[\\begin{align*}\n\\left( \\frac{n}{k} \\right)^k \\leq \\binom{n}{k} \\leq \\left( \\frac{en}{k} \\right)^k.\n\\end{align*}\\]\nThen we have the following.\nTheorem: If \\(\\mathbf{\\Pi} \\in \\mathbb{R}^{m \\times n}\\) is chosen from any distribution \\(\\mathcal{D}\\) satisfying the distributional JL lemma, then with probability \\(1-\\delta\\), \\[\n(1-\\epsilon) \\| \\mathbf{x} \\|_2^2 \\leq \\| \\mathbf{\\Pi x} \\|_2^2 \\leq (1+\\epsilon) \\| \\mathbf{x} \\|_2^2\n\\] for all \\(k\\)-sparse \\(\\mathbf{x}\\) as long as \\(m = O\\left( \\frac{k\\log(n) + \\log(1/\\delta)}{\\epsilon^2} \\right)\\).\n\n\nInefficient Algorithm\nNow that we have established what type of matrices satisfy RIP and how to find them, we will discuss how to recover \\(\\mathbf{x}\\) from \\(\\mathbf{b} = \\mathbf{Ax}\\).\nBy the next theorem, there is a simple algorithm that recovers \\(\\mathbf{x}\\) from \\(\\mathbf{b}\\) exactly.\n\\(\\ell_0\\)-Minimization Theorem: Suppose we are given \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{b} = \\mathbf{Ax}\\) for an unknown \\(k\\)-sparse \\(\\mathbf{x} \\in \\mathbb{R}^n\\). If \\(\\mathbf{A}\\) is \\((2k,\\epsilon)\\)-RIP for any \\(\\epsilon&lt; 1\\), then \\(\\mathbf{x}\\) is the unique minimizer of \\[\n\\min_{\\mathbf{z} \\in \\mathbb{R}^d} \\| \\mathbf{z} \\|_0 \\quad \\textrm{subject to} \\quad \\mathbf{Az} = \\mathbf{b}.\n\\]\nProof: Consider any $ with \\(\\mathbf{Ay} = \\mathbf{b}\\) and \\(\\| \\mathbf{y} \\|_0 \\leq \\| \\mathbf{x} \\|_0 \\leq k\\).\nWe know that \\[\\begin{align*}\n\\mathbf{A}(\\mathbf{y} - \\mathbf{x}) =\n\\mathbf{Ay} - \\mathbf{Ax}\n= \\mathbf{b} - \\mathbf{b} = 0\n\\end{align*}\\] so \\(\\| \\mathbf{A(y-x)} \\|_2 = 0\\). But \\(\\mathbf{y-x}\\) is \\(2k\\)-sparse and \\(\\mathbf{A}\\) is \\((2k,\\epsilon)\\)-RIP so \\[\\begin{align*}\n(1-\\epsilon) \\| \\mathbf{y-x} \\|_2^2 \\leq \\| \\mathbf{A(y-x)} \\|_2^2 \\leq (1+\\epsilon) \\| \\mathbf{y-x} \\|_2^2.\n\\end{align*}\\] So then \\(\\| \\mathbf{y-x}\\|_2 \\leq 0\\) and we must have \\(\\mathbf{y} = \\mathbf{x}\\).\nThe theorem establishes that information theoretically we can recover \\(\\mathbf{x}\\) from \\(\\mathbf{b}\\). However, solving the \\(\\ell_0\\)-minimization problem is computationally difficult. In fact, (one of) the best algorithms we know that solves it is to try all possible \\(\\binom{d}{k}\\) subsets of \\(\\mathbf{x}\\) and choose the one that minimizes \\(\\| \\mathbf{Ax} - \\mathbf{b} \\|_2\\). This algorithm takes \\(O(d^k)\\) time which is not practical for large \\(d\\). We will address a faster method shortly.\n\n\nEfficient Algorithm\nSolving the \\(\\ell_0\\)-minimization problem is computationally difficult. Thankfully, we can relax the problem to a convex optimization problem that is exponentially faster in \\(k\\) and gives the same solution.\n\\(\\ell_1\\)-Minimization Theorem: Suppose we are given \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{b} = \\mathbf{Ax}\\) for an unknown \\(k\\)-sparse \\(\\mathbf{x} \\in \\mathbb{R}^n\\). If \\(\\mathbf{A}\\) is \\((2k,\\epsilon)\\)-RIP for any \\(\\epsilon&lt; .17\\), then \\(\\mathbf{x}\\) is the unique minimizer of \\[\n\\min_{\\mathbf{z} \\in \\mathbb{R}^d} \\| \\mathbf{z} \\|_1 \\quad \\textrm{subject to} \\quad \\mathbf{Az} = \\mathbf{b}.\n\\]\nBefore we prove the theorem, let’s get some intuition for why it holds. The \\(\\ell_1\\)-norm is the sum of the absolute values of the entries of \\(\\mathbf{z}\\). As depicted on the left figure below, the sets with equal \\(\\ell_1\\)-norm form a diamond. At the same time, the constraint that \\(\\mathbf{Az} = \\mathbf{b}\\) is a hyperplane that is random (since \\(\\mathbf{A}\\) is random). Then, with high probability, the intersection of the constraint and the objective will be a vertex of the diamond which corresponds to a sparse solution.\n\n \n\nThe same intuition does not hold for the \\(\\ell_2\\)-norm. The sets with equal \\(\\ell_2\\)-norm form a sphere. So the intersection of the constraint and the objective will be a random point on the circle which corresponds to a dense solution.\nWe will next prove the \\(\\ell_1\\)-minimization theorem. There are two tools that we’ll use.\nTool 1: For any vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), \\[\\begin{align*}\n\\|\\mathbf{a+b} \\| \\geq \\| \\mathbf{a} \\| - \\| \\mathbf{b} \\|.\n\\end{align*}\\] We can see this from the triangle inequality that \\(\\| \\mathbf{x} + \\mathbf{y} \\| \\leq \\| \\mathbf{x} \\| + \\| \\mathbf{y} \\|\\). Plugging in \\(\\mathbf{x} = \\mathbf{a} - \\mathbf{b}\\) and \\(\\mathbf{y} = \\mathbf{b}\\) then rearranging gives the result. Notice that this works for any norm.\nTool 2: For any vector \\(k\\)-sparse vector \\(\\mathbf{w}\\), we have \\[\\begin{align*}\n\\|\\mathbf{w}\\|_2 \\leq \\| \\mathbf{w} \\|_1 \\leq \\sqrt{k} \\| \\mathbf{w} \\|_2.\n\\end{align*}\\] The first inequality follows from the definition of the \\(\\ell_2\\)-norm and the second inequality follows from the Cauchy-Schwarz inequality. In particular, \\[\\begin{align*}\n\\| \\mathbf{w} \\|_2^2 = \\sum_{i=1}^d w_i^2 \\leq \\sum_{i=1}^d w_i^2 \\sum_{i=1}^d \\sum{j=1}^d |w_i| |w_j| = \\sum_{i=1}^d |w_i| \\sum_{j=1}^d |w_j| = \\| \\mathbf{w} \\|_1^2\n\\end{align*}\\] and \\[\\begin{align*}\n\\| \\mathbf{w} \\|_1^2 = \\sum_{i=1}^d |w_i|^2 \\mathbb{1}[w_i \\neq 0] \\leq \\left( \\sum_{i=1}^d |w_i|^2 \\right) \\left( \\sum_{j: w_j \\neq 0}1 \\right) = k \\| \\mathbf{w} \\|_2^2\n\\end{align*}\\] where the inequality follows by Cauchy-Schwarz.\nWith these tools, we are now ready to prove the \\(\\ell_1\\)-minimization theorem.\nProof: By way of contradiction, we will assume that \\(\\mathbf{x}\\) is not the optimal solution. Then there exists some non-zero \\(\\mathbf{\\Delta}\\) such that \\(\\| \\mathbf{x} + \\mathbf{\\Delta} \\|_1 \\leq \\| \\mathbf{x} \\|_1\\) and \\(\\mathbf{A}(\\mathbf{x} + \\mathbf{\\Delta}) = \\mathbf{b}\\). The challenge is that we can not assume that \\(\\mathbf{\\Delta}\\) is sparse. We will argue that \\(\\mathbf{\\Delta}\\) is approximately sparse.\n\n\n\nLet \\(S\\) be the set of \\(k\\) non-zero indices in \\(\\mathbf{x}\\). Let \\(T_1\\) be the set of \\(2k\\) indices not in \\(S\\) with the largest absolute values in \\(\\mathbf{\\Delta}\\). Let \\(T_2\\) be the set of \\(2k\\) indices not in \\(S \\cup T_1\\) with the next largest magnitudes, et cetera.\nWe will first show that \\(\\mathbf{\\Delta}\\) is approximately sparse in the \\(\\ell_1\\)-norm. To see this, we can write\n\\[\\begin{align*}\n\\| \\mathbf{x} \\|_1 &\\geq \\| \\mathbf{x} + \\mathbf{\\Delta} \\|_1  \\\\\n&= \\| \\mathbf{x}_S + \\mathbf{\\Delta}_S \\|_1 + \\| \\mathbf{\\Delta}_{\\bar{S}} \\|_1 \\\\\n&\\geq \\| \\mathbf{x}_S \\|_1 - \\| \\mathbf{\\Delta}_S \\|_1 + \\| \\mathbf{\\Delta}_{\\bar{S}} \\|_1\n\\end{align*}\\] where the first inequality follows from assumption, the first equality follows since \\(\\mathbf{x}\\) only has non-zero entries on \\(S\\), and the second inequality follows from Tool 1. Rearranging, we have that \\(\\|\\mathbf{\\Delta}_S\\|_1 \\geq \\| \\mathbf{\\Delta}_{\\bar{S}} \\|_1\\).\nNext, we will show that \\(\\mathbf{\\Delta}\\) is approximately sparse in the \\(\\ell_2\\)-norm. To see this, we can write \\[\\begin{align*}\n\\| \\mathbf{\\Delta}_S\\|_2\n\\geq \\frac1{\\sqrt{k}} \\| \\mathbf{\\Delta}_S \\|_1\n\\geq \\frac1{\\sqrt{k}} \\| \\mathbf{\\Delta}_{\\bar{S}} \\|_1\n= \\frac1{\\sqrt{k}} \\sum_{j\\geq 1} \\| \\mathbf{\\Delta}_{T_j} \\|_1\n\\end{align*}\\] where the first inequality follows from Tool 2 and the second inequality follows from the previous \\(\\ell_1\\)-norm sparsity result. Since there are \\(2k\\) indices in \\(T_j\\), we know that $ | _{T_j} |1 2k ({T_j})$. Simultaneously, we know that \\(\\| \\mathbf{\\Delta}_{T_{j+1}} \\|_2^2 \\leq 2k \\max(\\mathbf{\\Delta}_{T_{j+1}})^2\\) so \\(\\| \\mathbf{\\Delta}_{T_{j+1}} \\|_2 \\leq \\sqrt{2k} \\max(\\mathbf{\\Delta}_{T_{j+1}})\\). Combining the last two inequalities, we have that \\[\\begin{align*}\n\\sqrt{2k} \\| \\mathbf{\\Delta}_{T_{j+1}} \\|_2\n\\leq 2k \\max(\\mathbf{\\Delta}_{T_{j+1}})\n\\leq \\| \\mathbf{\\Delta}_{T_j} \\|_1.\n\\end{align*}\\] Plugging this inequality back in, we know that \\[\\begin{align*}\n\\| \\mathbf{\\Delta}_S \\|_2 \\geq \\sqrt{2} \\sum_{j \\geq 2} \\| \\mathbf{\\Delta}_{T_j} \\|_2.\n\\end{align*}\\]\nFinally, we will show the contradiction. We know $( + ) = and \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) so \\(\\mathbf{A} \\mathbf{\\Delta} = 0\\). By the \\((3k, \\epsilon)\\)-RIP, we know that \\[\\begin{align*}\n0 &= \\| \\mathbf{A \\Delta} \\|_2\n\\\\&= \\| \\mathbf{A \\Delta}_{S \\cup T_1} \\|_2\n- \\sum_{j \\geq 2} \\| \\mathbf{A \\Delta}_{\\bar{S}} \\|_2\n\\\\&\\geq (1-\\epsilon) \\| \\mathbf{\\Delta}_{S \\cup T_1} \\|_2\n- (1+\\epsilon) \\sum_{j \\geq 2} \\| \\mathbf{\\Delta}_{T_j} \\|_2\n\\\\&\\geq (1-\\epsilon) \\| \\mathbf{\\Delta}_{S} \\|_2\n- \\frac{(1+\\epsilon)}{\\sqrt{2}} \\| \\mathbf{\\Delta}_{S} \\|_2\n\\\\&= \\left( (1-\\epsilon) - \\frac{(1+\\epsilon)}{\\sqrt{2}} \\right) \\| \\mathbf{\\Delta}_{S} \\|_2\n\\end{align*}\\] But this is a contradiction since \\(\\left( (1-\\epsilon) - \\frac{(1+\\epsilon)}{\\sqrt{2}} \\right) &gt; 0\\) for \\(\\epsilon &lt; .17\\).\nSo we know that a solution to the \\(\\ell_1\\)-minimization problem recovers \\(\\mathbf{x}\\). We can solve the \\(\\ell_1\\)-minimization problem in polynomial time using linear programming or, alternatively, running gradient descent with an \\(\\ell_1\\)-regularization term."
  },
  {
    "objectID": "notes/07_dimensionality_reduction.html",
    "href": "notes/07_dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Despite all the prior warning that low-dimensional space looks nothing like high-dimensional space, we’ll next learn about how to compress high-dimensional vectors into low-dimensional vectors.\nWe will be very careful not to compress the vectors too far. An extremely simple method known as the Johnson-Lindenstrauss random projection pushes right up to the edge of how much compression is possible.\nLet’s see the Johnson-Lindenstrauss random projection in action.\nJohnson-Lindenstrauss Lemma: Consider any set of \\(n\\) data points \\(\\mathbf{q}_1, \\ldots, \\mathbf{q}_n \\in \\mathbb{R}^d\\). There exists a linear map \\(\\mathbf{\\Pi}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\\) for \\(k = O\\left(\\frac{\\log n}{\\epsilon^2}\\right)\\) such that for all \\(i,j\\), \\[\\begin{align}\n(1-\\epsilon) \\|\\mathbf{q}_i - \\mathbf{q}_j\\|_2\n\\leq \\|\\mathbf{\\Pi} \\mathbf{q}_i - \\mathbf{\\Pi} \\mathbf{q}_j\\|_2\n\\leq (1+\\epsilon) \\|\\mathbf{q}_i - \\mathbf{q}_j\\|_2\n\\end{align}\\] with probability 9/10.\nThe reason the result is called a lemma is because Johnson and Lindenstrauss used it as a stepping stone to proving a different result. However, the lemma is immensely useful in its own right and has become a fundamental tool in machine learning.\nWe can visualize the dimensionality reduction in the figure below.\n\n\n\nThe Johnson-Lindenstrauss lemma is useful when \\(d\\) is large and \\(k\\) is small because we go from a representation \\(\\mathbf{q}_i \\in \\mathbb{R}^d\\) to a representation \\(\\mathbf{\\Pi} \\mathbf{q}_i \\in \\mathbb{R}^k\\).\nSince \\((1+\\epsilon)^2 = 1 + O(\\epsilon)\\) and \\((1-\\epsilon)^2 = 1 - O(\\epsilon)\\) for small \\(\\epsilon\\), we can write an equivalent formulation of the Johnson-Lindenstrauss lemma statement for the squared \\(\\ell_2\\)-norm:\n\\[\\begin{align}\n(1-\\epsilon) \\|\\mathbf{q}_i - \\mathbf{q}_j\\|_2^2\n\\leq \\|\\mathbf{\\Pi} \\mathbf{q}_i - \\mathbf{\\Pi} \\mathbf{q}_j\\|_2^2\n\\leq (1+\\epsilon) \\|\\mathbf{q}_i - \\mathbf{q}_j\\|_2^2.\n\\end{align}\\]\n\nClustering Application\nConsider the \\(k\\)-means clustering problem. We are given data points \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n \\in \\mathbb{R}^d\\). Let \\(C\\) be a set of clusters \\(\\{C_1, \\ldots, C_k\\}\\). The cost of the cluster is \\[\\begin{align}\n\\textrm{Cost}(C) =\n\\sum_{j=1}^k \\frac1{2 |C_j|}\n\\sum_{u,v \\in C_j} \\| \\mathbf{a}_u - \\mathbf{a}_v \\|_2^2.\n\\end{align}\\] The problem is to find a set of clusters \\(C\\) that minimizes the cost.\n\n\n\nSolving the \\(k\\)-means clustering problem is NP-hard. However, there are many good approximation algorithms. But all the approximation algorithms have at least a linear dependence on the dimension of the points \\(d\\).\nWe can use the Johnson-Lindenstrauss lemma to reduce the dimension of the data. Then we can run the \\(k\\)-means algorithm on the low-dimensional data. Formally, we’ll solve \\(k\\)-means clustering on the data set \\(\\mathbf{\\Pi} \\mathbf{a}_1, \\ldots, \\mathbf{\\Pi} \\mathbf{a}_n\\) and get a solution of clusters \\(C = \\{C_1, \\ldots, C_k \\}\\). The cost of the approximate solution is \\[\\begin{align}\n\\widetilde{\\textrm{Cost}}(C)\n= \\sum_{j=1}^k \\frac1{2 |C_j|}\n\\sum_{u,v \\in C_j} \\| \\mathbf{\\Pi} \\mathbf{a}_u - \\mathbf{\\Pi} \\mathbf{a}_v \\|_2^2.\n\\end{align}\\]\nSince the costs are all sums of squared norms, we can apply the Johnson-Lindenstrauss lemma and get that \\[\\begin{align}\n(1-\\epsilon) \\textrm{Cost}(C)\n\\leq \\widetilde{\\textrm{Cost}}(C)\n\\leq (1+\\epsilon) \\textrm{Cost}(C)\n\\end{align}\\] for all clusterings. The reason the result holds for all clusterings is because the cost of each clustering depends only on the norms of the differences between data points.\nSuppose we use an \\(\\alpha\\)-approximation algorithm to find clusters \\(C = \\{C_1, \\ldots, C_k \\}\\) such that \\[\\begin{align}\n\\widetilde{Cost}(C) \\leq (1+\\alpha)\n\\widetilde{Cost}(\\widetilde{C}^*)\n\\end{align}\\] where \\(\\widetilde{C}^*\\) is the optimal clustering for the compressed data points. Notice that we can relate the optimal clustering for the compressed data points \\(\\widetilde{C^*}\\) to the optimal clustering for the original data points \\(C^*\\). We have \\[\\begin{align}\n\\widetilde{\\textrm{Cost}}(\\widetilde{C}^*)\n\\leq \\widetilde{\\textrm{Cost}}(C^*)\n\\leq (1+\\epsilon) \\textrm{Cost}(C^*)\n\\end{align}\\] where the first inequality followed because \\(\\widetilde{C}^*\\) is the optimal solution for the compressed data points and the second inequality followed from the application of the Johnson-Lindenstrauss lemma. Now we can show that the approximate solution we get on the compressed data points is a good approximation for the optimal solution on the original data points. We have \\[\\begin{align}\n\\textrm{Cost}(C) &\\leq\n\\frac1{1-\\epsilon} \\widetilde{\\textrm{Cost}}(C) \\\\\n&\\leq (1+O(\\epsilon)) (1+\\alpha) \\widetilde{\\textrm{Cost}}(\\widetilde{C}^*) \\\\\n&\\leq (1+O(\\epsilon)) (1+\\alpha) (1+\\epsilon) \\textrm{Cost}(C^*) \\\\\n&= (1+O(\\alpha + \\epsilon)) \\textrm{Cost}(C^*).\n\\end{align}\\]\nThis is just one of the many Johnson-Lindenstrauss applications. Another popular application is to reduce the dimension for support vector machines.\n\n\nDistributional Johnson-Lindenstrauss Lemma\nNotice that the Johnson-Lindenstrauss lemma is only helpful for us if we can efficiently compute the linear map \\(\\mathbf{\\Pi}\\). Fortunately, we can construct a linear map \\(\\mathbf{\\Pi}\\) that is easy to compute. One possible construction of \\(\\mathbf{\\Pi}\\) is a random Gaussian matrix where each entry \\(\\mathbf{\\Pi}_{i,j}\\) is drawn from \\(\\mathcal{N}(0,1)\\) and rescaled by \\(\\frac1{\\sqrt{k}}\\) to preserve the norm of the vector.\nThe map \\(\\mathbf{\\Pi}\\) is oblivious to the data set. This is different from other techniques like PCA, and machine learning in general, that depend on the data set.\nThere are many other possible choices that work for the random projection: We can use random binary variables, sparse random matrices, and pseudorandom matrices. Each construction has its own advantages and disadvantages.\n\n\n\nIntuitively, the close points will remain close after the projection and the far points will remain far.\nThe stepping stone to proving the Johnson-Lindenstrauss lemma is the distributional Johnson-Lindenstrauss lemma.\nDistributional Johnson-Lindenstrauss Lemma: Let \\(\\mathbf{\\Pi} \\in \\mathbb{R}^{k \\times d}\\) be chosen so that each entry is drawn from the standard normal \\(\\mathcal{N}(0,1)\\) and rescaled by \\(\\frac1{\\sqrt{k}}\\). If we choose \\(k = O \\left( \\frac{\\log(1/\\delta)}{\\epsilon^2} \\right)\\), then for any fixed \\(\\mathbf{x} \\in \\mathbb{R}^d\\), with probability \\(1-\\delta\\), \\[\\begin{align}\n(1-\\epsilon) \\|\\mathbf{x}\\|_2^2\n\\leq \\|\\mathbf{\\Pi} \\mathbf{x}\\|_2^2\n\\leq (1+\\epsilon) \\|\\mathbf{x}\\|_2^2.\n\\end{align}\\]\nWe can use the distributional JL lemma to prove the JL lemma: Let \\(\\mathbf{x} = \\mathbf{q}_i - \\mathbf{q}_j\\) for some \\(i,j\\). Then \\[\\begin{align}\n(1-\\epsilon) \\|\\mathbf{q}_i - \\mathbf{q}_j \\|_2^2\n\\leq \\|\\mathbf{\\Pi} \\mathbf{q}_i - \\mathbf{\\Pi} \\mathbf{q}_j \\|_2^2\n\\leq (1+\\epsilon) \\|\\mathbf{q}_i - \\mathbf{q}_j \\|_2^2.\n\\end{align}\\] This only works for a single pair of points but there are roughly \\(n^2\\) pairs of points that we care about. So setting \\(\\delta = \\frac1{10 n^2}\\) and applying the union bound gives the JL lemma with \\[\nO\\left(\\frac{\\log (1/\\delta)}{\\epsilon^2}\\right)\n= O\\left(\\frac{\\log n} {\\epsilon^2}\\right).\n\\]\nWith the distributional JL lemma, we can also prove the result we used before while analyzing a Gaussian random variable. (To stay consistent with the way we presented the JL lemma, say that \\(\\mathbf{g} \\in \\mathbb{R}^k\\).) We claimed that: \\[\\begin{align}\n\\Pr \\left(\n\\| \\mathbf{g} \\|_2^2 \\leq \\frac12 \\mathbb{E}[ \\| \\mathbf{g} \\|_2^2 ]\n\\right)\n\\leq \\frac{1}{2^{c d}}\n\\end{align}\\] for some constant \\(c\\).\nTo convert this to the language of the distributional JL lemma, set \\(\\epsilon = \\frac12\\) and divide both sides of the inner inequality by \\(k\\). Recall we showed that \\(\\mathbb{E}[ \\| \\mathbf{g} \\|_2^2 ] = k\\). So \\[\\begin{align}\n\\| \\mathbf{g} \\|_2^2 \\leq \\frac12 \\mathbb{E}[ \\| \\mathbf{g} \\|_2^2 ]\n\\Leftrightarrow\n\\| \\mathbf{g} \\frac1{\\sqrt{k}} \\|_2^2 \\leq \\frac12.\n\\end{align}\\]\nNotice that the vector \\(\\mathbf{g} \\frac1{\\sqrt{k}}\\) is a random vector with each entry drawn from \\(\\mathcal{N}(0,1)\\) and rescaled by \\(\\frac1{\\sqrt{k}}\\). We can think about this event as multiplying a standard basis vector by our random projection matrix \\(\\mathbf{\\Pi}\\). Then the distributional JL lemma gives \\[\\begin{align}\n\\Pr \\left(\n\\| \\mathbf{g} \\|_2^2 \\leq \\frac12 \\mathbb{E}[ \\| \\mathbf{g} \\|_2^2 ]\n\\right)\n= \\Pr \\left(\n\\| \\mathbf{g} \\frac1{\\sqrt{k}} \\|_2^2 \\leq \\frac12\n\\right)\n\\leq \\delta.\n\\end{align}\\] We know \\(k = O(\\log(1/\\delta))\\) so solving for \\(\\delta\\) gives \\(\\delta = \\frac1{2^{c d}}\\) for some constant \\(c\\).\n\n\nProving the Distributional JL Lemma\nWe have now reduced proving the JL lemma to proving the distributional JL lemma. That is, we want to argue that \\[\\begin{align*}\n(1-\\epsilon) \\|\\mathbf{x}\\|_2^2\n\\leq \\| \\mathbf{\\Pi} \\mathbf{x} \\|_2^2\n\\leq (1+\\epsilon) \\|\\mathbf{x}\\|_2^2.\n\\end{align*}\\] with probability \\(1-\\delta\\).\nWe’ll start by showing that \\(\\mathbb{E}[ \\| \\mathbf{\\Pi} \\mathbf{x} \\|_2^2] = \\|\\mathbf{x}\\|_2^2\\). Let \\(\\frac1{\\sqrt{k}} \\mathbf{\\pi}_i\\) be the \\(i\\)th row of \\(\\mathbf{\\Pi}\\). By the way we built \\(\\mathbf{\\Pi}\\), we have that each entry in \\(\\mathbf{\\pi}_i\\) is drawn from a standard normal distribution \\(\\mathcal{N}(0,1)\\). We can then write\n\\[\\begin{align*}\n\\mathbb{E}[ \\| \\mathbf{\\Pi} \\mathbf{x} \\|_2^2 ]\n= \\sum_{i=1}^k \\frac1{k} \\mathbb{E}[ \\langle \\mathbf{\\pi}_i, \\mathbf{x}\\rangle^2 ]\n= \\frac1{k} \\sum_{i=1}^k \\mathbb{E}\\left[\n\\sum_{j=1}^d \\left( \\pi_{i}[j] \\mathbf{x}[j] \\right)^2\n\\right]\n\\end{align*}\\] where \\(\\pi_{i}[j]\\) is the \\(j\\)th entry of \\(\\mathbf{\\pi}_i\\) and is drawn from the standard normal distribution.\nWhat type of random variable is this sum? We will use the following useful fact about Gaussians.\nFact (Stability of Gaussian Random Variables): Consider Gaussian random variables \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\). Then \\[X_1 + X_2 \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2).\\]\nThe fact is intuitive but the proofs are involved.\nWe can apply this fact to our sum above. In particular, let \\[\\begin{align*}\nZ_i = \\sum_{j=1}^d \\pi_{i,j} \\mathbf{x}_j.\n\\end{align*}\\] We will briefly argue that \\(\\pi_{i}[j] \\cdot \\mathbf{x}_j \\sim \\mathcal{N}(0, \\mathbf{x}_j^2)\\). If we assume that \\(\\pi_{i}[j] \\cdot \\mathbf{x}_j\\) is still a Gaussian random variable then we know the expectation is \\(0\\) by linearity of expectation and the variance is \\(\\mathbf{x}_j^2\\) since \\(\\textrm{Var}(c X) = c^2 \\textrm{Var}(X)\\) for any random variable \\(X\\) and constant \\(c\\). We will use this observation and apply the stability of Gaussian random variables to see that \\[\\begin{align*}\nZ_i \\sim \\mathcal{N}(0, \\|\\mathbf{x}\\|_2^2).\n\\end{align*}\\] Then we can write \\[\\begin{align*}\n\\mathbb{E}\\left[ \\| \\mathbf{\\Pi} \\mathbf{x} \\|_2^2 \\right]\n= \\frac1{k} \\sum_{i=1}^k \\mathbb{E}[Z_i^2]\n= \\frac1{k} \\sum_{i=1}^k \\| \\mathbf{x} \\|_2^2\n= \\| \\mathbf{x} \\|_2^2.\n\\end{align*}\\] Here, we observed that \\(\\textrm{Var}(Z_i) = \\mathbb{E}[Z_i^2] - \\mathbb{E}[Z_i]^2 = \\mathbb{E}[Z_i^2] = \\|\\mathbf{x}\\|_2^2\\) since \\(\\mathbb{E}[Z_i] = 0\\).\nNow that we know the expectation of \\(\\| \\mathbf{\\Pi} \\mathbf{x} \\|_2^2\\), we will apply a concentration inquality to show that the random variable \\(\\| \\mathbf{\\Pi} \\mathbf{x} \\|_2^2\\). We will use a specialized concentration inequality for chi-squared random variables with \\(k\\) degrees of freedom which are simply sums of \\(k\\) squared Gaussian random variables.\nConcentration of Chi-squared Random Variables: Let \\(Z\\) be a chi-squared random variable with \\(k\\) degrees of freedom. Then \\[\\begin{align*}\n\\Pr \\left( |Z - \\mathbb{E}[Z] | \\geq \\epsilon \\mathbb{E}[Z] \\right) \\leq 2 e^{-\\epsilon^2 k / 8}.\n\\end{align*}\\]\nIn our case, \\[\\begin{align*}\nZ = \\|\\mathbf{\\Pi x}\\|_2^2\n= \\frac1{k} \\sum_{i=1}^k (\\langle \\pi_i,\\mathbf{x} \\rangle)^2\n= \\frac1{k} \\sum_{i=1}^k Z_i^2.\n\\end{align*}\\]\nApplying the concentration inequality to \\(Z\\), we have \\[\\begin{align*}\n\\Pr(|\\|\\mathbf{\\Pi x}\\|_2^2 - \\| \\mathbf{x}\\|_2^2 | \\geq \\epsilon \\| \\mathbf{x}\\|_2^2)\n= \\Pr(|Z - \\mathbb{E}[Z] | \\geq \\epsilon \\mathbb{E}[Z])\n\\leq 2 e^{-\\epsilon^2 k / 8} = \\delta.\n\\end{align*}\\]\nSolving for \\(k\\), we have \\[\\begin{align*}\n\\delta = 2 e^{-\\epsilon^2 k / 8}\n\\Leftrightarrow \\log \\frac2{\\delta} = \\epsilon^2 k / 8\n\\Leftrightarrow\nk = \\frac{8}{\\epsilon^2} \\log \\frac2{\\delta}.\n\\end{align*}\\]\nWe have now shown the distributonal JL lemma. But we may find our result surprising because we previously saw how high-dimensional geometry differs wildly from low-dimensional geometry.\nIn order to reconcile these two results, let’s imagine the following hard case of \\(n\\) mutually orthogonal unit vectors in \\(\\mathbb{R}^d\\). Let \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^d\\) be mutually orthogonal vectors so that \\[\\begin{align*}\n\\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2 = 2.\n\\end{align*}\\] The JL lemma tells us we can compress these vectors to \\(k = O(\\log n / \\epsilon^2)\\) dimensions while preserving the distances up to a factor of \\(1 \\pm \\epsilon\\). From our result for nearly orthogonal vectors, we know that there are \\(2^{O(\\epsilon^2 \\log n /\\epsilon^2)} \\geq n\\) vectors that are \\(\\epsilon\\) orthogonal in \\(O(\\log n / \\epsilon^2)\\) dimensions. So the JL lemma pushes us just to the limit of the dimension we can compress to while still approximately preserving inner products.\nAn alternate view is that, without additional structure, we expect that learning in \\(d\\) dimensions requires \\(n=2^d\\) data points. If we really had a data set that large, then the JL lemma would be vacuous since \\(\\log n = d\\).\nThe JL lemma tells us how we can preserve \\(\\ell_2\\)-norm distances between points. We’ll next see how we can preserve similarity between points."
  },
  {
    "objectID": "notes/05_concentration_inequalities.html",
    "href": "notes/05_concentration_inequalities.html",
    "title": "Concentration Inequalities",
    "section": "",
    "text": "Let’s recall Chebyshev’s inequality.\nChebyshev’s Inequality: Let \\(X\\) be a random variable with expectation \\(\\mu=\\mathbb{E}[X]\\) and variance \\(\\sigma^2 = \\textrm{Var}[X]\\). Then for any \\(k &gt; 0\\),\n\\[\n\\Pr(|X - \\mu| &gt; k \\sigma) \\leq \\frac{1}{k^2}.\n\\]\nLast time, we applied Chebyshev’s inequality to the load balancing problem. In particular, we showed if we assign \\(n\\) requests to \\(n\\) servers, the server with the maximum load has \\(O(\\sqrt{n})\\) requests with high probability. We proved this result by applying Chebyshev’s to a particular server and then applying the union bound to get a bound on the maximum load across all servers. Recall that we proved both Chebyshev’s inequality and the union bound with Markov’s inequality. So, if you squint, we just used Markov’s inequality twice.\nToday, we’ll prove a stronger result that the server with the maximum load has \\(O(\\log n)\\) requests with high probability. For this result, we’ll need a stronger concentration inquality than Chebyshev’s."
  },
  {
    "objectID": "notes/05_concentration_inequalities.html#improving-chebyshevs-inequality",
    "href": "notes/05_concentration_inequalities.html#improving-chebyshevs-inequality",
    "title": "Concentration Inequalities",
    "section": "Improving Chebyshev’s Inequality",
    "text": "Improving Chebyshev’s Inequality\nWe’ll see that Chebyshev’s inequality is accurate for some random variables. But, for many other random variables, the inequality is loose.\nOne random variable for which Chebyshev’s inequality is loose is the normal distribution.\nGaussian Tail Bound: Consider a random variable \\(X\\) drawn from the normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then for any \\(k &gt; 0\\), \\[\n\\Pr \\left( | X - \\mu | \\geq k \\sigma \\right)\n\\leq 2 e^{-k^2/2}.\n\\]\nComparing the Gaussian tail bound to Chebyshev’s inequality, we see that the Gaussian Tail Bound is exponentially better. Let’s see the difference graphically in the figure below. (Notice that the vertical access is on a logarithmic scale.) By \\(10\\) standard deviations above the mean, the Gaussian tail bound gives a bound that is 18 orders of magnitude smaller than the bound from Chebyshev’s inequality!\n\nBased on how loosely Chebyshev’s inequality bounds the Gaussian distribution, we might suspect that Chebyshev’s inequality is loose in general. But, there are examples of random variables where Chebyshev’s inequality gives the exactly the right probabilities.\nExample Random Variable: Fix a particular value \\(k\\). We’ll construct a random variable \\(X\\) with mean \\(0\\) and variance \\(1\\). In particular, let\n\\[\nX = \\begin{cases}\n-k & \\textrm{with probability } \\frac{1}{2k^2} \\\\\n0 & \\textrm{with probability } 1-\\frac{1}{k^2} \\\\\nk & \\textrm{with probability } \\frac{1}{2k^2}.\n\\end{cases}\n\\]\nWe should first check that \\(X\\) is a proper random variable and all the probabilities sum to \\(1\\). Next, notice that \\(\\mathbb{E}[X] = 0\\) and so \\[\n\\textrm{Var}[X] = \\mathbb{E}[X^2] = (-k)^2 \\cdot \\frac{1}{2k^2} + (k)^2 \\cdot \\frac{1}{2k^2} = 1.\n\\]\nThen Chebyshev’s inequality tells us that \\[\n\\Pr(|X| \\geq k) \\leq \\frac{1}{k^2}.\n\\] This is exactly true for our random variable \\(X\\) so Chebyshev’s inequality is tight for this random variable. Notice that we constructed the random variable after fixing the value \\(k\\). Do you think there’s one random variable that is tight for all values of \\(k\\)?\nWhile Chebyshev’s inequality is tight for some random variables, it is loose for many other random variables. We may therefore suspect that if we make stronger assumptions on the random variables, we can get better concentration inequalities. The central limit theorem gives us a hint about what type of random variables we should consider.\nCentral Limit Theorem: Any sum of mutually independent and identically distributed random variables \\(X_1, \\ldots, X_n\\) with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\) converges to a Gaussian random variable with mean \\(n \\cdot \\mu\\) and variance \\(n \\cdot \\sigma^2\\) as \\(k\\) goes to infinity. Formally, \\[\n\\lim_{n \\rightarrow \\infty}\n\\sum_{i=1}^n X_i \\sim \\mathcal{N}(n \\mu, n \\sigma^2).\n\\]\nBy linearity of expectation and linearity of variance, we know what the mean and variance of the sum should be. The interesting part of the central limit theorem is that the sum converges to a Gaussian distribution.\nWe stated the central limit theorem for random variables that are identically distributed so that we could cleanly describe the expectation and variance of the sum. But the central limit theorem also holds for random variables that are not identically distributed.\nFor the central limit theorem to hold, we assumed that the random variables are mutually independent. Recall that \\(X_1, \\ldots, X_n\\) are mutually independent if, for all values \\(v_1, \\ldots, v_n\\), we have \\[\n\\Pr( X_1=v_1, \\ldots, X_n = v_n)\n\\] \\[\n= \\Pr(X_1=v_1) \\cdot \\ldots \\cdot \\Pr(X_n = v_n).\n\\]\nLet’s consider the central limit in the context of the coin flip example. The figure below shows how closely the sum of \\(n\\) coin flips is approximated by the corresponding Gaussian distribution. As \\(n\\) increases, the approximation gets better and better.\n\nUsing Chebyshev’s inequality, we showed that if we flip a fair coin 100 times, the probability we see fewer than 30 or more than 70 heads is at most \\(6.25\\%\\). Let’s assume that the central limit theorem holds exactly for 100 coin flips. Then the sum of the random variables \\(X_1, \\ldots, X_{100}\\) would be distributed as \\(\\mathcal{N}(50, 25)\\). (Recall we computed that the mean is 50 and the variance is 25 using a sum of indicator random variables for the event that each flip landed heads.) By the Gaussian tail bound, \\[\n\\Pr(|X - \\mu| \\geq k \\cdot \\sigma) =\n\\Pr(|X - 50| \\geq 4 \\cdot 5 )\n\\] \\[\n\\leq 2 e^{-4^2/2} = 2 e^{-8} \\approx .06\\%\n\\] Notice that we set \\(k=4\\) to get the probability that the number of heads is more than 20 away from its mean. The probability we get using the Gaussian tail bound is much smaller than the probability we got using Chebyshev’s inequality! But, we made a big assumption that the central limit theorem holds exactly for 100 coin flips.\nLuckily, there are concentration inequalities that allow us to formally get exponentially small probability bounds.\n\nExponential Concentration\nThere are many exponential concentration inequalities. Each one makes a different set of assumptions on the random variable and, depending on how stringent the assumptions, gives a different bound. Sometimes the bounds are stated with multiplicative error and sometimes the bounds are stated with additive error. The trick is determining which bound is appropriate for the application at hand. We’ll state three exponential concentration inequalities today but there are many more. If you’re having trouble finding an appropriate bound for your case, Wikipedia is your friend.\nChernoff Bound: Let \\(X_1, \\ldots, X_n\\) be independent binary random variables. That is, \\(X_i \\in \\{0,1\\}\\). Let \\(p_i = \\mathbb{E}[X_i]\\) where \\(0 &lt; p_i &lt; 1\\). Choose a parameter \\(\\epsilon &gt; 0\\). Then the sum \\(S = \\sum_{i=1}^n X_i\\), which has mean \\(\\mu = \\sum_{i=1}^n p_i\\), satisfies \\[\n\\Pr( S \\geq (1+\\epsilon) \\mu )\n\\leq \\exp\\left(\\frac{-\\epsilon^2 \\mu}{2+\\epsilon}\\right)\n\\] and, if \\(0 &lt; \\epsilon &lt; 1\\), \\[\n\\Pr( S \\leq (1-\\epsilon) \\mu)\n\\leq \\exp\\left(\\frac{-\\epsilon^2 \\mu}{2}\\right).\n\\]\nNotice that the first inequality gives an upper bound on the pobability that \\(S\\) exceeds is expectation by a factor of \\(1+\\epsilon\\) while the second inequality gives an upper bound on the probability that \\(S\\) falls below its expectation by a factor of \\(1-\\epsilon\\).\nUsing union bound, we can combine them into a single inequality.\n\\[\n\\Pr(|S - \\mu| \\geq \\epsilon \\mu) \\leq 2 \\exp\\left(\\frac{-\\epsilon^2 \\mu}{3}\\right).\n\\]\n\nThe Chernoff bound may seem overly restrictive because we require that each variable is binary. The Bernstein inequality relaxes the assumption so that we can consider random variables defined on the inverval from \\(-1\\) to \\(1\\).\nBernstein Inequality: Let \\(X_1, \\ldots, X_n\\) be independent random variables with each \\(X_i \\in [-1, 1]\\). Let \\(\\mu = \\sum_{i=1}^n \\mathbb{E}[X_i]\\) and \\(\\sigma^2 = \\sum_{i=1}^n \\textrm{Var}[X_i]\\). Then, for any \\(k \\leq \\frac{\\sigma}{2}\\), the sum \\(S = \\sum_{i=1}^n X_i\\) satisfies \\[\n\\Pr(|S - \\mu| &gt; k \\sigma) \\leq 2 \\exp\\left(\\frac{-k^2}{4}\\right).\n\\] Notice that the bound is similar to the Chernoff bound but the denominator in the exponent is larger. As we weaken the assumptions on the random variables, we get weaker bounds.\nOf course, while weaker than Chernoff bound, the assumption in the Bernstein bound is still not very general. Hoeffding’s inequality relaxes it further.\nHoeffding’s Inequality: Let \\(X_1, \\ldots, X_n\\) be independent random variables with each \\(X_i \\in [a_i, b_i]\\). Let \\(\\mu= \\sum_{i=1}^n \\mathbb{E}[X_i]\\). Then, for any \\(k &gt;0\\), the sum \\(S = \\sum_{i=1}^n X_i\\) satisfies \\[\n\\Pr( | S - \\mu | &gt; k )\n\\leq 2 \\exp\\left(\\frac{-k^2}{\\sum_{i=1}^n (b_i - a_i)^2}\\right).\n\\]\nNotice that \\(\\sigma\\) no longer appears in the bound. In some sense, the sum of the intervals in the denominator of the exponent is a measure of the variance of the random variables.\nWe won’t see the proofs of these concentration inequalities. The general techniques are to apply Markov’s inequality to cleverly chosen random variables. Recall that we proved Chebyshev’s inequality by applying Markov’s inequality to the random variable \\((X-\\mathbb{E}[X])^2\\). Similarly, many exponential concentration inequalities are proved by defining \\(q\\)th central moments \\[\\mathbb{E}[(X-\\mathbb{E}[X])^q]\\] or functions like \\(\\exp(X - \\mathbb{E}[X])\\) and applying Markov’s inequality.\n\n\nCoin Flips\nNow that we have rigorous exponential concentration inequalities, let’s apply them to the coin flip problem.\nBiased Coin Bound: Let the probability that a coin lands heads be \\(b\\). Choose \\(n \\geq \\frac{3\\log(2/\\delta)}{\\epsilon^2}\\). If we flip a biased coin \\(n\\) times, let \\(S\\) be the number of heads. Notice that \\(\\mu = bn\\). Then \\[\n\\Pr(| S - b k | \\geq \\epsilon n) \\leq \\delta.\n\\]\nProof: We use the Chernoff bound because the number of heads is a sum of binary random variables. From the Chernoff bound, we know that for any \\(\\epsilon'\\), \\[\n\\Pr( | S - bn | \\geq \\epsilon' bn ) \\leq 2 \\exp\\left(\\frac{-\\epsilon'^2 bn}{3}\\right).\n\\] Let’s set \\(\\epsilon' = \\frac{\\epsilon}{b}\\). Then \\[\n\\Pr( | S - bn | \\geq \\epsilon n ) \\leq 2 \\exp\\left(\\frac{-\\epsilon^2 n}{3b}\\right)\n\\leq 2 \\exp\\left(\\frac{-\\epsilon^2 n}{3}\\right).\n\\] For the last inequality, we used the fact that \\(b \\leq 1\\). If we want the probability to be at most \\(\\delta\\), then we can set \\(\\delta\\) greater than the probability and solve for \\(k\\). Doing this, we see that \\[\n2 \\exp\\left(\\frac{-\\epsilon^2 n}{3}\\right) \\leq \\delta\n\\Leftrightarrow\n\\frac{-\\epsilon^2 n}{3} \\leq \\log \\frac{\\delta}{2}\n\\Leftrightarrow\nn \\geq \\frac{3 \\log(2/\\delta)}{\\epsilon^2}.\n\\]\nNotice that we have a very gentle dependence on \\(\\delta\\). If we increase \\(\\delta\\) from \\(1/10\\) to \\(1/100\\) or from \\(1/100\\) to \\(1/10000\\), then \\(\\log(2/\\delta)\\) only increases by a factor of \\(2\\).\n\n\nLoad Balancing Problem\nLet’s apply our new tool to the load balancing problem. Recall we randomly assigned \\(n\\) jobs to \\(n\\) servers using hash functions.\n\nWe defined \\(S_i\\) to be the number of jobs assigned to server \\(i\\). Our goal was to find the smallest \\(B\\) for which we could prove \\[\n\\Pr(\\max_{i \\in [n]} S_i \\geq B)\n\\leq\\frac{1}{10}.\n\\]\nUsing Chebyshev’s, we got \\(B = \\sqrt{n}\\). Let’s see if we can do better. Remember that it suffices to prove that, for any \\(i\\), \\(\\Pr(S_i \\geq B) \\leq \\frac{1}{10n}\\). To see this, observe the definition of the maximum and apply the union bound, \\[\n\\Pr(\\max_i S_i \\geq B)\n= \\Pr( S_1 \\geq B \\cup \\ldots \\cup S_n \\geq B)\n\\] \\[\n\\leq \\Pr(S_1 \\geq B) + \\ldots + \\Pr(S_n \\geq B)\n\\leq n \\frac{1}{10n} = \\frac{1}{10}.\n\\]\nConsider a single server \\(i\\). Let \\(X_j\\) be the indicator random variable that job \\(j\\) is assigned to server \\(i\\). Then the number of jobs assigned to server \\(i\\) is \\(S_i = \\sum_{j=1}^n X_j\\). Since the probability job \\(j\\) goes to server \\(i\\) is \\(\\frac{1}{n}\\), we have \\(\\mathbb{E}[X_j] = \\frac{1}{n}\\) and so \\(\\mathbb{E}[S_i] = \\sum_{j=1}^n \\frac{1}{n} = 1\\). Recall the first Chernoff bound tells us that \\[\n\\Pr(S_i \\geq 1+\\epsilon)\n\\leq \\exp\\left(\\frac{-\\epsilon^2}{2+\\epsilon}\\right)\n\\] where we plugged in \\(\\mu=1\\). We know the probability has to be at most \\(\\frac{1}{10n}\\). We upper bound the probability to avoid the addition in the denominator of the exponent then we set the upperbound to be at most \\(\\frac{1}{10n}\\). Doing this, we get \\[\n\\exp\\left(\\frac{-\\epsilon^2}{2+\\epsilon}\\right)\n\\leq \\exp\\left(\\frac{-\\epsilon^2}{2\\epsilon}\\right) \\leq \\frac{1}{10n}\n\\] as long as \\(\\epsilon \\geq 2\\). Then \\[\n\\Leftrightarrow\n-\\frac{1}{2}\\epsilon \\leq \\log \\frac{1}{10n}\n\\Leftrightarrow\n\\epsilon \\geq 2 \\log(10n).\n\\] Plugging in this choice of \\(\\epsilon\\), we have \\[\n\\Pr(S \\geq 1 + 2 \\log(10n))\n\\leq \\frac{1}{10n}.\n\\]\nSo the server with the maximum load has at most \\(O(\\log n)\\) jobs with high probability. This is much better than the \\(O(\\sqrt{n})\\) bound we proved with Chebyshev’s inequality.\nIn practice, there’s another strategy for assigning jobs to servers that works surprisingly well. The idea is called the power of two choices. Instead of assigning a job to a random server, we choose two random servers and assign the job to the least loaded server. With probability \\(1/10\\), the maximum loaded server is \\(O(\\log \\log n)\\). This is amazing! We barely changed the algorithm and we got an exponentially better bound. We may suspect that if we choose three random servers and assign the job to the least loaded server, then the maximum loaded server is \\(O(\\log \\log \\log n)\\). But, this is not the case. The bound stays asymptotatically the same. The analysis is notoriously tricky and we won’t cover it in this class. But, if you’re interested, you can find it in Section 2 of the notes here."
  },
  {
    "objectID": "notes/05_concentration_inequalities.html#fingerprinting",
    "href": "notes/05_concentration_inequalities.html#fingerprinting",
    "title": "Concentration Inequalities",
    "section": "Fingerprinting",
    "text": "Fingerprinting\nIn the rest of the class, we’ll discuss a randomized algorithm called fingerprinting. But first, we’ll revisit universal hash functions and explore prime numbers.\nUniversal Hash Functions: Consider a random hash function \\(h: \\mathcal{U} \\rightarrow \\{1, \\ldots, m\\}\\). We say \\(h\\) is universal if, for any fixed \\(x,y \\in \\mathcal{U}\\), \\[\n\\Pr(h(x) = h(y)) \\leq \\frac{1}{m}.\n\\]\nRecall that we can efficiently construct a universal hash function with the following approach. Let \\(p\\) be a prime number between \\(|\\mathcal{U}|\\) and \\(2|\\mathcal{U}|\\). Let \\(a\\) and \\(b\\) be random numbers in \\(0, \\ldots, p\\) and make sure \\(a \\neq 0\\). Then the hash function \\[\nh(x) = [(ax + b) \\mod p] \\mod m\n\\] is universal.\nWe won’t prove that \\(h\\) is universal but we will get a flavor for what tools are used in the proof.\nNotice that once we have a prime number, we only have to store \\(a\\), \\(b\\), \\(p\\), and \\(m\\). Then computing the hash function only requires a few arithmetic operations. This hash function seems easy to implement except for the challenge of finding a large prime number to use.\n\nFinding Prime Numbers\nFinding a prime number seems pretty tough so let’s consider the simpler problem of checking whether a number is prime. Given a number \\(x\\), how can we efficiently check that \\(x\\) is prime? Recall a number is prime if it is larger than 1 and can only be divided evenly by \\(1\\) and itself. The first few primes are \\(2,3,5,7,11,13,17,19,\\ldots\\). Is 2023 prime? What about 2342098230982509? How can we check in general?\nSuppose we have an integer represented as a length \\(n\\) binary string. For example, \\[\nx = 011100100110011111\\ldots0101011.\n\\] The naive prime checking algorithm of simply dividing \\(x\\) by all integers less than \\(x\\) runs in \\(O(2^{n})\\). The NYU Greene super computing cluster has 2 petaflops of throughput. When \\(n=128\\), we would need 1 million Greene clusters running for 1 million years to check if \\(x\\) is prime.\nFortunately, there is a much faster algorithm. In papers published in 1976 and 1980, Miller and Rabin presented a randomized algorithm that runs in time \\(O(n^3 \\log(1/\\delta))\\). With probability \\(1-\\delta\\), the algorithm determines if an \\(n\\)-bit integer is prime. If \\(n = 128\\) and \\(\\delta = 10^{-9}\\), then \\(n^3 \\log(1/\\delta) \\approx 60\\) million operations. We can run this in less than a tenth of a second on a modern computer! In addition to the practical applications, the Miller-Rabin algorithm was a big breakthrough that popularized randomized algorithms.\n\nWhy was it such a big deal to get an efficient algorithm for checking if a number is prime? Well, one big reason is that prime numbers form the basis for modern public key cryptography. In cryptography, we imagine there are two parties, Alice and Bob, that communicate with each other. Bob wants to send Alice a message so that only Alice can read it. If someone else intercepts it, the message should be unreadable.\n\nThe obvious way for Alice and Bob to communicate securely is to share a secret key in advance. This is the approach that persisted for centuries. However, physically meeting to share a secret key is impractical if there are many senders and receivers.\nA more clever way for Alice and Bob to communicate securely is to use a lock box. The lock box has the property that anyone can deliver mail but only Alice can read the mail.\nThe way we implement the lock box in practice is with RSA encryption. The idea is to have a private key and a public key. The private key consists of two very large prime numbers \\(p\\) and \\(q\\). The public key is the product \\(pq\\). Even though checking if a number is prime can be done quickly, we still do not have efficient algorithms for factoring numbers. (In fact, one of the reasons that quantum computers are so exciting is that they can factor numbers efficiently. But, alas, we don’t know how to build a quantum computer large enough to factor any interesting number.)\nThe challenge of RSA encryption is to find two large primes. This is the same problem we faced when we wanted to construct a hash function!\nHere’s a naive algorithm for finding primes: Pick a random large number \\(x\\) and check if it is prime. If it’s not prime, we repeat. Surprisingly, this algorithm works! The reason it works is because there are lots of primes even among large numbers. The prime number theorem formalizes this statement.\nPrime Number Theorem: Let \\(\\pi(x)\\) denote the number of primes less than some integer \\(x\\). For \\(x &gt; 55\\), \\[\n\\frac{x}{\\log x} \\leq \\pi(x)\n\\leq \\frac{x}{\\log x - 4}.\n\\]\nThis is somewhat surprising because as numbers get larger, there are more smaller numbers that could be their factors.\nLet’s plot the number of primes and the bound in the prime number thoerem. (Note the upper bound only makes sense for \\(x \\geq 55\\).)\n\nThe prime number theorem tells us that if we select a random 128 bit number \\(x\\), the chance that it is prime is greater than \\[\n\\frac{1}{\\log(2^{128})} \\geq 1/90.\n\\]\nAfter a few hundred tries, we will almost definitely find a prime number. In general, we need \\(O(n)\\) tries to find an \\(n\\)-bit prime number.\nIn the remainder of the class, we’ll discuss a simple but important application of prime numbers to hashing.\n\n\nAlgorithm\nOur goal is to construct a compact “fingerprint” \\(h(f)\\) for any given file \\(f\\) with two properties:\n\nThe fingerprints \\(h(f_1)\\) and \\(h(f_2)\\) should be different with high probability if the contents of \\(f_1\\) and \\(f_2\\) differ at all.\nIf the contents of \\(f_1\\) and \\(f_2\\) are identical, we should have \\(h(f_1) = h(f_2)\\).\n\n\nFingerprinting is useful for quickly checking if two versions of the same file are identical. This is quite helpful for version control on systems like Git. The advantage is that we do not need to communicate the entire file between the server and local computer to perform the check; we only need to communicate the small fingerprint.\nAnother application is to check if two images are identical. This is useful in contexts where we want to remove duplicate pictures. However, if the images are changed at all (e.g. compressed or converted to a different format), the fingerprints will be different. In a later class, we’ll see a method which is robust to these changes.\nThe approach for we’ll learn about today is called a Rabin fingerprint. Let the file \\(f=010\\ldots100\\) be represented as \\(n\\) bits. We’ll interpret \\(f\\) as a number between 0 and \\(2^n\\).\nWe’ll construct the fingerprint function \\(h\\) as follows. We choose a random prime number \\(p\\) between \\(2\\) and \\(t n \\log(2n)\\) for some constant \\(t\\). Then \\[\nh(f) = f \\mod p.\n\\] Notice that we can store \\(h(f)\\) in \\(\\log p\\). This is at most \\[\n\\log (tn \\log tn) \\leq \\log((tn)^2)\n\\] \\[\n= O(\\log( tn)) = O(\\log n + \\log t)\n\\] bits.\nLet’s analyze this fingerprint function.\nClaim: If \\(f_1 \\neq f_2\\), then \\(h(f_1) = h(f_2)\\) with probability at most \\(\\frac{2}{t}\\).\nSince our fingerprint only takes \\(O(\\log n + \\log t)\\) space, we can set \\(t\\) to be super large. Then, effectively, the probability of \\(h(f_1)\\) and \\(h(f_2)\\) colliding is negligible for all practical purposes.\nObserve that if \\(h(f_1) = h(f_2)\\), then \\[\nf_1 - f_2 \\mod p = 0.\n\\] In other words, we only fail if \\(d = f_1 - f_2\\) is divisible by \\(p\\).\nWe’ll analyze the chance that \\(d\\), which is an integer less than \\(2^n\\), is divisible by a random prime \\(p \\in \\{2, \\ldots, tn \\log(tn)\\}\\).\nThe first step is to upper bound the number of distinct prime factors of \\(d\\). Since each prime factor is at least \\(2\\) and \\(d\\) is at most \\(2^n\\), there can be at most \\(n\\) distinct prime factors of \\(d\\).\nThe second step is to lower bound the number of primes less than \\(tn \\log(tn)\\). By the prime number theorem, there are at least \\(\\frac{tn \\log(tn)}{\\log(tn \\log(tn))}\\) primes in this range. So the chance that a random prime \\(p\\) is a factor of \\(d\\) is at most \\[\n\\frac{n}{\\frac{tn \\log(tn)}{\\log(tn \\log(tn))}}\n= \\frac{\\log(tn \\log(tn))}{t \\log(tn)}\n\\leq \\frac{2 \\log(tn)}{t \\log(tn)} = \\frac{2}{t}.\n\\] So, for two files \\(f_1 \\neq f_2\\), the chance that \\(h(f_1) = h(f_2)\\) is at most \\(\\frac{2}{t}\\).\nLet’s see how much space we need for the fingerprint in practice. Set \\(t\\) to be \\(10^{18}\\). (For context, \\(10^{-18}\\) is the chance you win the Powerball lottery twice in a row.) Then the fingerprint size is at most \\(2 \\log_2(nt) = 2 \\log_2(n) + 2 \\log_2(10^{18})\\) bits. Suppose we are finger printing 1 megabyte image files so \\(n \\approx 8 \\times 10^6\\). Then the fingerprint size is \\(166\\) bits. This amounts to a 50,000 time reduction in space compared to sending the original file!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Randomized Algorithms for Data Science",
    "section": "",
    "text": "A course on how to leverage randomness to build fast algorithms for data science problems.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: Tuesdays and Thursdays from 2:45 to 4:00pm in Kravis 164.\nOffice Hours: Mondays and Thursdays from 12:30 to 2pm in Adams 213.\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (e.g., websites, people, LLMs).\n\n\nQuizzes: There will be short quizzes at the beginning of (randomly) selected classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\nExams: The two midterm exams are the primary method of assessing your understanding of the material.\nProject: The project offers a chance to explore an area that interests you, practice writing high quality code, and develop your ability to communicate technical ideas to an audience.\n\n\nResources: This class is based on Chris Musco’s phenomenal algorithmic machine learning and data science course at NYU. While we do not have a textbook, I have prepared written notes for every lecture; I highly recommend you read the notes before each class.\n\n\n\nWeek\n\n\nTuesday\n\n\nThursday\n\n\nSlides\n\n\nAssignments\n\n\n\n\nStreaming & Sketching\n\n\n\n\nWeek 1 (1/20 and 1/22)\n\n\nMath Review\n\n\nSet Size Estimation\n\n\nSlides\n\n\nProblem 1\n\n\n\n\nWeek 2 (1/27 and 1/29)\n\n\nSet Size Estimation\n\n\nFrequent Items\n\n\n\n\nProblem 2\n\n\n\n\nWeek 3 (2/3 and 2/5)\n\n\nFrequent Items\n\n\nDistinct Elements\n\n\n\n\n\n\n\n\nWeek 4 (2/10 and 2/12)\n\n\nDistinct Elements\n\n\nLoad Balancing\n\n\n\n\n\n\n\n\nWeek 5 (2/17 and 2/19)\n\n\nLoad Balancing\n\n\nConcentration Inequalities\n\n\n\n\n\n\n\n\nWeek 6 (2/24 and 2/26)\n\n\nConcentration Inequalities\n\n\nHigh-Dimensional Geometry\n\n\n\n\n\n\n\n\nWeek 7 (3/3 and 3/5)\n\n\nHigh-Dimensional Geometry\n\n\nMidterm Exam\n\n\n\n\n\n\n\n\nWeek 8 (3/10 and 3/12)\n\n\nDimensionality Reduction\n\n\nDimensionality Reduction\n\n\n\n\n\n\n\n\nWeek 9 (3/17 and 3/19)\n\n\nSpring Break (No Class)\n\n\nSpring Break (No Class)\n\n\n\n\n\n\n\n\nLinear Algebra & Spectral Methods\n\n\n\n\nWeek 10 (3/24 and 3/26)\n\n\nSimilarity Estimation\n\n\nSimilarity Estimation\n\n\n\n\n\n\n\n\nWeek 11 (3/31 and 4/2)\n\n\nSingular Value Decomposition\n\n\nSingular Value Decomposition\n\n\n\n\n\n\n\n\nWeek 12 (4/7 and 4/9)\n\n\nSpectral Graph Theory\n\n\nSpectral Graph Theory\n\n\n\n\n\n\n\n\nWeek 13 (4/14 and 4/16)\n\n\nSketched Regression\n\n\nSketched Regression\n\n\n\n\n\n\n\n\nWeek 14 (4/21 and 4/23)\n\n\nExplainable AI\n\n\nExplainable AI\n\n\n\n\n\n\n\n\nWeek 15 (4/28 and 4/30)\n\n\nExplainable AI\n\n\nMidterm Exam\n\n\n\n\n\n\n\n\nWeek 16 (5/5 and 5/7)\n\n\nProject Preparation\n\n\nReading Day (No Class)\n\n\n\n\n\n\n\n\nWeek 17 (5/12 and 5/14)\n\n\nFinals (No Class)\n\n\nProject Presentation 2-5pm"
  },
  {
    "objectID": "notes/18_fast_jl_transform.html",
    "href": "notes/18_fast_jl_transform.html",
    "title": "Fast Johnson-Lindenstrauss Transform",
    "section": "",
    "text": "Last class, we discussed how to speed up regression using randomization. Consider the feature matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and target vector \\(\\mathbf{b} \\in \\mathbb{R}^n\\). We wanted to find the least squares solution \\[\\mathbf{x}^* = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^d} \\|\\mathbf{Ax} - \\mathbf{b}\\|_2^2.\\] But solving the problem directly is expensive: It would take \\(O(nd^2)\\) time to compute the solution \\(\\mathbf{x}^*=(\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}\\) or \\(O(nd)\\cdot(\\# \\text{iterations})\\) time to compute the solution iteratively.\nInstead, we used a random JL matrix \\(\\mathbf{\\Pi} \\in \\mathbb{R}^{m \\times n}\\) to reduce the dimension of the problem and find \\[\n\\tilde{\\mathbf{x}} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^m} \\|\\mathbf{\\Pi Ax} - \\mathbf{\\Pi b}\\|_2^2.\n\\] It would take \\(O(md^2)\\) time to compute the solution \\(\\tilde{\\mathbf{x}}=(\\mathbf{\\Pi A}^\\top \\mathbf{\\Pi A})^{-1} \\mathbf{\\Pi A}^\\top \\mathbf{\\Pi b}\\) or \\(O(md)\\cdot(\\# \\text{iterations})\\) time to compute the solution iteratively.\nBut even computing \\(\\mathbf{\\Pi A}\\) is expensive. Since \\(\\mathbf{\\Pi}\\) is \\(m \\times n\\) and \\(\\mathbf{A}\\) is \\(n \\times d\\), it would take \\(O(mnd)=O(nd^2)\\) time to compute the matrix product \\(\\mathbf{\\Pi A}\\).\nOur goal today is to develop faster Johnson-Lindenstrauss projections.\n\n\n\nTypically, we use sparse or structured matrices instead of fully random JL matrices because they are faster to compute.\nWe will develop a method that reduces a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) down to \\(m \\approx \\frac{\\log(1/\\delta)}{\\epsilon^2}\\) dimensions in roughly \\(O(n \\log n)\\) time and guarantee that \\[\n(1-\\epsilon) \\|\\mathbf{x}\\|_2^2 \\leq \\|\\mathbf{\\Pi x}\\|_2^2 \\leq (1+\\epsilon) \\|\\mathbf{x}\\|_2^2.\n\\]\nRecall that once this bound is proven, linearity lets us preserve quantities like \\(\\| \\mathbf{y} - \\mathbf{x} \\|_2\\) and \\(\\| \\mathbf{Ax} - \\mathbf{b} \\|_2\\) for all \\(\\mathbf{x}\\).\nLet \\(\\mathbf{S} \\in \\mathbb{R}^{m \\times n}\\) be a random sampling matrix. Every row contains a value \\(s=\\sqrt{n/m}\\) in a single location and is zero everywhere else.\n\n\n\nIf we take \\(m\\) samples, then \\(\\tilde{\\mathbf{x}} = \\mathbf{Sx}\\) is a random projection that can be computed in \\(O(m)\\) time. But, there’s an issue. The approach only works well if \\(\\mathbf{x}\\) is “flat”.\n\n\n\nClaim: If \\(\\mathbf{x}_i ^2 \\leq \\frac{c}{n} \\| \\mathbf{x} \\|_2^2\\) for all \\(i\\), then \\(m = O(c \\log(1/\\delta) /\\epsilon^2)\\) samples suffice to preserve the \\(\\ell_2\\)-norm within an \\(\\epsilon\\) multiplicative factor with probability \\(1-\\delta\\).\nThe claim follows from the standard Hoeffding inequality.\nUnfortunately, we can’t guarantee that \\(\\mathbf{x}\\) is flat. However, we can multiply by a mixing matrix \\(\\mathbf{M}\\) which ensures it cannot be too concentrated in one place.\nWe will show a mixing matrix \\(\\mathbf{M}\\) that satisfies the following properties:\n\n\\(\\| \\mathbf{Mx} \\|_2^2 = \\|\\mathbf{x}\\|_2^2\\) exactly.\nEvery entry in \\(\\mathbf{Mx}\\) is bounded. That is, \\([\\mathbf{Mx}]_i^2 \\leq \\frac{c}{n} \\| \\mathbf{x} \\|_2^2\\) for some factor \\(c\\).\nWe can compute \\(\\mathbf{Mx}\\) in \\(O(n \\log n)\\) time.\n\nThen we will multiply by a subsampling matrix \\(\\mathbf{S}\\) to reduce the dimension. The projection is\n\\[\\mathbf{\\Pi x} = \\mathbf{SM x}.\\]\nGood mixing matrices should look random. In fact, for \\(\\mathbf{Mx}\\) to preserve the \\(\\ell_2\\)-norm of any \\(\\mathbf{x}\\) with high probability, \\(\\mathbf{M}\\) must be a random matrix.\nWe can see this starting from the observation that \\(\\| \\mathbf{Mx} \\|_2^2 = \\|\\mathbf{x}\\|_2^2\\) so \\(\\mathbf{M}\\) must be an orthogonal matrix. Since \\(\\mathbf{M}\\) is orthogonal, it has an inverse \\(\\mathbf{M}^{-1}\\). Then we can solve for an \\(\\mathbf{x}\\) such that \\(\\mathbf{e}_1 = \\mathbf{Mx}\\) where \\(\\mathbf{e}_1\\) is the first standard basis vector. In particular, \\(\\mathbf{x} = \\mathbf{M}^{-1} \\mathbf{e}_1\\). If \\(\\mathbf{M}\\) is known in advance, we can adversarily choose \\(\\mathbf{x}\\) to give concentrated mass to the first coordinate. Our solution is to use a random mixing matrix \\(\\mathbf{M}\\) so that, with high probability, \\(\\mathbf{Mx}\\) is flat even if \\(\\mathbf{x}\\) is not.\nWe have argued that \\(\\mathbf{M}\\) must be a random orthogonal matrix. But, for our approach to work, we need to be able to compute \\(\\mathbf{Mx}\\) quickly. So we will use a pseudorandom matrix instead.\nWe will use the mixing matrix \\(\\mathbf{M} = \\mathbf{HD}\\) where:\n\n\\(\\mathbf{D} \\in \\mathbb{R}^{n \\times n}\\) is a diagonal matrix with the diagonal entries \\(D_{i,i} = \\pm 1\\) chosen uniformly at random.\n\\(\\mathbf{H} \\in \\mathbb{R}^{n \\times n}\\) is a Hadamard matrix.\n\nThe Hadamard matrix is an orthogonal matrix closely related to the discrete Fourier matrix. It has three critical properties:\n\n\\(\\| \\mathbf{Hx} \\|_2^2 = \\|\\mathbf{x}\\|_2^2\\) exactly. Thus \\(\\| \\mathbf{HDx} \\|_2^2 = \\|\\mathbf{Dx}\\|_2^2 = \\|\\mathbf{x}\\|_2^2\\).\n\\(\\mathbf{Hx}\\) can be computed in \\(O(n \\log n)\\) time.\nAll of the entries in \\(\\mathbf{H}\\) have the same magnitude.\n\nWe will assume that \\(n\\) is a power of 2. For \\(k=0,1,\\ldots\\), the \\(k\\)th Hadamard matrix \\(\\mathbf{H}_k\\) is defined recursively as\n\\[\\begin{align*}\n\\mathbf{H}_0 &= \\begin{bmatrix} 1 \\end{bmatrix} \\\\\n\\mathbf{H}_1 &= \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\\\\n\\mathbf{H}_2 &= \\frac{1}{\\sqrt{2}} \\begin{bmatrix} \\mathbf{H}_1 & \\mathbf{H}_1 \\\\ \\mathbf{H}_1 & -\\mathbf{H}_1 \\end{bmatrix} \\\\\n\\mathbf{H}_k &= \\frac{1}{\\sqrt{2}} \\begin{bmatrix} \\mathbf{H}_{k-1} & \\mathbf{H}_{k-1} \\\\ \\mathbf{H}_{k-1} & -\\mathbf{H}_{k-1} \\end{bmatrix}.\n\\end{align*}\\]\nThe \\(n\\) by \\(n\\) Hadamard matrix has all entries equal to \\(\\pm 1/\\sqrt{n}\\).\nProperty 1: For any \\(k = 0,1,\\ldots\\), we have \\(\\| \\mathbf{H}_k \\mathbf{x} \\|_2^2 = \\|\\mathbf{x}\\|_2^2\\) for all \\(\\mathbf{x} \\in \\mathbb{R}^n\\). That is, \\(\\mathbf{H}_k\\) is orthogonal.\nWe will show the property via induction. Assume the property holds for \\(\\mathbf{H}_{k-1}\\) so \\(\\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} = \\mathbf{I}_{2^{k-1}}\\). Then \\[\\begin{align*}\n\\mathbf{H}_k^\\top \\mathbf{H}_k\n&= \\frac{1}{\\sqrt{2}} \\begin{bmatrix} \\mathbf{H}_{k-1}^\\top & \\mathbf{H}_{k-1}^\\top \\\\ \\mathbf{H}_{k-1}^\\top & -\\mathbf{H}_{k-1}^\\top \\end{bmatrix}\n\\frac{1}{\\sqrt{2}} \\begin{bmatrix} \\mathbf{H}_{k-1} & \\mathbf{H}_{k-1} \\\\ \\mathbf{H}_{k-1} & -\\mathbf{H}_{k-1} \\end{bmatrix} \\\\\n&= \\frac{1}{2} \\begin{bmatrix} \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} + \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} & \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} - \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} \\\\ \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} - \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} & \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} + \\mathbf{H}_{k-1}^\\top \\mathbf{H}_{k-1} \\end{bmatrix} \\\\\n&= \\frac{1}{2} \\begin{bmatrix} 2\\mathbf{I}_{2^{k-1}} & 0 \\\\ 0 & 2\\mathbf{I}_{2^{k-1}} \\end{bmatrix} = \\mathbf{I}_{2^{k}}.\n\\end{align*}\\]\nProperty 2: We can compute \\(\\mathbf{\\Pi x} = \\mathbf{S HDx}\\) in \\(O(n \\log n)\\) time.\nNotice that \\[\\begin{align*}\n\\mathbf{H}_kx\n&= \\frac{1}{\\sqrt{2}} \\begin{bmatrix} \\mathbf{H}_{k-1} & \\mathbf{H}_{k-1} \\\\ \\mathbf{H}_{k-1} & -\\mathbf{H}_{k-1} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x}_a \\\\ \\mathbf{x}_b \\end{bmatrix} \\\\\n&= \\frac{1}{\\sqrt{2}} \\begin{bmatrix} \\mathbf{H}_{k-1} \\mathbf{x}_a + \\mathbf{H}_{k-1} \\mathbf{x}_b \\\\ \\mathbf{H}_{k-1} \\mathbf{x}_a - \\mathbf{H}_{k-1} \\mathbf{x}_b \\end{bmatrix}\n\\end{align*}\\] where \\(\\mathbf{x}_a\\) and \\(\\mathbf{x}_b\\) are the first and second halves of \\(\\mathbf{x}\\). Since we only need to compute \\(\\mathbf{H}_{k-1} \\mathbf{x}_a\\) and \\(\\mathbf{H}_{k-1} \\mathbf{x}_b\\) each once,\\(\\mathbf{H}_k \\mathbf{x}\\) is \\(T(n) = 2T(n/2) + O(n) = O(n \\log n)\\).\nProperty 3: The randomized Hadamard matrix \\(\\mathbf{HD}\\) is a good mixing matrix for smoothing out vectors.\n\n \n\nThe figure on the left is a Hadamard matrix \\(\\mathbf{H}\\) where blue squares are \\(\\frac1{\\sqrt{n}}\\) and white squares are \\(-\\frac1{\\sqrt{n}}\\). The figure on the right is a randomized Hadamard matrix \\(\\mathbf{HD}\\).\nPseudorandom objects like this appear all the time in computer science. For example, error correcting codes, efficient hash functions, and more.\nWe will prove the following.\nSHRT Mixing Lemma: Let \\(\\mathbf{H}\\) be an \\(n \\times n\\) Hadamard matrix and \\(\\mathbf{D}\\) be an \\(n \\times n\\) diagonal matrix with \\(D_{i,i} = \\pm 1\\) chosen uniformly at random. For any \\(\\mathbf{x} \\in \\mathbb{R}^n\\), let \\(\\mathbf{z} = \\mathbf{HDx}\\). Then, with probability at least \\(1-\\delta\\), we have \\[\nz_i^2 \\leq \\frac{c \\log(n/\\delta)}{n} \\| \\mathbf{z} \\|_2^2\n\\] for all \\(i\\) where \\(c\\) is a universal constant.\nProof: Let \\(\\mathbf{h}_i^\\top\\) be the \\(i\\)th row of \\(\\mathbf{H}\\). Then \\(z_i = \\mathbf{h}_i^\\top \\mathbf{Dx}\\) and \\[\\begin{align*}\n\\mathbf{h}_i^\\top \\mathbf{D} = \\frac{1}{\\sqrt{n}} \\begin{bmatrix} 1 & 1 & \\cdots & - 1 & -1 \\end{bmatrix}\n\\begin{bmatrix} D_{1,1} \\\\ & D_{2,2} \\\\ & & \\ddots \\\\ & & & D_{n,n} \\end{bmatrix}\n\\end{align*}\\] where \\(D_{1,1}, \\ldots, D_{n,n}\\) are independent random variables with \\(D_{,j} = \\pm 1\\) chosen uniformly at random. Equivalently, \\[\\begin{align*}\n\\mathbf{h}_i^\\top \\mathbf{D} = \\frac{1}{\\sqrt{n}} \\begin{bmatrix} R_1 & R_2  & \\ldots & R_n \\end{bmatrix}\n\\end{align*}\\] where \\(R_1, \\ldots, R_n\\) are independent random variables with \\(R_j = \\pm 1\\) chosen uniformly at random. Then \\(z_i = \\frac{1}{\\sqrt{n}} \\sum_{j=1}^n R_j x_j\\). Notice that \\(z_i\\) is a random variable with mean 0 and variance \\(\\frac{1}{n} \\| \\mathbf{x} \\|_2^2\\).\nTo see this, we can write \\[\\begin{align*}\n\\mathbb{E} \\left[ z_i \\right]\n= \\frac{1}{\\sqrt{n}} \\sum_{j=1}^n x_j \\mathbb{E} \\left[ R_j \\right] = 0\n\\end{align*}\\] and, by linearity of variance, \\[\\begin{align*}\n\\textrm{Var} \\left[ z_i \\right]\n= \\frac{1}{n} \\sum_{j=1}^n x_j^2 \\textrm{Var} \\left[ R_j \\right] = \\frac{1}{n} \\sum_{i=1}^n x_i^2 = \\frac{1}{n} \\| \\mathbf{x} \\|_2^2.\n\\end{align*}\\]\nWe will apply a Bernstein type concentration inequality to prove the bound.\nRademacher Concentration: Let \\(R_1, \\ldots, R_n\\) be independent random variables with \\(R_j = \\pm 1\\) chosen uniformly at random. Then, for any \\(t &gt; 0\\) and vector \\(\\mathbf{a} \\in \\mathbb{R}^n\\), we have \\[\n\\Pr \\left(\n\\sum_{i=1}^n R_i a_i \\geq t \\| \\mathbf{a} \\|_2\n\\right)\n\\leq e^{-t^2/2}.\n\\]\nThis inequality is called the Khintchine inequality. It is specialized to sums of scaled \\(\\pm 1\\)’s and is a bit tighter and easier to apply than the generic Bernstein bound.\nApplying the inequality, we have, with probability \\(1-\\delta\\), \\[\\begin{align*}\nz_i \\leq \\sqrt{ \\frac{c \\log(n/\\delta)}{n} \\|} \\mathbf{x} \\|_2 = \\sqrt{ \\frac{c \\log(n/\\delta)}{n} \\|} \\mathbf{z} \\|_2\n\\end{align*}\\] where the second equality follows because multiplying by \\(\\mathbf{H}\\) and \\(\\mathbf{D}\\) preserve the \\(\\ell_2\\)-norm. As shown earlier, we can thus guarantee that \\[\n(1-\\epsilon) \\| \\mathbf{z} \\|_2^2 \\leq \\| \\mathbf{Sz} \\|_2^2 \\leq (1+\\epsilon) \\| \\mathbf{z} \\|_2^2\n\\] as long as \\(\\mathbf{S} \\in \\mathbb{R}^{m \\times n}\\) is a subsampled randomized Hadamard transform with \\(m = O\\left(\\frac{\\log(n/\\delta) \\log(1/\\delta)}{\\epsilon^2}\\right)\\).\nWe have \\(\\| \\mathbf{Sz} \\|_2^2 = \\| \\mathbf{SHDx} \\|_2^2 = \\| \\mathbf{\\Pi x} \\|_2^2\\) and \\(\\| \\mathbf{z} \\|_2^2 = \\| \\mathbf{x} \\|_2^2\\) so we are done.\nIn words, the SHRT mixing lemma shows that the mixed vector is very close to uniform with high probability. As we saw earlier, we can therefore argue that \\(\\| \\mathbf{S z} \\|_2^2 \\approx \\| \\mathbf{z} \\|_2^2.\\)\nThe main result then follows directly from our sampling result.\nFast JL Lemma: Let \\(\\mathbf{\\Pi} = \\mathbf{SHD} \\in \\mathbb{R}^{m \\times n}\\) be a subsampled randomized Hadamard transform with \\(m = O\\left(\\frac{\\log(n/\\delta) \\log(1/\\delta)}{\\epsilon^2}\\right)\\). Then, for any fixed \\(\\mathbf{x}\\), we have \\[\n(1-\\epsilon) \\| \\mathbf{x} \\|_2^2 \\leq \\| \\mathbf{\\Pi x} \\|_2^2 \\leq (1+\\epsilon) \\| \\mathbf{x} \\|_2^2\n\\] with probability \\(1-\\delta\\).\nWe can apply the theorem to regression. We compute \\(\\mathbf{\\Pi A}\\) in \\(O(nd \\log n)\\) time instead of \\(O(nd^2)\\) time. This is because there are \\(d\\) columns and each column can be computed in \\(O(n \\log n)\\) time. The result is quite impressive because there are only \\(O(nd)\\) entries in \\(\\mathbf{A}\\) so the projection is nearly linear.\nHowever, we may want to go faster when \\(\\mathbf{A}\\) is sparse. For this setting, Clarkson and Woodruff in 2013 showed that we can compute \\(\\mathbf{\\Pi A}\\) with an ultra-sparse matrix in \\(O(\\text{nnz}(\\mathbf{A}))\\) time where \\(\\text{nnz}(\\mathbf{A})\\) is the number of non-zero entries in \\(\\mathbf{A}\\). The proofs use totally different techniques than the Johnson-Lindenstrauss and \\(\\epsilon\\)-net arguments we used.\nWe covered this algorithm because it is simple and easy to implement. The algorithm has also been used for accelerating vector dimensionality reduction, linear algebra, locality sensitive hasing, and randomized kernel learning methods."
  },
  {
    "objectID": "notes/04_load_balancing.html",
    "href": "notes/04_load_balancing.html",
    "title": "Load Balancing",
    "section": "",
    "text": "We’ve seen how to apply linearity of variance and Chebyshev’s inequality to the distinct elements problem. In this class, we will see how to apply these tools to the load balancing problem.\nSuppose Google answers map search queries using \\(n\\) servers \\(A_1, \\ldots, A_n\\). Given a query like “New York to Rhode Island”, a common practice is to choose a random hash function \\(h: \\mathcal{U} \\rightarrow \\{1, \\ldots, q\\}\\) and to route this query to the server corresponding to its hashed value.\nOur goal is to ensure that requests are distributed evenly, so no one server gets loaded with too many requests. We want to avoid downtime and slow responses to clients.\nWhy should we use a hash function instead of just distributing requests randomly? Well, if a query has already been answered on a server, we want to direct the same query to it again so we don’t have to recompute the same answer on different servers.\n\n\nSuppose we have \\(n\\) servers and \\(m\\) requests \\(x_1, \\ldots, x_m\\). Let \\(s_i\\) be the number of requests sent to server \\(i \\in \\{1, \\ldots, n\\}\\). Formally, \\[\ns_i = \\sum_{j=1}^m \\mathbb{1}[h(x_j)=i].\n\\] Our goal is to understand the value of the maximum load on any server, which can be written as the random variable \\[\nS = \\max_{i \\in \\{1, \\ldots, n\\}} s_i.\n\\]\nA good first step in any analysis of random variables is to think about expectations. If we have \\(n\\) servers and \\(m\\) requests, for any \\(i \\in \\{1, \\ldots, n\\}\\), \\[\n\\mathbb{E}[s_i] = \\sum_{j=1}^m \\mathbb{E}\\left[\\mathbb{1}[h(x_j)=1]\\right]\n= \\frac{m}{n}.\n\\] But it’s very unclear what the expectation of \\(S\\) is. In particular, \\(\\mathbb{E}[S] \\neq \\max_{i \\in \\{1, \\ldots, n\\}} \\mathbb{E}[s_i]\\).\nCan you convince yourself that for two random variables \\(A\\) and \\(B\\), \\(\\mathbb{E}[\\max(A,B)] \\neq \\max(\\mathbb{E}[A], \\mathbb{E}[B])\\)? One example to think about is when \\(A\\) and \\(B\\) are independent variables that are each equally likely to be \\(1\\) and \\(0\\).\nIn order to reduce notation and keep the math simple, let’s assume that \\(m=n\\). That is, we assume that we have exactly the same number of servers and requests. As we did in the last problem, we’ll continue to assume we have access to a uniformly random hash function \\(h\\) so that \\(\\Pr(h(x) = h(y))=\\frac{1}{m}\\).\nWe can visualize the load balancing problem in the following figure: Each request is placed in a server uniformly and at random.\n\n\n\nWe know that the expected number of requests per server is \\(\\mathbb{E}[s_i] = \\frac{m}{n} = 1\\) under our assumption that we have the same number of requests and servers. We would like to prove \\[\n\\Pr\\left(\n\\max_{i} s_i \\geq C\n\\right) \\leq \\frac{1}{10}\n\\] where \\(C\\) is a small value (much smaller than \\(n\\)). Putting it another way, we would like to prove \\[\n\\Pr\\left( (s_1 \\geq C) \\cup (s_2 \\geq C) \\cup \\ldots \\cup (s_n \\geq C)\\right)\n\\leq \\frac{1}{10}.\n\\] Notice these statements are equivalent since the max is greater than \\(C\\) if at least one of the values is greater than \\(C\\).\nWhenever we look at the union of different events, we should immediately think of the union bound! Recall that the union bound allows us to nicely upper bound the union of events even when the events have complicated and dependent dynamics.\n\n\n\nUnion Bound: For any events \\(E_1, \\ldots, E_n\\), \\[\n\\Pr(E_1 \\cup \\ldots \\cup E_n) \\leq \\Pr(E_1) + \\ldots + \\Pr(E_n).\n\\]\n\n\n\nWe saw the proof by picture last class. This time, we’ll prove the union bound using Markov’s inequality. Ironically, three of the tools that we’ve used so far (Markov’s inequality, Chebyshev’s inequality, and the union bound) are all Markov’s inequality.\nProof: Let \\(\\mathbb{1}[E_i]\\) be the indicator random variable that event \\(E_i\\) occurs. Define \\(X= \\sum_{i=1}^n \\mathbb{1}[E_i]\\) as the number of events that occur. Markov’s inequality tells us that \\[\n\\Pr \\left( E_1 \\cup \\ldots \\cup E_n \\right)\n= \\Pr \\left( \\sum_{i=1}^n \\mathbb{1}[E_i] \\geq 1 \\right)\n\\leq \\mathbb{E}[X]\n\\] \\[\n= \\sum_{i=1}^n \\mathbb{E}[\\mathbb{1}[E_i]]\n= \\Pr(E_1) + \\ldots + \\Pr(E_n).\n\\]\n\n\n\nIf we can prove that \\(\\Pr(s_i \\geq C) \\leq \\frac{1}{10n}\\), then the union bound immediately gives the last two bounds that we wanted. Why? Well, \\[\n\\Pr(\\max_i s_i \\geq C) \\leq \\sum_{i=1}^n \\Pr(s_i \\geq C)\n\\leq \\sum_{i=1}^n \\frac{1}{10n} = \\frac{1}{10}\n\\] where the second inequality follows by the union bound.\nIt should look hard to prove \\(\\Pr(s_i \\geq C) \\leq \\frac{1}{10n}\\). This implies that \\(s_i &lt; C\\) for a particular server \\(i\\) with very high probabiity \\(1- \\frac{1}{10n}\\). Unfortunately, Markov’s inequality is too weak for this. We’ll instead use Chebyshev’s inequality but we’ll first have to understand the variance \\(\\textrm{Var}(s_i) = \\sigma^2\\). Let \\(s_{i,j}\\) be the indicator random variable that the \\(j\\)th request goes to the \\(i\\)th server. We have \\[\ns_i = \\sum_{j=1}^n s_{i,j}.\n\\] Since each \\(s_{i,j}\\) and \\(s_{i,j'}\\) are independent, we can apply linearity of variance and get \\[\n\\textrm{Var}(s_i)\n=\\sum_{j=1}^n \\textrm{Var}(s_{i,j}).\n\\] We know that server \\(i\\) gets request \\(j\\) with probability \\(\\frac{1}{n}\\) so \\(s_i = 1\\) with probability \\(\\frac{1}{n}\\) and \\(s_i=0\\) otherwise. This tells us that \\(\\mathbb{E}[s_{i,j}] = \\frac{1}{n}\\) and \\(\\mathbb{E}[s_{i,j}^2] = \\frac{1}{n}\\). Then \\[\n\\textrm{Var}(s_{i,j}) = \\frac{1}{n} - \\frac{1}{n^2} \\leq \\frac{1}{n}\n\\] and \\[\n\\textrm{Var}(s_i) \\leq n \\frac{1}{n} = 1.\n\\] Similarly, we know \\(\\mathbb{E}[s_i] = 1\\).\nApplying Chebyshev’s inequality gives \\[\n\\Pr\\left(\n|s_i - 1| \\geq k\n\\right)\n\\leq \\frac{1}{k^2}.\n\\]\nSetting \\(k=\\sqrt{10n}\\) gives \\[\n\\Pr(|s_i - 1| \\geq \\sqrt{10n}) \\leq \\frac{1}{10n}\n\\implies\n\\Pr(s_i \\geq \\sqrt{10n}+1) \\leq \\frac{1}{10n}\n\\] By the union bound argument from before, we have \\[\n\\Pr \\left(\n\\max_{i \\in \\{1, \\ldots, n\\}} s_i \\geq \\sqrt{10n} + 1\n\\right)\n\\leq \\frac{1}{10}.\n\\]\nIn words, we proved that when hashing \\(n\\) requests into \\(n\\) servers, the server with the maximum number of requests contains no more than \\(\\sqrt{10n} + 1\\) requests with probability \\(\\frac{9}{10}\\).\nWe analyzed and solved the load balancing problem which is important and widely applicable. But the real value of what we did today is not the specific problem we solved but the techniques we used to solve them:\n\nWe used the union bound to control the maximum of many random variables.\nWe used Chebyshev’s inequality to bound a variable whose variance we could can compute.\nIn order to compute the variance, we took a random variable and decomposed it into a sum of independent random variables so that we could apply linearity of variance."
  },
  {
    "objectID": "notes/04_load_balancing.html#load-balancing",
    "href": "notes/04_load_balancing.html#load-balancing",
    "title": "Load Balancing",
    "section": "",
    "text": "We’ve seen how to apply linearity of variance and Chebyshev’s inequality to the distinct elements problem. In this class, we will see how to apply these tools to the load balancing problem.\nSuppose Google answers map search queries using \\(n\\) servers \\(A_1, \\ldots, A_n\\). Given a query like “New York to Rhode Island”, a common practice is to choose a random hash function \\(h: \\mathcal{U} \\rightarrow \\{1, \\ldots, q\\}\\) and to route this query to the server corresponding to its hashed value.\nOur goal is to ensure that requests are distributed evenly, so no one server gets loaded with too many requests. We want to avoid downtime and slow responses to clients.\nWhy should we use a hash function instead of just distributing requests randomly? Well, if a query has already been answered on a server, we want to direct the same query to it again so we don’t have to recompute the same answer on different servers.\n\n\nSuppose we have \\(n\\) servers and \\(m\\) requests \\(x_1, \\ldots, x_m\\). Let \\(s_i\\) be the number of requests sent to server \\(i \\in \\{1, \\ldots, n\\}\\). Formally, \\[\ns_i = \\sum_{j=1}^m \\mathbb{1}[h(x_j)=i].\n\\] Our goal is to understand the value of the maximum load on any server, which can be written as the random variable \\[\nS = \\max_{i \\in \\{1, \\ldots, n\\}} s_i.\n\\]\nA good first step in any analysis of random variables is to think about expectations. If we have \\(n\\) servers and \\(m\\) requests, for any \\(i \\in \\{1, \\ldots, n\\}\\), \\[\n\\mathbb{E}[s_i] = \\sum_{j=1}^m \\mathbb{E}\\left[\\mathbb{1}[h(x_j)=1]\\right]\n= \\frac{m}{n}.\n\\] But it’s very unclear what the expectation of \\(S\\) is. In particular, \\(\\mathbb{E}[S] \\neq \\max_{i \\in \\{1, \\ldots, n\\}} \\mathbb{E}[s_i]\\).\nCan you convince yourself that for two random variables \\(A\\) and \\(B\\), \\(\\mathbb{E}[\\max(A,B)] \\neq \\max(\\mathbb{E}[A], \\mathbb{E}[B])\\)? One example to think about is when \\(A\\) and \\(B\\) are independent variables that are each equally likely to be \\(1\\) and \\(0\\).\nIn order to reduce notation and keep the math simple, let’s assume that \\(m=n\\). That is, we assume that we have exactly the same number of servers and requests. As we did in the last problem, we’ll continue to assume we have access to a uniformly random hash function \\(h\\) so that \\(\\Pr(h(x) = h(y))=\\frac{1}{m}\\).\nWe can visualize the load balancing problem in the following figure: Each request is placed in a server uniformly and at random.\n\n\n\nWe know that the expected number of requests per server is \\(\\mathbb{E}[s_i] = \\frac{m}{n} = 1\\) under our assumption that we have the same number of requests and servers. We would like to prove \\[\n\\Pr\\left(\n\\max_{i} s_i \\geq C\n\\right) \\leq \\frac{1}{10}\n\\] where \\(C\\) is a small value (much smaller than \\(n\\)). Putting it another way, we would like to prove \\[\n\\Pr\\left( (s_1 \\geq C) \\cup (s_2 \\geq C) \\cup \\ldots \\cup (s_n \\geq C)\\right)\n\\leq \\frac{1}{10}.\n\\] Notice these statements are equivalent since the max is greater than \\(C\\) if at least one of the values is greater than \\(C\\).\nWhenever we look at the union of different events, we should immediately think of the union bound! Recall that the union bound allows us to nicely upper bound the union of events even when the events have complicated and dependent dynamics.\n\n\n\nUnion Bound: For any events \\(E_1, \\ldots, E_n\\), \\[\n\\Pr(E_1 \\cup \\ldots \\cup E_n) \\leq \\Pr(E_1) + \\ldots + \\Pr(E_n).\n\\]\n\n\n\nWe saw the proof by picture last class. This time, we’ll prove the union bound using Markov’s inequality. Ironically, three of the tools that we’ve used so far (Markov’s inequality, Chebyshev’s inequality, and the union bound) are all Markov’s inequality.\nProof: Let \\(\\mathbb{1}[E_i]\\) be the indicator random variable that event \\(E_i\\) occurs. Define \\(X= \\sum_{i=1}^n \\mathbb{1}[E_i]\\) as the number of events that occur. Markov’s inequality tells us that \\[\n\\Pr \\left( E_1 \\cup \\ldots \\cup E_n \\right)\n= \\Pr \\left( \\sum_{i=1}^n \\mathbb{1}[E_i] \\geq 1 \\right)\n\\leq \\mathbb{E}[X]\n\\] \\[\n= \\sum_{i=1}^n \\mathbb{E}[\\mathbb{1}[E_i]]\n= \\Pr(E_1) + \\ldots + \\Pr(E_n).\n\\]\n\n\n\nIf we can prove that \\(\\Pr(s_i \\geq C) \\leq \\frac{1}{10n}\\), then the union bound immediately gives the last two bounds that we wanted. Why? Well, \\[\n\\Pr(\\max_i s_i \\geq C) \\leq \\sum_{i=1}^n \\Pr(s_i \\geq C)\n\\leq \\sum_{i=1}^n \\frac{1}{10n} = \\frac{1}{10}\n\\] where the second inequality follows by the union bound.\nIt should look hard to prove \\(\\Pr(s_i \\geq C) \\leq \\frac{1}{10n}\\). This implies that \\(s_i &lt; C\\) for a particular server \\(i\\) with very high probabiity \\(1- \\frac{1}{10n}\\). Unfortunately, Markov’s inequality is too weak for this. We’ll instead use Chebyshev’s inequality but we’ll first have to understand the variance \\(\\textrm{Var}(s_i) = \\sigma^2\\). Let \\(s_{i,j}\\) be the indicator random variable that the \\(j\\)th request goes to the \\(i\\)th server. We have \\[\ns_i = \\sum_{j=1}^n s_{i,j}.\n\\] Since each \\(s_{i,j}\\) and \\(s_{i,j'}\\) are independent, we can apply linearity of variance and get \\[\n\\textrm{Var}(s_i)\n=\\sum_{j=1}^n \\textrm{Var}(s_{i,j}).\n\\] We know that server \\(i\\) gets request \\(j\\) with probability \\(\\frac{1}{n}\\) so \\(s_i = 1\\) with probability \\(\\frac{1}{n}\\) and \\(s_i=0\\) otherwise. This tells us that \\(\\mathbb{E}[s_{i,j}] = \\frac{1}{n}\\) and \\(\\mathbb{E}[s_{i,j}^2] = \\frac{1}{n}\\). Then \\[\n\\textrm{Var}(s_{i,j}) = \\frac{1}{n} - \\frac{1}{n^2} \\leq \\frac{1}{n}\n\\] and \\[\n\\textrm{Var}(s_i) \\leq n \\frac{1}{n} = 1.\n\\] Similarly, we know \\(\\mathbb{E}[s_i] = 1\\).\nApplying Chebyshev’s inequality gives \\[\n\\Pr\\left(\n|s_i - 1| \\geq k\n\\right)\n\\leq \\frac{1}{k^2}.\n\\]\nSetting \\(k=\\sqrt{10n}\\) gives \\[\n\\Pr(|s_i - 1| \\geq \\sqrt{10n}) \\leq \\frac{1}{10n}\n\\implies\n\\Pr(s_i \\geq \\sqrt{10n}+1) \\leq \\frac{1}{10n}\n\\] By the union bound argument from before, we have \\[\n\\Pr \\left(\n\\max_{i \\in \\{1, \\ldots, n\\}} s_i \\geq \\sqrt{10n} + 1\n\\right)\n\\leq \\frac{1}{10}.\n\\]\nIn words, we proved that when hashing \\(n\\) requests into \\(n\\) servers, the server with the maximum number of requests contains no more than \\(\\sqrt{10n} + 1\\) requests with probability \\(\\frac{9}{10}\\).\nWe analyzed and solved the load balancing problem which is important and widely applicable. But the real value of what we did today is not the specific problem we solved but the techniques we used to solve them:\n\nWe used the union bound to control the maximum of many random variables.\nWe used Chebyshev’s inequality to bound a variable whose variance we could can compute.\nIn order to compute the variance, we took a random variable and decomposed it into a sum of independent random variables so that we could apply linearity of variance."
  },
  {
    "objectID": "notes/02_frequent_items.html",
    "href": "notes/02_frequent_items.html",
    "title": "Frequent Items Estimation",
    "section": "",
    "text": "The frequent items problem is to identify the items that appear most often in a stream. For example, we may want to find the most popular products on Amazon, the most watched videos on YouTube, or the most searched queries on Google. We process each item as it appears in the stream and, at any moment, our goal is to return the most frequent items without having to scan through a database.\nThe obvious algorithm for the frequent items problem is to store each item and the number of times we’ve seen it. The issue is that we would then need space linear in the number of unique items. If the items are pairs of products, for example, then the space scales quadratically. If the items are triplets of videos, then the space scales cubically. Clearly, we need a more efficient algorithm. It turns out that we can’t solve the problem exactly with less space but we can solve the problem approximately.\nConsider a stream of \\(n\\) items \\(x_1, \\ldots, x_n\\). We’ll call the set of all items in the stream \\(U\\). Let \\(k\\) be a positive integer and \\(\\epsilon &gt; 0\\) be a small constant. The frequent items estimation problem is to return\n\nevery item that appears at least \\(\\frac{n}{k}\\) times and\nonly items that appear at least \\((1-\\epsilon)\\frac{n}{k}\\) times.\n\nWe’ll see how to use a randomized hashing algorithm to solve the problem. The algorithm addresses the slightly different problem of estimating the frequency of any item \\(v\\) in the stream. Formally, let \\(f(v) = \\sum_{i=1}^n \\mathbb{1}[x_i=v]\\) be the number of times item \\(v\\) appears in the stream. Here, we use the notation \\(\\mathbb{1}[x_i=v]\\) to denote the indicator random variable that the \\(i\\)th item in the stream is \\(v\\). An indicator random variable is 1 if the event happens and 0 otherwise.\nOur goal is to return an estimate \\(\\hat{f}(v)\\) so that \\(f(v) \\leq \\hat{f}(v) \\leq f(v) + \\frac{\\epsilon}{k} n\\) with high probability. If we have these estimates, observe that we can can solve the frequent items estimation problem simply by returning all items for which \\(\\hat{f}(v) \\geq \\frac{n}{k}\\).\nThe key ingredient of the algorithm is hash functions.\n\nHash Functions\nLet the hash function \\(h\\) be a random function from the set of all items \\(\\mathcal{U}\\) to the integers \\(\\{1,\\ldots, m\\}\\). The hash function is constructed using a seed of random numbers and then the function is fixed. Given an item \\(x\\), the hash function always returns the same hashed output \\(h(x)\\).\nDefinition: A hash function \\(h: \\mathcal{U} \\to \\{1, \\ldots, m\\}\\) is uniform random if\n\n\\(\\Pr(h(x)=i) = \\frac{1}{m}\\) for all items \\(x\\) and \\(i \\in \\{1, \\ldots, m\\}\\) and\n\\(h(x)\\) and \\(h(y)\\) are independent random variables for all \\(x,y \\in U\\).\n\nNotice that the independence condition implies \\(\\Pr(h(x)=h(y)) = \\frac{1}{m}\\).\nIn general, it is not possible to efficiently implement uniform random hash functions. But, for our application, we only need a universal hash function which can be implemented efficiently. Let \\(p\\) be a prime number between \\(|\\mathcal{U}|\\) and \\(2|\\mathcal{U}|\\). Choose \\(a,b\\) randomly from \\(0,\\ldots,p\\) so that \\(a \\neq 0\\), then define the hash function \\[h(x) = (a x + b \\mod p) \\mod m.\\] Check out these lecture notes to learn why \\(\\Pr(h(x) = h(y)) \\leq \\frac{1}{m}\\) for this hash function. (As we’ll soon see, this is the condition we need for the algorithm.)\n\n\nCount-Min Sketch\nWith handy hash functions, we’re now ready to describe the algorithm. We first choose a random hash function \\(h\\) and initialize an \\(m\\)-length array with \\(0\\) in every cell. Items arrive in a stream and we hash each one to one of the \\(m\\) cells in the array. We then add 1 to the number in the cell. Formally, given item \\(x_i\\), we set \\(A[h(x_i)] = A[h(x_i)]+1\\).\n\n\n\nIn the figure, Amazon products appear one after the other in a stream. We hash the fish tank to the second cell in the array, the basketball to the first cell, and so on. Crucially, when we see the fish tank again, we hash it to the same cell.\nAfter processing the stream, our estimate for the frequency of item \\(v\\) is \\(\\hat{f}(v) = A[h(v)]\\). Notice that \\(\\hat{f}(v) \\geq f(v)\\) since every time we saw item \\(v\\), we added 1 to the corresponding cell in the array. The inequality appears because we could have overcounted when other items hash to the same cell.\nFormally, our estimate for the frequency is \\[\n\\hat{f}(v) = f(v) + \\sum_{y \\in \\mathcal{U} \\setminus v} f(y) \\mathbb{1}[h(y)=h(v)].\n\\] Our estimate for the frequency contains the true frequency plus an error term for all the other items that are hashed to the same cell.\nLet’s take the expectation of the error term: \\[\n\\mathbb{E}\\left[\\sum_{y \\in U \\setminus v} f(y) \\mathbb{1}[h(y)=h(v)]\\right]\n= \\sum_{y \\in \\mathcal{U} \\setminus v} f(y) \\mathbb{E}[\\mathbb{1}[h(y)=h(v)]]\n\\] \\[\n\\leq \\sum_{y \\in \\mathcal{U} \\setminus v} f(y) \\frac{1}{m} = \\frac{1}{m} \\sum_{y \\in \\mathcal{U} \\setminus v} f(y) \\leq \\frac{n}{m}.\n\\] We used linearity of expectation, the special property of indicator random variables and the probability of collisions in our hash function in the first inequality. We used that the sum of frequencies has to sum to \\(n\\) in the last equality.\nWe have a non-negative random variable and a bound on its expectation, let’s apply Markov’s inequality to the error term\n\\[\n\\Pr\\left( \\sum_{y \\in \\mathcal{U} \\setminus v} f(y) \\mathbb{1}[h(y)=h(v)] \\geq \\frac{2n}{m} \\right)\n\\leq \\frac{n/m}{2n/m} = \\frac{1}{2}.\n\\] So we’ve shown that our estimate \\(A[h(v)]\\) satisfies\n\\[\nf(v) \\leq A[h(v)] \\leq f(v) + \\frac{2n}{m}\n\\] with probability \\(\\frac{1}{2}\\) for any \\(v\\). If we set \\(m = \\frac{2k}{\\epsilon}\\), we can solve the problem with error \\(\\frac{\\epsilon n}{k}\\). But our success probability is a little low, how can we improve it?\nA common approach in many randomized algorithms is to boost the success probability by repeating the core subroutine. In our case, we’ll maintain \\(t\\) independent hash functions and arrays.\n\n\n\nAs depicted in the figure, each item gets hashed into one cell in every array using the respective hash function. So the update on item \\(x_i\\) for every array \\(j \\in \\{1, \\ldots, t\\}\\) is \\(A_j[h_j(x_i)] = A_j[h_j(x_i)] + 1\\).\nThen, when we’re computing our estimate for the frequnecy of an item, we look at each cell it appears in and take the minimum. Formally, \\(\\hat{f}(v) = \\min A_j[h_j(v)]\\). We take the minimum because each array only has one-sided error; its estimate for the frequency is never smaller than the true frequency. If our estimates instead had two-sided error, we would prefer to take the mean or median.\nFor every array \\(j\\) and item \\(v\\) when we set \\(m=\\frac{2k}{\\epsilon}\\), we know that \\(f(v) \\leq A_j[h_j(v)] \\leq f(v) + \\frac{\\epsilon n}{k}\\) with probability \\(\\frac{1}{2}\\). Let’s compute the probability that our final estimate has this much error.\n\\[\n\\Pr \\left( \\hat{f}(v) &gt; f(v) + \\frac{\\epsilon n}{k} \\right)\n\\leq \\left( \\frac{1}{2} \\right)^t\n\\] Call the event that the frequency estimate has error more than \\(\\frac{\\epsilon n}{k}\\) an error. The inequality holds since our final estimate fails only if every single one of the arrays also independently fails. If we set \\(t=\\log_2{1/\\delta}\\) for some small constant \\(\\delta &gt; 0\\), then the failure probability is \\(\\delta\\). The success probability, the complement of the failure probability, is \\(1-\\delta\\).\nPutting it all together, we just proved that count-min sketch lets us estimate the frequency of each item in the stream up to error \\(\\frac{\\epsilon n}{k}\\) with probability at least \\(1-\\delta\\) in \\(O(mt) = O(\\log(1/\\delta) \\frac{k}{\\epsilon})\\) space. The first term in the space complexity comes from the number of arrays \\(t\\) and the second term comes from the space per array \\(m\\).\nHowever, this guarantee is only for a single item \\(v\\). We really want a guarantee for all items.\n\n\nUnion Bound\nAnother simple and powerful tool in randomized algorithm design is the union bound. The union bound allows us to easily analyze the complicated dynamics of many random events.\nLemma: For any random events \\(E_1, E_2, \\ldots, E_k\\),\n\\[\n\\Pr(E_1 \\cup E_2 \\cup \\ldots \\cup E_k)\n\\leq \\Pr(E_1) + \\Pr(E_2) + \\ldots + \\Pr(E_k).\n\\]\nProof: We’ll give a “proof by picture”. Each circle represents the outcome space corresponding to event \\(E_i\\). The total outcome space covered by any overlapping events is at most the outcome space covered by the events if they were to not overlap.\n\n\n\nLet’s apply the union bound to the total failure probability of our estimate. The algorithm fails if \\(\\hat{f}(v_i) &gt; \\frac{\\epsilon n }{k}\\) for any item \\(v_i\\). By the union bound, \\[\n\\Pr \\left(\n\\textrm{fail for } v_1\n\\cup \\textrm{fail for } v_2\n\\cup \\ldots \\cup\n\\textrm{fail for } v_{|\\mathcal{U}|}\n\\right)\n\\] \\[\n\\leq \\Pr(\\textrm{fail for } v_1)\n+ \\Pr(\\textrm{fail for } v_2)\n+ \\ldots +\n\\Pr(\\textrm{fail for } v_{|\\mathcal{U}|})\n\\] \\[\n= \\delta + \\delta + \\ldots + \\delta = |\\mathcal{U}| \\delta \\leq n \\delta.\n\\] Let’s set \\(\\delta=\\frac{1}{10n}\\). With probability \\(9/10\\), count-min sketch lets us estimate the frequency of all items in a stream up to error \\(\\frac{\\epsilon n}{k}\\). Recall this is is accurate enough to solve the frequent items estimation problem if we just return all items \\(v\\) with estimated frequency at least \\(\\frac{n}{k}\\)."
  },
  {
    "objectID": "notes/11_online_and_stochastic.html",
    "href": "notes/11_online_and_stochastic.html",
    "title": "Online and Stochastic Gradient Descent",
    "section": "",
    "text": "Recall the standard optimization setting. We are given a function \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) to minimize, a function oracle and a gradient oracle. The function oracle returns \\(f(\\mathbf{x})\\) and the gradient oracle returns \\(\\nabla f(\\mathbf{x})\\) for any input \\(\\mathbf{x} \\in \\mathbb{R}^d\\). The goal is to minimize the number of calls to the oracles to find an output \\(\\hat{\\mathbf{x}}\\) such that \\[f(\\hat{\\mathbf{x}}) \\leq \\min_{\\mathbf{x}} f(\\mathbf{x}) + \\epsilon\\] for \\(\\epsilon &gt; 0.\\)\nIn machine learning applications, the function \\(f\\) is typically a loss function defined for a particular training dataset. Today, we’ll discuss the online setting where the dataset changes over time. Examples of the online setting include:"
  },
  {
    "objectID": "notes/11_online_and_stochastic.html#online-learning",
    "href": "notes/11_online_and_stochastic.html#online-learning",
    "title": "Online and Stochastic Gradient Descent",
    "section": "Online Learning",
    "text": "Online Learning\nConsider a model \\(M_\\mathbf{x}\\) parameterized by parameters \\(\\mathbf{x}\\). We want to find good parameters to minimize a loss function \\(\\ell\\) but the data we are optimizing with respect to is changing over time. At each step \\(t=1,\\ldots,T\\), we receive data vectors \\(\\mathbf{a}^{(1)}, \\ldots, \\mathbf{a}^{(T)}\\). For each step \\(t\\), we choose a parameter vector \\(\\mathbf{x}^{(t)}\\). After we make the prediction \\(\\hat{y}^{(t)} = M_{\\mathbf{x}^{(t)}}(\\mathbf{a}^{(t)})\\), we receive the true label \\(y^{(t)}\\). We then use the information to choose the new parameter vector \\(\\mathbf{x}^{(t+1)}\\) for the next time step. The goal is to minimize the cumulative loss \\[\n\\mathcal{L} = \\sum_{t=1}^T \\ell(\\hat{y}^{(t)}, y^{(t)}).\n\\] This framework works for many loss functions and models. One popular setting is linear regression where \\[\n\\ell(\\hat{y}^{(t)}, y^{(t)}) =\n||\\langle \\mathbf{x}^{(t)}, \\mathbf{a}^{(t)} \\rangle - y^{(t)}||^2.\n\\] If the model was a neural network, we could update the model to make it more complicated. If the task was classification, we could use cross-entropy loss.\nLet’s abstract the online setting and analyze it. Instead of a single objective function with different data points, we have a sequence of objective functions \\(f^{(1)}, \\ldots, f^{(T)}: \\mathbb{R}^d \\to \\mathbb{R}\\). For time step \\(t=1,\\ldots, T\\), we select a vector \\(\\mathbf{x}^{(t)} \\in \\mathbb{R}^d\\). We then observe \\(f_t\\) and pay cost \\(f_t(\\mathbf{x}^{(t)})\\). The goal is to minimize the total cost \\[\n\\sum_{t=1}^T f_t(\\mathbf{x}^{(t)}).\n\\]\nIn the offline optimization setting, we wanted to find parameters \\(\\hat{\\mathbf{x}}\\) that approximately minimized the function \\(f\\). We will ask for a similar guarantee in the online setting. We want to choose parameters \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(T)}\\) such that \\[\n\\sum_{t=1}^T f_t(\\mathbf{x}^{(t)})\n\\leq \\left( \\min_{\\mathbf{x}} \\sum_{t=1}^T f_t(\\mathbf{x}) \\right) + \\epsilon.\n\\] Amazingly, we will make no assumptions that \\(f_1, \\ldots, f_T\\) are related to each other at all!\nIn the online setting, the error \\(\\epsilon\\) is called the regret of our solution sequence \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(T)}\\). Notice that the regret compares our solution sequence to the best fixed parameter in hindsight. Typically, we want \\(\\epsilon\\) to grow sublinearly in \\(T\\) so that the average regret goes to \\(0\\) as the number of iterations increases.\nA surprising characteristic of the formulation is that the solution sequence could actually give a better solution than the best fixed parameter in hindsight. Perhaps we could hope for a stronger gurantee such as \\[\n\\sum_{t=1}^T f_t(\\mathbf{x}^{(t)})\n\\leq \\left( \\sum_{t=1}^T \\min_{\\mathbf{x}} f_t(\\mathbf{x}) \\right) + \\epsilon.\n\\] The second guarantee differs from the first guarantee in that we are comparing our solution sequence to the optimal solution sequence that can vary wildly from one time step to the next.\nUnfortunately, the above guarantee is not possible in general. Consider convex functions \\[\nf_{t}(x) = | x- h_t|\n\\] where \\(h_t\\) is a sequence of random numbers sampled uniformly from 0 to 1. The right hand side of the above inequality is \\(0\\) since a optimal dynamic sequence can always select \\(x^{(t)} = h_t\\). However, the left hand side is \\(\\Omega(T)\\) since the online solution must pay \\(|x^{(t)} - h_t|\\) at each time step. (Because \\(h_t\\) is random, the online solution cannot predict \\(h_t\\) with average error less than \\(\\frac12\\).)\nSo we’ll settle for the first, weaker guarantee that \\[\n\\sum_{t=1}^T f_t(\\mathbf{x}^{(t)})\n\\leq \\left( \\min_{\\mathbf{x}} \\sum_{t=1}^T f_t(\\mathbf{x}) \\right) + \\epsilon.\n\\] There is a beautiful balance in the guarantee: If \\(f_1, \\ldots, f_T\\) are similar, we can learn to predict \\(f_t\\) well and we can hope for a small \\(\\epsilon\\). On the other hand, if \\(f_1, \\ldots, f_T\\) are different, we can’t hope for a small \\(\\epsilon\\) but \\(\\min_{\\mathbf{x}} \\sum_{t=1}^T f_t(\\mathbf{x})\\) will be large. Often times, we will be in the middle of the two extremes.\nConsider the following simple algorithm for the online problem. We start by choosing an initial parameter vector \\(\\mathbf{x}^{(0)}\\). Then for \\(t=1,\\ldots, T\\), we choose parameters \\[\n\\mathbf{x}^{(t)}\n= \\arg \\min_\\mathbf{x}\n\\sum_{j=1}^{t-1} f_{j_t}(\\mathbf{x}).\n\\] The above algorithm is called follow the leader. While it is simple and intuitive, the algorithm has two issues: one is related to computational complexity and one is related to accuracy. In terms of computations, the algorithm requires us to solve a convex optimization problem with on average \\(O(T)\\) terms at each time step for total of \\(O(T^2)\\) complexity. In terms of accuracy, the algorithm can overfit to the data. Consider the figure below where we are alternately given functions \\(f_a\\) and \\(f_b\\). If we optimize after receiving slightly more \\(f_a\\) functions, then we will overfit to \\(f_a\\) and do poorly on \\(f_b\\) exactly when we need to do well on \\(f_b\\).\n\n\n\nLuckily, we already know an algorithm which can be modified to address both issues."
  },
  {
    "objectID": "notes/11_online_and_stochastic.html#online-gradient-descent",
    "href": "notes/11_online_and_stochastic.html#online-gradient-descent",
    "title": "Online and Stochastic Gradient Descent",
    "section": "Online Gradient Descent",
    "text": "Online Gradient Descent\nOnline gradient descent is a modification of gradient descent for the online setting. As in the offline setting, we choose initial parameters \\(\\mathbf{x}^{(1)}\\) and learning rate \\(\\eta\\). Then for \\(t=1,\\ldots, T\\), we use parameters \\(\\mathbf{x}^{(t)}\\). We then observe \\(f_t\\) and pay cost \\(f_t(\\mathbf{x}^{(t)})\\). Next, we update the parameter with the gradient of \\(f_t\\): \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\nabla f_t(\\mathbf{x}^{(t)}).\n\\] Notice that if all the \\(f_t\\) are the same, the online gradient algorithm is the same as regular gradient descent.\nLet \\[\n\\mathbf{x}^* = \\arg \\min_\\mathbf{x}\n\\sum_{t=1}^T f_t(\\mathbf{x}).\n\\] We will show that online gradient descent achieves low regret with respect to the optimal fixed solution \\(\\mathbf{x}^*\\) under our favorite assumptions.\nRegret Bound: Suppose that \\(f_1, \\ldots, f_T\\) are convex, \\(G\\)-Lipschitz, and the gradients are \\(L\\)-Lipschitz. Then, after \\(T\\) steps, the regret is \\[\n\\epsilon =  \\sum_{t=1}^t f_t(\\mathbf{x}^{(t)})  -  \\sum_{t=1}^T f_t(\\mathbf{x}^*))\n\\leq RG \\sqrt{T}.\n\\]\nNotice that the average regret is bounded by \\(\\frac{RG}{\\sqrt{T}}\\). As the number of steps increases, the average regret goes to \\(0\\).\nThe amazing property of this result is that we made no assumptions on how \\(f_1, \\ldots, f_T\\) are related to each other. In fact, the functions could have even been chosen adversarily so that \\(f_t\\) depends on \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t-1)}\\).\nSimilar to the offline setting, we will show the result with the following intermediate claim. For all \\(t=1,\\ldots, T\\), we have \\[\nf_t(\\mathbf{x}^{(t)}) - f_t(\\mathbf{x}^*)\n\\leq \\frac{ \\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta} + \\frac{\\eta G^2}{2}.\n\\] The proof is actually the same as for the offline setting; we only use the convexity of the functions \\(f_t\\). Next, we’ll apply a telescoping sum to get \\[\\begin{align*}\n\\sum_{t=1}^T f_t(\\mathbf{x}^{(t)}) - f_t(\\mathbf{x}^*)\n&\\leq \\frac{ \\| \\mathbf{x}^{(1)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(T+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta} + \\frac{\\eta TG^2}{2} \\\\\n&\\leq \\frac{R^2}{2\\eta} + \\frac{\\eta TG^2}{2}\n= RG\\sqrt{T}\n\\end{align*}\\] where the equality follows by our choice of \\(\\eta = \\frac{R}{G\\sqrt{T}}\\).\nWith the online gradient descent bound in hand, we will apply it to analyze the performance of stochastic gradient descent."
  },
  {
    "objectID": "notes/11_online_and_stochastic.html#stochastic-gradient-descent",
    "href": "notes/11_online_and_stochastic.html#stochastic-gradient-descent",
    "title": "Online and Stochastic Gradient Descent",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nIn machine learning applications, we often want to minimize a function \\(f\\) that is a sum of many functions \\[\nf(\\mathbf{x}) = \\sum_{i=1}^n f_i(\\mathbf{x}).\n\\] Typically, each function \\(f_i\\) is a loss function for a single data point.\nStochastic gradient descent is a modification of gradient descent that takes advantage of the finite sum structure to speed up the algorithm when there are patterns in the data.\nThe key insight of stochastic gradient descent is that we can approximate the gradient of \\(f\\) by sampling a single function \\(f_j\\). To see this, observe that \\[\n\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\nabla f_i(\\mathbf{x}).\n\\] We will pick a random \\(j \\in \\{1,\\ldots, n\\}\\) and then use the gradient \\(\\nabla f_{j}\\) to update our parameters. Notice that, by the definition of expectation, \\[\n\\mathbb{E}[ \\nabla f_{j}(\\mathbf{x}) ] =\n\\sum_{i=1}^n \\frac1{n} \\nabla f_{i}(\\mathbf{x}) = \\frac1{n} \\nabla f(\\mathbf{x}).\n\\]\nThen \\(n \\nabla f_{j_t}(\\mathbf{x})\\) is an unbiased estimator of \\(\\nabla f(\\mathbf{x})\\). The advantage is that we can typically compute \\(\\nabla f_{j_t}(\\mathbf{x})\\) in \\(1/n\\) fraction of the time it takes to compute \\(f(\\mathbf{x})\\). Stochastic gradient trades slower convergence for cheaper iterations.\nThe stochastic gradient descent algorithm is as follows. Choose a staring vector \\(\\mathbf{x}^{(1)}\\) and learning rate \\(\\eta\\). For \\(t=1,\\ldots, T\\), we pick a random \\(j_t \\in \\{1,\\ldots, n\\}\\) uniformly at random. Then we update the parameters like so \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\nabla f_{j_t}(\\mathbf{x}^{(t)}).\n\\] At the end of the algorithm, we return the average parameters \\[\n\\hat{\\mathbf{x}} = \\frac1{T} \\sum_{t=1}^T \\mathbf{x}^{(t)}.\n\\] The reason we return the average parameters at the end is because we don’t want to spend time evaluating the full function to learn which parameters are best.\n\nWe will analyze stochastic gradient descent as a special case of online gradient descent. Consider the finite sum structure \\(f(\\mathbf{x}) = \\sum_{i=1}^n f_i(\\mathbf{x})\\) where each \\(f_i\\) is convex. In addition, we assume that each \\(f_i\\) is Lipschitz with constant \\(\\frac{G'}{n}\\). Notice that this assumption implies that \\(f\\) is \\(G'\\)-Lipschitz. We will view the stochastic gradient descent algorithm as an online algorithm where we are given the functions \\(f_1, \\ldots, f_n\\).\nWe will use the following inequality in the analysis.\nJensen’s Inequality: For a convex function \\(f\\) and points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(T)}\\), we have \\[\nf\\left(\\frac1{T} \\mathbf{x}^{(1)} + \\ldots + \\frac1{T} \\mathbf{x}^{(T)} \\right)\n\\leq \\frac1{T}\nf(\\mathbf{x}^{(1)}) + \\ldots + \\frac1{T} f(\\mathbf{x}^{(T)}).\n\\]\nStochastic Gradient Descent Convergence: We will show that after \\(T=\\frac{R^2 G'^2}{\\epsilon^2}\\) steps, we have \\[\n\\mathbb{E}[f(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*)]\n\\leq \\epsilon.\n\\]\nFirst, we have that \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*)\n&=\nf\\left(\\frac1{T} \\sum_{t=1}^T \\mathbf{x}^{(t)}\\right) - \\frac1{T} \\sum_{t=1}^T f(\\mathbf{x}^*) \\\\\n&\\leq \\frac1{T} \\sum_{t=1}^T f(\\mathbf{x}^{(t)}) - \\frac1{T} \\sum_{t=1}^T f(\\mathbf{x}^*) \\\\\n&= \\frac1{T} \\sum_{t=1}^T f(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^*)\n\\end{align*}\\] where the inequality follows by Jensen’s inequality.\nWe will consider the prior inequality in expectation. Then \\[\\begin{align*}\n\\mathbb{E}[f(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*)]\n&\\leq \\frac{1}{T} \\sum_{t=1}^T \\mathbb{E}[f(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^*)] \\\\\n&= \\frac1{T} \\sum_{t=1}^T n \\mathbb{E}[f_{j_t}(\\mathbf{x}^{(t)}) - f_{j_t}(\\mathbf{x}^*)] \\\\\n&= \\frac{n}{T} \\sum_{t=1}^T \\mathbb{E}[f_{j_t}(\\mathbf{x}^{(t)}) - f_{j_t}(\\mathbf{x}^*)].\n\\end{align*}\\] Consider the best offline solution \\[\n\\mathbf{x}^{\\text{offline}} =\n\\arg \\min_\\mathbf{x} \\sum_{t=1}^T f_{j_t}(\\mathbf{x}).\n\\] Notice that \\[\n\\sum_{t=1}^T f_{j_t}(\\mathbf{x}^{\\text{offline}})\n\\leq \\sum_{t=1}^T f_{j_t}(\\mathbf{x}^*)\n\\] by definition. Then, combining the last two inequalities, we have \\[\\begin{align*}\n\\mathbb{E}[f(\\hat{\\mathbf{x}}) - f(\\mathbf{x}^*)]\n&\\leq \\frac{n}{T} \\sum_{t=1}^T \\mathbb{E}[f_{j_t}(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^\\text{offline})] \\\\\n&\\leq \\frac{n}{T} \\left( R \\frac{G'}{n} \\sqrt{T} \\right)\n\\end{align*}\\] where the last inequality follows by the online gradient descent guarantee.\nLet’s compare our guarantees from gradient descent and stochastic gradient descent. For gradient descent, we can find an \\(\\epsilon\\) minimizer in \\(T=\\frac{R^2 G^2}{\\epsilon^2}\\) steps. For stochastic gradient descent, we can find a \\(\\epsilon\\) minimizer in \\(T=\\frac{R^2 G'^2}{\\epsilon^2}\\) steps.\nWe always have \\(G \\leq G'\\) since \\[\\begin{align*}\n\\max_{\\mathbf{x}} \\| \\nabla f(\\mathbf{x}) \\|_2\n&\\leq \\max_{\\mathbf{x}} \\sum_{i=1}^n \\| \\nabla f_i(\\mathbf{x}) \\|_2 \\\\\n&\\leq \\sum_{i=1}^n \\max_{\\mathbf{x}} \\| \\nabla f_i(\\mathbf{x}) \\|_2 \\\\\n&\\leq n \\frac{G'}{n} = G'.\n\\end{align*}\\] So gradient descent converges strictly faster than stochastic gradient descent. However, for a fair comparison, we should analyze the complexity of each algorithm. For gradient descent, the complexity is \\(T \\cdot O(n)\\) since each of the \\(T\\) iterations requires us to compute the gradient of \\(f\\) which is a sum of \\(n\\) functions. For stochastic gradient descent, the complexity is \\(T \\cdot O(1)\\) since each of the \\(T\\) iterations requires us to compute the gradient of a single function.\nWhen \\(G \\ll G'\\), gradient descent will perform better than stochastic gradient descent. When \\(G\\) is closer to \\(G'\\), stochastic gradient descent will perform better. An extreme case when \\(G = G'\\) is when \\(f\\) is a sum of \\(n\\) identical functions.\nLet’s consider the case of unstructured data where the gradients look like random vectors. That is, each entry of \\(\\nabla f_i(\\mathbf{x})\\) is a standard normal random variable. The expected norm of the gradient of a single function is \\[\\begin{align*}\n\\mathbb{E}[\\| \\nabla f_i(\\mathbf{x}) \\|_2^2]\n= \\mathbb{E}  \\left[\n\\sum_{j=1}^d n_{ij}^2 \\right]\n= d\n\\end{align*}\\] since each \\(n_{ij}\\) has variance \\(1\\) and mean \\(0\\). The expected norm of the gradient of the whole function is \\[\\begin{align*}\n\\mathbb{E}[\\| \\nabla f(\\mathbf{x}) \\|_2^2]\n= \\mathbb{E} \\left[ \\left\\|\n\\sum_{i=1}^n \\nabla f_i(\\mathbf{x}) \\right\\|_2^2 \\right]\n= dn\n\\end{align*}\\] since each \\(\\left[\\sum_{i=1}^n f_i(\\mathbf{x})\\right]_j\\) has variance \\(n\\) and mean \\(0\\).\nFrom the analysis, we can see that random gradients are a worst case scenario for stochastic gradient descent. Generally, stochastic gradient performs better when there is more structure in the data. Luckily, structured data sets such as MNIST (shown below) are standard in machine learning."
  },
  {
    "objectID": "notes/11_online_and_stochastic.html#preconditioning",
    "href": "notes/11_online_and_stochastic.html#preconditioning",
    "title": "Online and Stochastic Gradient Descent",
    "section": "Preconditioning",
    "text": "Preconditioning\nInstead of minimizing \\(f\\), the idea of preconditioning is to find another function \\(g\\) which is better suited for first order optimization but has the same minimizer as \\(f\\).\nClaim: Let \\(h: \\mathbb{R}^d \\to \\mathbb{R}^d\\) be an invertible function. Let \\(g(\\mathbf{x}) = f(h(\\mathbf{x}))\\). Then \\[\n\\min_\\mathbf{x} f(\\mathbf{x}) = \\min_\\mathbf{x} g(\\mathbf{x}).\n\\]\nProof: Let \\(\\mathbf{x}_g^* = \\arg \\min_\\mathbf{x} g(\\mathbf{x})\\) and \\(\\mathbf{x}_f^* = \\arg \\min_\\mathbf{x} f(\\mathbf{x})\\). Then \\[\\begin{align*}\n\\min_\\mathbf{x} f(\\mathbf{x}) \\leq f(h(\\mathbf{x}_g^*)) = g(\\mathbf{x}_g^*)\n\\end{align*}\\] and \\[\\begin{align*}\n\\min_\\mathbf{x} g(\\mathbf{x}) \\leq g(h^{-1}(\\mathbf{x}_f^*))\n= f(h(h^{-1}(\\mathbf{x}_f^*)))\n= f(\\mathbf{x}_f^*).\n\\end{align*}\\]\nSince \\(\\min_\\mathbf{x} f(\\mathbf{x}) \\leq \\min_\\mathbf{x} g(\\mathbf{x})\\) and \\(\\min_\\mathbf{x} g(\\mathbf{x}) \\leq \\min_\\mathbf{x} f(\\mathbf{x})\\), the claim follows.\nIn order to optimize the function efficiently, we require that \\(g\\) is convex. Often, we choose a linear function \\(h\\) so \\(g\\) is convex if \\(f\\) is convex. In particular, let \\(\\mathbf{P}\\) be an invertible \\(d \\times d\\) matrix. So the preconditioner is given by \\(g(\\mathbf{x}) = f(\\mathbf{P} \\mathbf{x})\\).\nThere are several additional goals we consider.\n\nWe want \\(g\\) to be better conditioned (e.g. smooth and strongly convex) than \\(f\\).\nWe want to be able to compute \\(\\mathbf{P}\\) and \\(\\mathbf{P}^{-1}\\) efficiently.\n\nIt is often the case that \\(\\mathbf{P}\\) is chosen to be a diagonal matrix. For example, let’s consider linear regression where \\(\\| \\mathbf{Ax} - \\mathbf{b} \\|_2^2\\)$ is the loss function, a common choice of preconditioner is \\(\\mathbf{P} = \\text{diag}(\\mathbf{A}^T \\mathbf{A})^{-1}\\).\nWe can think of preconditioning as variable step sizes. If \\(g(\\mathbf{x}) = f(\\mathbf{Px})\\) then \\(\\nabla g(\\mathbf{x}) = \\mathbf{P}^\\top \\nabla f(\\mathbf{Px})\\).\nIf we run gradient descent on \\(g\\), the update is \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\mathbf{P}^\\top \\nabla f(\\mathbf{Px}^{(t)}).\n\\]\nMultiply by \\(\\mathbf{P}\\) on both sides and let \\(\\mathbf{y}^{(t)} = \\mathbf{Px}^{(t)}\\). Then the update is \\[\n\\mathbf{y}^{(t+1)} = \\mathbf{y}^{(t)} - \\eta \\mathbf{P}^2 \\nabla f(\\mathbf{y}^{(t)}).\n\\]\nWhen \\(\\mathbf{P}\\) is a diagonal matrix, the reformulation in terms of \\(\\mathbf{y}\\) is just gradient descent with a different step size for each parameter.\nThere are many algorithms based on the idea of preconditioning including AdaGrad, RMSprop, and the Adam optimizer."
  },
  {
    "objectID": "notes/14_singular_value_decomposition.html",
    "href": "notes/14_singular_value_decomposition.html",
    "title": "Singular Value Decomposition",
    "section": "",
    "text": "The next part of this course will cover numerical linear algebra."
  },
  {
    "objectID": "notes/14_singular_value_decomposition.html#linear-algebra-review",
    "href": "notes/14_singular_value_decomposition.html#linear-algebra-review",
    "title": "Singular Value Decomposition",
    "section": "Linear Algebra Review",
    "text": "Linear Algebra Review\nRecall that an eigenvalue of a square matrix \\(\\mathbf{X} \\in \\mathbb{R}^{d \\times d}\\) is any vector \\(\\mathbf{v}\\) such that \\(\\mathbf{Xv} = \\lambda \\mathbf{v}\\) for some scalar \\(\\lambda\\).\nThe matrix \\(\\mathbf{X}\\) has at most \\(d\\) linearly independent eigenvectors. If it has a full set of \\(d\\) eigenvectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_d\\) where \\(\\|\\mathbf{v}_i\\|_2 = 1\\) for all \\(i\\) and with eigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_d\\), the matrix is called diagonalizable and can be written as \\[\n\\mathbf{X} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}.\n\\]\nDecomposing a matrix into its eigenvectors and eigenvalues is called eigendecomposition.\nWhile eigendecomposition only applies to square matrices, we can extend the idea to rectangular matrices with a related tool called singular value decomposition. But first, let’s review eigendecomposition. If a square matrix \\(\\mathbf{V}\\) has orthonormal rows, it also has orthonormal columns.\n\n\n\nThat is, \\(\\mathbf{V} \\mathbf{V}^\\top = \\mathbf{I}\\) because the eigenvectors are orthonormal. Then, by definition, \\(\\mathbf{V}^{-1} = \\mathbf{V}^\\top\\). Next, because \\(\\mathbf{V}^\\top\\) is full rank, we can write \\(\\mathbf{I} = \\mathbf{V}^\\top (\\mathbf{V}^\\top)^{-1}=\\mathbf{V}^\\top (\\mathbf{V}^{-1})^{-1}= \\mathbf{V}^\\top \\mathbf{V}\\).\nThis implies that, for any vector \\(\\mathbf{x}\\), \\(\\| \\mathbf{V x}\\|_2^2 = \\| \\mathbf{x} \\|_2^2 = \\| \\mathbf{V}^\\top \\mathbf{x} \\|_2^2\\).\nTo see this, we can write \\[\\begin{align*}\n\\| \\mathbf{V x}\\|_2^2\n= (\\mathbf{V x})^\\top (\\mathbf{V x})\n= \\mathbf{x}^\\top \\mathbf{V}^\\top \\mathbf{V x}\n= \\mathbf{x}^\\top \\mathbf{x}\n= \\| \\mathbf{x} \\|_2^2.\n\\end{align*}\\] A similar set of steps shows that \\(\\| \\mathbf{V}^\\top \\mathbf{x} \\|_2^2 = \\| \\mathbf{x} \\|_2^2\\).\nWe have the same property for the Frobenius norm of a matrix. For any square matrix \\(\\mathbf{X} \\in \\mathbb{R}^{d \\times d}\\), the Frobenius norm \\(\\| \\mathbf{X} \\|_F^2\\) is defined as the sum of squared entries \\(\\sum_{i,j} x_{ij}^2\\). To see that the same property holds for the Frobenius norm, we can write \\[\\begin{align*}\n\\| \\mathbf{V X}\\|_F^2\n= \\sum_{i=1}^d \\| \\mathbf{V} \\mathbf{X}_i \\|_2^2\n= \\sum_{i=1}^d \\| \\mathbf{X}_i \\|_2^2\n= \\| \\mathbf{X} \\|_F^2\n\\end{align*}\\] where \\(\\mathbf{X}_i\\) denotes the \\(i\\)th column of \\(\\mathbf{X}\\) and the second equality follows from the previous result. A similar set of steps shows that \\(\\| \\mathbf{V}^\\top \\mathbf{X} \\|_F^2 = \\| \\mathbf{X} \\|_F^2\\)."
  },
  {
    "objectID": "notes/14_singular_value_decomposition.html#singular-value-decomposition",
    "href": "notes/14_singular_value_decomposition.html#singular-value-decomposition",
    "title": "Singular Value Decomposition",
    "section": "Singular Value Decomposition",
    "text": "Singular Value Decomposition\nSingular value decomposition is one of the most fundamental results in linear algebra. Without loss of generality, suppose that \\(n \\geq d\\). Any matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) can be written in the form \\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\n\\] where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\), \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{d \\times d}\\), and \\(\\mathbf{V} \\in \\mathbb{R}^{d \\times d}\\). The matrix \\(\\mathbf{U}\\) satisfies \\(\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}\\) while \\(\\mathbf{V}\\) satisfies \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}\\). The matrix \\(\\mathbf{\\Sigma}\\) is diagonal with non-negative entries \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_d \\geq 0\\).\n\n\n\nSingular values are unique but factors are not. We would still get a valid decomposition if we multiply the \\(i\\)th column of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) by \\(-1\\).\nFor the eigendecomposition, we showed that \\(\\|\\mathbf{V} \\mathbf{x}\\|_2^2 = \\| \\mathbf{x} \\|_2^2 = \\| \\mathbf{V}^\\top \\mathbf{x} \\|_2^2\\). It then followed that \\(\\| \\mathbf{V} \\mathbf{X} \\|_F^2 = \\| \\mathbf{X} \\|_F^2\\) and \\(\\| \\mathbf{V}^\\top \\mathbf{X} \\|_F^2 = \\| \\mathbf{X} \\|_F^2\\). Not all of these properties are true for singular value decomposition.\nThese properties are not true for rectangular matrices. Let \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) with \\(n &gt; d\\) be a matrix with orthogonal columns. Then \\(\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}\\) but \\(\\mathbf{U} \\mathbf{U}^\\top \\neq \\mathbf{I}\\).\n\n\n\nSimilarly, for any \\(\\mathbf{x}\\), \\(\\| \\mathbf{U x}\\|_2^2 = \\| \\mathbf{x} \\|_2^2\\) but \\(\\| \\mathbf{U}^\\top \\mathbf{x} \\|_2^2 \\neq \\| \\mathbf{x} \\|_2^2\\).\nMultiplying a vector by a matrix \\(\\mathbf{U}\\) with orthonormal columns rotates and/or reflects the vector.\n\n\n\nMultiplying a vector by a matrix \\(\\mathbf{U}^\\top\\) with orthonormal rows projects the vector into a lower dimensional space (representing it as coordinates in the lower dimensional space).\n\n\n\nSo we always have that \\(\\| \\mathbf{U}^\\top \\mathbf{x} \\|_2 \\leq \\| \\mathbf{x} \\|_2\\).\nAn important takeaway from singular value decomposition is how to view matrix multiplication. We can view multiplying any vector \\(\\mathbf{a}\\) by a matrix \\(\\mathbf{X}\\) to form \\(\\mathbf{Xa}\\) as a composition of three operations:\n\nRotate and/or reflect the vector (multiplication by \\(\\mathbf{V}^\\top\\)).\nScale the coordinates (multiplication by \\(\\mathbf{\\Sigma}\\)).\nRotate and/or reflect the vector again (multiplication by \\(\\mathbf{U}\\)).\n\nWe can see this because \\[\n\\mathbf{X a} = \\mathbf{U} \\left( \\mathbf{\\Sigma} \\left( \\mathbf{V}^\\top \\mathbf{a} \\right) \\right).\n\\]\nLet’s compare singular value decomposition to eigendecomposition.\n\nSingular value decomposition exists for all matrices (square or rectangular) while eigendecomposition only exists for some square matrices.\nSingular values are always positive while eigenvalues can be positive or negative.\nThe factors \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) in singular value composition have orthogonal columns while the factor \\(\\mathbf{V}\\) in eigendecomposition has orthogonal columns if and only if \\(\\mathbf{X}\\) is full rank.\n\nWe can connect singular value decomposition with eigendecomposition by considering the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\). The factor \\(\\mathbf{U}\\) contains the orthogonal eigenvectors of \\(\\mathbf{X} \\mathbf{X}^\\top\\) while the factor \\(\\mathbf{V}\\) contains the orthogonal eigenvectors of \\(\\mathbf{X}^\\top \\mathbf{X}\\). To see this, recall that \\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\text{ and }\n\\mathbf{X}^\\top = \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{U}^\\top.\n\\] Then \\[\\begin{align*}\n\\mathbf{X}\\mathbf{X}^\\top\n= \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{U}^\\top\n= \\mathbf{U} \\mathbf{\\Sigma}^2 \\mathbf{U}^\\top\n\\end{align*}\\] and similarly \\[\\begin{align*}\n\\mathbf{X}^\\top \\mathbf{X}\n= \\mathbf{V} \\mathbf{\\Sigma}^2 \\mathbf{V}^\\top.\n\\end{align*}\\] An additional observation is that the singular values of \\(\\mathbf{X}\\) are the square roots of the eigenvalues of \\(\\mathbf{X}^\\top \\mathbf{X}\\) and \\(\\mathbf{X} \\mathbf{X}^\\top\\). That is, \\(\\sigma_i^2 = \\lambda_i(\\mathbf{X X}^\\top) = \\lambda_i(\\mathbf{X}^\\top \\mathbf{X})\\).\nThere are many applications of SVD including:\n\nComputing the pseudoinverse \\(\\mathbf{V} \\mathbf{\\Sigma}^{-1} \\mathbf{U}^\\top\\).\nReading off the condition number of \\(\\mathbf{X}\\) from the ratio of the largest and smallest singular values.\nComputing matrix norms like \\(\\| \\mathbf{X} \\|_2 =\\sigma_1\\) and \\(\\| \\mathbf{X} \\|_F = \\sqrt{\\sum_{i=1}^d \\sigma_i^2}\\).\nComputing the matrix square root \\(\\mathbf{X}^{1/2} = \\mathbf{V} \\mathbf{\\Sigma}^{1/2} \\mathbf{U}^\\top\\).\nPerforming principle component analysis.\n\nWe’ll focus on a particularly useful application called low-rank approximations of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "notes/14_singular_value_decomposition.html#best-low-rank-approximation",
    "href": "notes/14_singular_value_decomposition.html#best-low-rank-approximation",
    "title": "Singular Value Decomposition",
    "section": "Best Low-Rank Approximation",
    "text": "Best Low-Rank Approximation\nLow-rank approximations are very useful when our data has some structure. For example, if a dataset only has \\(k\\) unique data points, it will be exactly rank \\(k\\). If it has \\(k\\) “clusters” of data points (e.g. the 10 digits in the MNIST dataset), the matrix will often be very close to rank \\(k\\). Similarly, correlation between columns (data features) leads to a low-rank matrix.\nWe can exploit low-rank structure by using low-rank approximations to reduce the dimensionality of the data or visualizing the data in a lower dimensional space. Examples include data embeddings like word2vec or node2vec, reduced order modeling for solving physical equations, constructing preconditioners in optimization, and noisy triangulation.\nBecause low-rank approximations are so useful, we would like to know how to find the best low-rank approximation to a matrix \\(\\mathbf{X}\\). In fact, we will show that we can find the best rank-\\(k\\) approximation to \\(\\mathbf{X}\\) by computing the singular value decomposition of \\(\\mathbf{X}\\) and then setting all but the \\(k\\) largest singular values to zero.\n\n\n\nLet \\(\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\\) be the singular value decomposition of \\(\\mathbf{X}\\). Let \\(\\mathbf{X}_k\\) be the best rank-\\(k\\) approximation to \\(\\mathbf{X}\\) i.e. \\[\n\\mathbf{X}_k = \\arg \\min_{\\text{rank } k \\text{ } \\mathbf{B}} \\| \\mathbf{X} - \\mathbf{B} \\|_F^2.\n\\] We will show that \\(\\mathbf{X}_k = \\mathbf{U}_k \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\\) where \\(\\mathbf{U}_k\\) contains the first \\(k\\) columns of \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}_k\\) contains the first \\(k\\) singular values of \\(\\mathbf{\\Sigma}\\) and \\(\\mathbf{V}_k\\) contains the first \\(k\\) columns of \\(\\mathbf{V}\\).\nWe will repeatedly use two observations:\n\n\\(\\| \\mathbf{V} \\mathbf{X} \\|_F^2 = \\| \\mathbf{X} \\|_F^2\\) if \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}\\) by the connection to \\(\\ell_2\\) norm we saw before and\n\\(\\| \\mathbf{X}^\\top \\|_F^2 = \\| \\mathbf{X} \\|_F^2\\) since the Frobenius norm is the sum of squared entries (which is invariant under the transpose operation).\n\nWe can write \\[\\begin{align*}\n\\arg \\min_{\\text{rank } k \\text{ } \\mathbf{B}}\n\\| \\mathbf{X} - \\mathbf{B} \\|_F^2\n= \\arg \\min_{\\text{rank } k \\text{ } \\mathbf{B}}\n\\| \\mathbf{\\Sigma V}^\\top - \\mathbf{U}^\\top \\mathbf{B} \\|_F^2\n\\end{align*}\\] since \\(\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}\\).\nThen \\[\\begin{align*}\n\\arg & \\min_{\\text{rank } k \\text{ } \\mathbf{B}}\n\\| \\mathbf{\\Sigma V}^\\top - \\mathbf{U}^\\top \\mathbf{B} \\|_F^2 \\\\\n&= \\arg \\min_{\\text{rank } k \\text{ } \\mathbf{B}}\n\\| \\mathbf{V \\Sigma } - \\mathbf{B}^\\top \\mathbf{U} \\|_F^2 \\\\\n&= \\arg \\min_{\\text{rank } k \\text{ } \\mathbf{B}}\n\\| \\mathbf{\\Sigma} - \\mathbf{V}^\\top \\mathbf{B}^\\top \\mathbf{U} \\|_F^2\n\\end{align*}\\] since \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}\\).\nThen we choose \\(\\mathbf{B}\\) so that \\(\\mathbf{V}^\\top \\mathbf{B}^\\top \\mathbf{U}\\) is the matrix which agrees with \\(\\mathbf{\\Sigma}\\) on the largest \\(k\\) singular values. It is intuitive but not obvious that the best rank-\\(k\\) approximation to a diagonal matrix is a diagonal matrix that agrees on the largest \\(k\\) values.\nClaim (Orthogonal Projection): Consider any orthogonal matrix \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\). Then \\[\n\\| \\mathbf{X} - \\mathbf{X W W}^\\top \\|_F^2\n= \\| \\mathbf{X} \\|_F^2 - \\| \\mathbf{X W W}^\\top \\|_F^2.\n\\]\nWe will prove the orthogonal claim later. Using the claim, observe that the problem of finding the best rank \\(k\\) approximation \\(\\mathbf{Z}\\) to a diagonal matrix \\(D\\) is equivalent to \\[\\begin{align*}\n&\\arg \\min_{\\text{rank $k$ } \\mathbf{Z}}\n\\| \\mathbf{D} - \\mathbf{Z} \\|_F^2 \\\\\n&= \\arg \\min_{\\text{orthonormal rank $k$ } \\mathbf{Z}}\n\\| \\mathbf{D}  - \\mathbf{D Z Z}^\\top \\|_F^2 \\\\\n&= \\arg \\min_{\\text{orthonormal rank $k$ } \\mathbf{Z}}\n\\| \\mathbf{D} \\|_F^2 - \\| \\mathbf{D Z Z}^\\top \\|_F^2 \\\\\n&= \\arg \\max_{\\text{orthonormal rank $k$ } \\mathbf{Z}}\n\\| \\mathbf{Z}^\\top \\mathbf{D} \\|_F^2 \\\\\n&= \\arg \\max_{\\text{orthonormal rank $k$ } \\mathbf{Z}} \\sum_{i=1}^d \\| \\mathbf{Z}_i \\|_2^2 \\sigma_i^2\n\\end{align*}\\] where the second equality follows from the orthogonal projection claim, the third equality follows because \\(\\| \\mathbf{D} \\|_F^2\\) is a constant with respect to \\(\\mathbf{Z}\\), and \\(\\mathbf{Z}_i\\) is the \\(i\\)th row of \\(\\mathbf{Z}\\) in the final expression. Since \\(\\mathbf{Z}\\) is orthonormal and rank \\(k\\), we have \\[\\sum_{i=1}^d \\| \\mathbf{Z}_i \\|_2^2 = k.\\] To see this, observe that sum of squared row norms is the sum of squared column norms which is \\(k\\). Since \\(\\mathbf{Z}\\) has \\(k\\) orthonormal columns and \\(d &lt; k\\), we have \\[\n\\| \\mathbf{Z}_i \\|_2^2 \\leq 1.\n\\] To see this, imagine “completing” \\(\\mathbf{Z}\\) by adding columns to make its rows span the entire space. After adding the columns, each row would be a unit vector so each row of \\(\\mathbf{Z}\\) can have norm at most \\(1\\). With these observations, \\[\n\\max_{\\text{orthonormal rank $k$ } \\mathbf{Z}} \\sum_{i=1}^d \\| \\mathbf{Z}_i \\|_2^2 \\sigma_i^2\n= \\sum_{i=1}^d \\sigma_i^2\n\\] since we want to put as much mass on the larger singular values as possible.\nNow that we know the SVD of a diagonal matrix is the best rank-\\(k\\) approximation, we will return to the orthogonal projection claim. One useful observation of the claim is that \\[\n\\arg \\min_{\\mathbf{W} \\in \\mathbb{R}^{d \\times k}}\n\\| \\mathbf{X} - \\mathbf{X W W}^\\top \\|_F^2\n= \\arg \\max_{\\mathbf{W} \\in \\mathbb{R}^{d \\times k}} \\| \\mathbf{X W W}^\\top \\|_F^2.\n\\] Since \\(\\| \\mathbf{X} \\|_F^2\\) is a constant with respect to \\(\\mathbf{W}\\), we can maximize \\(\\| \\mathbf{X W W}^\\top \\|_F^2\\) to minimize \\(\\| \\mathbf{X} - \\mathbf{X W W}^\\top \\|_F^2\\). This is often the perspective people take when thinking about principle component analysis.\nWe will show the orthogonal claim from the observation that the column \\(\\mathbf{X}_i\\) is orthogonal to itself projected onto \\(\\mathbf{W W}^\\top\\) and then applying the Pythagorean theorem.\n\n\n\nThe orthogonal projection claim combined with our characterization of the optimal low rank approximation gives us \\[\n\\| \\mathbf{X} - \\mathbf{X}_k \\|_F^2\n= \\| \\mathbf{X} \\|_F^2 - \\| \\mathbf{X}_k \\|_F^2\n= \\sum_{i=1}^n \\sigma_i^2 - \\sum_{i=1}^k \\sigma_i^2\n= \\sum_{i=k+1}^n \\sigma_i^2.\n\\]\nWe can see this because \\[\\begin{align*}\n\\| \\mathbf{X} \\|_F^2\n= \\| \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\|_F^2\n= \\| \\mathbf{\\Sigma} \\mathbf{V}^\\top \\|_F^2\n= \\| \\mathbf{\\Sigma} \\|_F^2\n\\end{align*}\\] where we used our favorite tricks that \\(\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}\\), \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}\\), and the Frobenius norm is invariant with respect to transposes.\nThe characterization of our low-rank approximation error in terms of the singular values gives a sense of how low-rank a matrix is. Data with structure will have a small number of large singular values and a large number of small singular values.\n\n\n\nIn contrast, data with no structure will have singular values that are all roughly the same size.\n\n\n\nNow that we know the best low-rank approximation is the truncated SVD, all that remains is to find the SVD.\nWe can find the SVD with the following approach:\n\nCompute \\(\\mathbf{X}^\\top \\mathbf{X}\\) in \\(O(nd^2)\\) time.\nFind eigendecomposition of \\(\\mathbf{X}^\\top \\mathbf{X} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\) in \\(O(d^3)\\) time using methods like the QR algorithm in \\(O(d^3)\\) time.\nFinally, compute \\(\\mathbf{L} = \\mathbf{X V}\\) and then set \\(\\sigma_i = \\| \\mathbf{L}_i \\|_2\\) and \\(\\mathbf{U}_i = \\mathbf{L}_i / \\sigma_i\\) for \\(i = 1, \\ldots, d\\) in \\(O(n d^2)\\) time.\n\nThe total time complexity is \\(O(nd^2 + d^3 + nd^2) = O(nd^2)\\). If we use the SVD only for low rank approximation, notice that we didn’t really need to find all the singular vectors and values.\nWe can save time by computing an approximate solution to the SVD. In particular, we will only compute the top \\(k\\) singular vectors and values. We can do this with iterative algorithms that achieve time complexity \\(O(ndk)\\) instead of \\(O(nd^2)\\). There are many algorithms for this problem:\n\nKrylov subspace methods like the Lanczos method are most commonly used in practice.\nPower method is the simplest Krylov subspace method and still works very well.\n\nWe will focus on the power method next time."
  },
  {
    "objectID": "notes/03_distinct_elements.html",
    "href": "notes/03_distinct_elements.html",
    "title": "Distinct Elements",
    "section": "",
    "text": "We’ve seen the power of linearity of expectation, Markov’s inequality, and the union bound. This class, we will learn and use linearity of variance and Chebyshev’s inequality."
  },
  {
    "objectID": "notes/03_distinct_elements.html#chebyshevs-inequality",
    "href": "notes/03_distinct_elements.html#chebyshevs-inequality",
    "title": "Distinct Elements",
    "section": "Chebyshev’s Inequality",
    "text": "Chebyshev’s Inequality\nLemma: Let \\(X\\) be a random variable expectation \\(\\mathbb{E}[X]\\) and variance \\(\\sigma^2 = \\textrm{Var}[X]\\). Then for any \\(k &gt; 0\\),\n\\[\n\\Pr(|X - \\mathbb{E}[X]| &gt; k \\sigma) \\leq \\frac{1}{k^2}.\n\\]\nWhen the variance is smaller, we expect \\(X\\) to concentrate more closely around its expectation. Chebyshev’s inequality is one way to formalize this intuition.\nThere are two main benefits of Chebyshev’s over Markov’s :\n\nChebyshev’s applies to any random variable \\(X\\). In contrast, Markov’s requires that the random variable \\(X\\) is non-negative.\nChebyshev’s gives a two-sided bound: we bound the probability that \\(|X - \\mathbb{E}[X]|\\) is large, which means that \\(X\\) isn’t too far above or below its expectation. In contrast, Markov’s only bounds the probability that \\(X\\) is larger than \\(\\mathbb{E}[X]\\).\n\nWhile Chebyshev’s gives a more general bound, it also needs a bound on the variance of \\(X\\). In general, bounding the variance is harder than bounding the expectation.\nNote: There’s no hard rule for which to apply! Both Markov’s and Chebyshev’s are useful in different settings.\nProof of Chebyshev’s Inequality: We’ll apply Markov’s inequality to the non-negative random variable \\(S = (X - \\mathbb{E}[X])^2\\). By Markov’s, \\[\n\\Pr\\left(S \\geq t \\right) \\leq \\frac{\\mathbb{E}[S]}{t}.\n\\] Observe that \\(\\mathbb{E}[S] = \\mathbb{E}[(X-\\mathbb{E}[X])^2] = \\textrm{Var}(X)\\), plug in \\(t = k^2 \\sigma^2\\), and use the definition of \\(S\\). Then\n\\[\n\\Pr \\left((X - \\mathbb{E}[X])^2 \\geq k^2 \\sigma^2\\right)\n\\leq \\frac{\\textrm{Var}(X)}{k^2 \\sigma^2}.\n\\] We’ll take the square root of the event inside the probability (this preserves the inequality) and observe that \\(\\textrm{Var}(X) = \\sigma^2\\). Then simplifying\n\\[\n\\Pr\\left(|X - \\mathbb{E}[X]| \\geq k \\sigma\\right) \\leq \\frac{1}{k^2}.\n\\]"
  },
  {
    "objectID": "notes/03_distinct_elements.html#linearity-of-variance",
    "href": "notes/03_distinct_elements.html#linearity-of-variance",
    "title": "Distinct Elements",
    "section": "Linearity of Variance",
    "text": "Linearity of Variance\nIn order to use Chebyshev’s inequality, we need to bound the variance. Luckily, there’s a helpful tool to help us compute the variance when we have a sum of independent random variables.\nFact: For pairwise independent random variables \\(X_1, \\ldots, X_m\\),\n\\[\n\\textrm{Var}[X_1 + X_2 + \\ldots + X_m] = \\textrm{Var}[X_1]\n+ \\textrm{Var}[X_2] + \\ldots + \\textrm{Var}[X_m].\n\\]\nNotice that we require pairwise independence so for any \\(i, j \\in \\{1, \\ldots, m\\}\\), \\(X_i\\) and \\(X_j\\) are independent. Pairwise independence is a strictly weaker requirement than \\(k\\)-wise independence (when \\(k&gt;2\\)) which requires that for all \\(k\\) variables \\(X_1, \\ldots, X_k\\) and all values \\(v_1, \\ldots, v_k\\), \\[\n\\Pr(X_1 = v_1, \\ldots, X_k=v_k) = \\Pr(X_1=v_1) \\cdot \\ldots \\cdot \\Pr(X_k = v_k).\n\\]\nQuestion: Can you think of three random variables that are pairwise independent but not 3-wise independent?\nHere’s one answer: Let \\(X_1\\) be a random variable that is equally likely to be \\(1\\) and \\(-1\\), let \\(X_2\\) be another random variable that is equally to be \\(1\\) and \\(-1\\), and let \\(X_3 = X_1 X_2\\). If we only look at two of the random variables, they appear independent. But if we look at all three of them, the values of two give us the value of the third.\n\nCoin Example\nIn order to gain familiarity with Chebyshev’s inequality and linearity of variance, let’s go through an example with coin flips. Let \\(C_1, \\ldots, C_{100}\\) be independent random variables that are \\(1\\) with probability \\(\\frac{1}{2}\\) and \\(0\\) otherwise.\nLet the number of heads \\(H\\) be \\(\\sum_{i=1}^{100} C_i\\). By linearity of expectation, we know that \\[\n\\mathbb{E}[H] = \\sum_{i=1}^{100} \\mathbb{E}[C_i] = \\sum_{i=1}^{100} \\frac{1}{2}=50.\n\\] Since \\(C_i\\) is an indicator random variable, we know \\[\n\\textrm{Var}(C_i) = \\mathbb{E}[C_i^2] - \\mathbb{E}[C_i]^2 = \\frac{1}{2} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4}.\n\\] Then, by linearity of variance (all the flips are independent), we know that \\[\n\\textrm{Var}(H) = \\sum_{i=1}^{100} \\textrm{Var}(C_i) = \\sum_{i=1}^{100} \\frac{1}{4} = 25.\n\\]\nNow let’s apply Chebyshev’s inequality. We know \\(\\mathbb{E}[H] = 50\\) and \\(\\textrm{Var}(H) = \\sigma^2 =25\\). \\[\n\\Pr(|H - 50| \\geq 5 \\cdot 4 ) \\leq \\frac{1}{4^2} = \\frac{1}{16}.\n\\] So, with at least \\(93\\%\\) chance, we get between 30 and 70 heads."
  },
  {
    "objectID": "notes/03_distinct_elements.html#distinct-elements",
    "href": "notes/03_distinct_elements.html#distinct-elements",
    "title": "Distinct Elements",
    "section": "Distinct Elements",
    "text": "Distinct Elements\nThe first problem that we’ll consider is estimating distinct elements. The problem, like many we’ll see in this class (e.g. frequent items estimation), is posed in the streaming model. In this model, we have a massive data set that arrives in a sequential stream. There is far too much data to store or process in a single location but we still want to analyze the data. To do this, we must compress the data on the fly, storing some smaller data structure which still contains useful information.\nThe input to the distinct elements problem is \\(x_1, \\ldots, x_n\\) where each item is in a huge universe of items \\(\\mathcal{U}\\). The output is the number of distinct inputs \\(D\\). For example, if the input is \\(1, 10, 2, 4, 9, 2, 10, 4\\) then the output is \\(D=5\\).\nThe distinct elements problem has many applications: Distinct users visiting a webpage. Distinct values in a database columns. Distinct queries to a search engine. Distinct motifs in a DNA sequence. Because the problem is so general, implementations of the algorithm we’ll describe today (and several refinements of it) are deployed in practice at companies like Google, Yahoo, Twitter, Facebook, and many more.\nThe naive approach to solve the problem is to build a dictionary of all items seen so far. Every time we see an element, we hash it and check if we already have it in the dictionary. Unfortunately, the space complexity is \\(O(D)\\) and, if we have millions or billions of distinct elements, this naive approach is infeasible.\nOur goal is to return an estimate \\(\\hat{D}\\) that satisfies \\[\n(1-\\epsilon) D \\leq \\hat{D} \\leq (1+\\epsilon) D\n\\] but only uses \\(O(1/\\epsilon^2)\\) space. This should be surprising: We can use space that is (basically) independent of the number of distinct elements!\nThe algorithm we’ll use to accomplish this is surprisingly simple. Choose a random hash function \\(h: U \\rightarrow [0,1]\\). We’ll initialize an estimate \\(S\\). For every item we see in the stream, we’ll set \\[\nS \\gets \\min(S, h(x_i)).\n\\] Once we processed the stream, return \\(\\frac{1}{S} - 1\\).\nThe figure below describes the algorithm. Elements arrive in a stream and we hash each item to the real number line between \\(0\\) and \\(1\\). By the definition of the hash function, each repeated item hashes to the same point. We maintain a value \\(S\\) hashed value we’ve seen so far.\n\n\n\n\nWhy do we return the estimate \\(\\hat{D} = \\frac{1}{S}- 1\\)? Intuitively, when \\(D\\) is larger, \\(S\\) will be smaller because we get more chances to get small values. But the reason behind the exact estimate is that\nLemma: \\(\\mathbb{E}[S] = \\frac{1}{D+1}\\).\nWe’ll see two proofs.\nCalculus Proof: We’ll use the identity that \\[\n\\mathbb{E}[X] = \\int \\Pr(X \\geq x) dx\n\\] for a continuous and non-negative random variable \\(X\\). To see this, notice that \\[\nX = \\int_{x=0}^X dx = \\int_{x=0}^\\infty \\mathbb{1}[X \\geq x] dx.\n\\] Taking expectation yields the identity. There are other ways to prove this identity described in notes here.\nNow let’s deploy the identity for the continuous random variable \\(S\\): \\[\n\\mathbb{E}[S] = \\int_{s=0}^1 \\Pr(S \\geq s) ds\n= \\int_{s=0}^1 (1-s)^D d s\n\\] \\[\n= \\left. \\frac{-(1-s)^{D+1}}{D+1} \\right|_{s=0}^1 = \\frac{1}{D+1}.\n\\] In the second equality, we used the observation that the minimum \\(S\\) is greater than a value \\(s\\) if and only if all \\(D\\) hashed values are greater than \\(s\\).\nWe can understand every step of the calculus proof but it doesn’t give us a deeper understanding for why the lemma is true. Let’s turn to another proof “from the book”. This phrase comes from Paul Erdős, a prolific Hungarian mathematician, who believed that there are proofs so simple and elegant that they were written in a divine book.\nProof from the Book: We’ll use the following observation: For any event \\(A\\) and random variable \\(x\\), \\[\\begin{align}\n    \\mathbb{E}_x[\\Pr[A|x]]\n    = \\mathbb{E}_x[\\mathbb{E}[\\mathbb{1}[A] \\mid x]]\n    = \\mathbb{E}[\\mathbb{1}[A]]\n    = \\Pr[A].\n\\end{align}\\] The first and last equality follow because \\(\\mathbb{1}[A]\\) is an indicator random variable for event \\(A\\). The middle equality follows by the law of total expectation. After a little bit of thought, the law of total expectation is intuitive. But you can prove it to yourself by plugging in the definition of expectations and exchanging sums/integrals, or you can find the proof online here.\nBack to our proof, we know that \\(h_1,\\ldots,h_D\\) are i.i.d. (independent and identically distributed) uniform samples drawn from the interval \\([0,1]\\). Recall that \\(S = \\min_{i\\in[D]} h_i\\). Since all the \\(h_i\\) are drawn from the interval between \\(0\\) and \\(1\\), we can interpret \\(S\\) as the probability that the next hashed value is less than the minimum of the first \\(D\\) hashed values. Mathematically, \\[\\begin{align}\n    S = \\Pr[h_{D+1} \\leq \\min_{i\\in[D]} h_i \\mid h_1,\\ldots,h_D]\n\\end{align}\\] Now we can compute the expectation of \\(S\\): \\[\\begin{align*}\n    \\mathbb{E}_{h_1,\\ldots,h_D}[S]\n    &= \\mathbb{E}_{h_1,\\ldots,h_D}[\\Pr[h_{D+1} \\leq \\min_{i\\in[D]} h_i \\mid h_1,\\ldots,h_D]]\\\\\n    &= \\Pr_{h_1,\\ldots,h_{D+1}}[h_{D+1} \\leq \\min_{i\\in[D]} h_i] \\\\\n    &= \\frac1{D+1}.\n\\end{align*}\\] The first equality follows by our alternative definition of \\(S\\), the second equality follows by the first observation in the proof, the final equality follows because each \\(h_i\\) is equally likely to be the minimum of all \\(D+1\\).\n\nWe now know the expectation of \\(S\\) (twice) but, in order to apply Chebyshev’s inequality, we need to also bound the variance. Recall that\n\\[\n\\textrm{Var}(S) = \\mathbb{E}[S^2] - \\mathbb{E}[S]^2.\n\\]\nWe know \\(\\mathbb{E}[S]\\) but we don’t yet know \\(\\mathbb{E}[S^2]\\).\nLemma: \\(\\mathbb{E}[S^2] = \\frac{2}{(D+1)(D+2)}\\).\nCalculus Proof: Using a calculus proof similar to the one for the expectation, we know \\[\n\\mathbb{E}[S^2] = \\int_{s=0}^1 \\Pr(S^2 \\geq s) ds\n= \\int_{s=0}^1 \\Pr(S \\geq \\sqrt{s}) ds\n= \\int_{s=0}^1 (1-\\sqrt{s})^D d s.\n\\] Using the WolframAlpha query here, we can find that the last equality is \\[\n\\frac{2}{(D+1)(D+2)}.\n\\]\nAgain, the calculus proof is correct but it doesn’t give us a deeper understanding.\nProof from the Book: We can apply the same machinery that we used in the prior proof from the book. The key observation now is that \\(S^2\\) is the probability that the next two hashed values are both less than the minimum of the first \\(D\\) hashed values. Formally, \\[\nS^2 = \\Pr(\\max(h_{D+1}, h_{D+2}) \\leq \\min(h_1, \\ldots, h_D) | h_1, \\ldots, h_D).\n\\] What’s the probability that both of the next hashed values are less the minimum of the first \\(D\\) hashed values? Well, that’s the probability that either the first hashed value is less than the minimum of the first \\(D+1\\) hashed values and the second hashed value is less than all \\(D+2\\) hashed values or the second hashed value is less than the minimum of the first \\(D+1\\) hashed values and the first hashed value is less than all \\(D+2\\) hashed values. The probability of the first event is \\(\\frac{1}{(D+1)(D+2)}\\) and the probability of the second event is \\(\\frac{1}{(D+1)(D+2)}\\).\nUsing the same analysis from the prior proof from the book, we know that \\[\\begin{align}\n\\mathbb{E}_{h_1, \\ldots, h_D}[S^2]\n&= \\Pr_{h_1, \\ldots, h_{D+2}}(\\max(h_{D+1}, h_{D+2}) \\leq \\min(h_1, \\ldots, h_D)) \\\\\n&= \\frac{2}{(D+1)(D+2)}.\n\\end{align}\\]\n\nRemember our goal is to compute the variance. With our expressions for \\(\\mathbb{E}[S]\\) and \\(\\mathbb{E}[S^2]\\), we know \\[\n\\textrm{Var}(S) = \\frac{2}{(D+1)(D+2)} - \\frac{1}{(D+1)^2}\n\\leq \\frac{1}{(D+1)^2}.\n\\]\nLet’s try applying Chebyshev’s inequality. We know \\(\\mathbb{E}[S] = \\frac{1}{D+1} = \\mu\\) and \\(\\textrm{Var}(S) \\leq \\frac{1}{(D+1)^2} = \\mu^2\\). The bound we want is \\(\\Pr(|S - \\mu | \\geq \\epsilon \\mu) \\leq \\delta\\) where \\(\\delta\\) is a (small) probability of failure. Instead, Chebyshev’s inequality gives \\[\n\\Pr(|S - \\mu| \\geq \\epsilon \\mu) = \\Pr( |S - \\mu| \\geq \\epsilon \\sigma )\n\\leq \\frac{1}{\\epsilon^2}.\n\\] But \\(\\epsilon\\) is a number less than \\(1\\) so the bound is vacuous!!\n\nVariance Reduction\nJust as we repeated the core subroutine in the count-min algorithm, we’ll repeat core subroutine in this algorithm. The new algorithm is to choose \\(k\\) random hash functions \\(h_1, \\ldots, h_k: \\mathcal{U} \\rightarrow [0, 1]\\). We’ll keep \\(k\\) independent sketches of the minimum value \\(S_1, \\ldots, S_k\\) that are all initialized to \\(1\\). Then, when we see item \\(x_i\\), we update \\[S_j \\gets \\min(S_j, h_j(x_i))\\] for every \\(j \\in \\{1, \\ldots, k\\}\\). At the end of the stream, we take the average \\(S = (S_1 + \\ldots + S_k)/k\\) and return the estimate \\(\\hat{D} = \\frac{1}{S} - 1\\).\nOur new algorithm is an example of a general strategy for variance reduction. We repeat many independent trials and take the mean. Given i.i.d. (independent, identically distributed) random variables, \\(X_1, \\ldots, X_k\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we know that\n\\[\n\\mathbb{E} \\left[ \\frac{1}{k} \\sum_{i=1}^k X_i \\right]\n= \\frac{1}{k} \\sum_{i=1}^k \\mathbb{E} \\left[  X_i \\right]\n= \\mu\n\\] and\n\\[\n\\textrm{Var}\\left[\n\\frac{1}{k}\n\\sum_{i}^k X_i\\right]\n= \\frac{1}{k^2}\\sum_{i}^k \\textrm{Var} \\left[X_i\\right]\n= \\frac{\\sigma^2}{k}\n\\] where we used linearity of expectation and linearity of variance (since each \\(X_i\\) is independent).\nApplying the variance reduction analysis to our algorithm, we have that \\(\\mathbb{E}[S] = \\mu\\) as before but now \\(\\textrm{Var}(S) \\leq \\frac{\\mu^2}{k}\\). Then Chebyshev’s inequality gives \\[\n\\Pr\\left(|S - \\mu| \\geq c \\frac{\\mu}{\\sqrt{k}} \\right) \\leq \\frac{1}{c^2}.\n\\] Setting \\(c = \\frac{1}{\\sqrt{\\delta}}\\) and \\(k = \\frac{1}{\\epsilon^2 \\delta}\\), we have \\[\n\\Pr(|S - \\mu| \\geq \\epsilon \\mu) \\leq \\delta.\n\\]\nWe’re nearly done except that we have only bounded \\(S\\) around its expectation. We really need to bound \\(\\hat{D}\\) around its expectation. We know that with probability \\(\\delta\\), \\[\n(1-\\epsilon) \\mathbb{E}[S]\n\\leq S \\leq (1+\\epsilon) \\mathbb{E}[S].\n\\] Inverting gives, \\[\n\\frac{1}{(1+\\epsilon) \\mathbb{E}[S]} \\leq \\frac{1}{S} \\leq \\frac{1}{(1-\\epsilon) \\mathbb{E}[S]}.\n\\] We can use the fact (easily verified on a graphing calculator like Desmos) that \\(\\frac{1}{1+\\epsilon} \\leq 1 - 2\\epsilon\\) and \\(\\frac{1}{1-\\epsilon} \\geq 1 + 2\\epsilon\\). We subtract 1 from each term to get \\[\n(1-2\\epsilon) \\frac{1}{\\mathbb{E}[S]} - 1\n\\leq \\frac{1}{S} - 1 \\leq (1+2\\epsilon)\n\\frac{1}{\\mathbb{E}[S]} - 1.\n\\] Then, for \\(\\mathbb{E}[S] \\leq \\frac12\\), we have \\[\n(1-4 \\epsilon)\\left( \\frac{1}{\\mathbb{E}[S]} - 1 \\right)\n\\leq \\frac{1}{S} - 1\n\\leq (1+4\\epsilon) \\left( \\frac{1}{\\mathbb{E}[S]} - 1 \\right).\n\\] (You can check the inequalities by plotting here.) Since \\(D = \\frac{1}{\\mathbb{E}[S]} - 1\\) and \\(\\hat{D} = \\frac{1}{S} - 1\\), we have that with probability \\(\\delta\\), \\[\n(1-4\\epsilon) D \\leq \\hat{D} \\leq (1+4\\epsilon) D.\n\\] We lose a small factor on the \\(\\epsilon\\) but this goes into the big-O notation. So our final space complexity is \\(O(k \\log D) = O\\left(\\frac{\\log D}{\\epsilon^2 \\delta}\\right)\\). The \\(\\log D\\) factor comes from the fact that each of the \\(k\\) hash functions needs \\(\\log D\\) bits to represent the input. Impressively, the space complexity has no linear dependence on the number of distinct elements \\(D\\). We know that the \\(\\frac{1}{\\epsilon^2}\\) dependence cannot be improved but we can get a better bound depending on \\(\\log(1/\\delta)\\) instead of \\(1/\\delta\\) by using a stronger concentration inequality.\nFor our analysis, we assumed that we could hash to real numbers on \\([0,1]\\). In practice, the hash function maps to bit vectors and the estimate \\(S\\) is the number of leading zeros in the bit vector. Intuitively, the more distinct hashes we see, the higher we expect the maximum number of leading zeros to be.\nWhile the space complexity is very small, the true benefit of the algorithm is that it can be implemented in a distributed setting. The estimates \\(S_i\\) from each machine can be sent to a central location and the final \\(S = \\min_i S_i\\) can be computed there. Use cases of the algorithm include counting the number of distinct users in New York City that made at least one search containing the word ‘car’ in the last month or counting the number of distinct subject lines in emails sent by users that have registered in the last week (to detect spam). Answering such a query requires a distributed linear scan over the database that can be done in less than two seconds using Google’s implementation."
  },
  {
    "objectID": "notes/06_high_dimensional_geometry.html",
    "href": "notes/06_high_dimensional_geometry.html",
    "title": "High-Dimensional Geometry",
    "section": "",
    "text": "A unifying theme of this course is how we deal with data in high dimensions. We’ll see how to use:\nWe often visualize data and algorithms in one, two, or three dimensions. Today, we’ll prove that high-dimensional space looks fundamentally different from low-dimensional space. In particular, we’ll see how our understanding and intuition from low-dimensional space quickly breaks in higher dimensions. After establishing the weirdness of high-dimensional space, we’ll happily ignore the weirdness and see how to reduce the dimension of high-dimensional data."
  },
  {
    "objectID": "notes/06_high_dimensional_geometry.html#high-dimensional-geometry-is-weird",
    "href": "notes/06_high_dimensional_geometry.html#high-dimensional-geometry-is-weird",
    "title": "High-Dimensional Geometry",
    "section": "High-Dimensional Geometry is Weird",
    "text": "High-Dimensional Geometry is Weird\nRecall the inner product between two \\(d\\)-dimensional vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\). We’ll use bolded notation like \\(\\mathbf{x}\\) to denote vectors and non-bolded notation like \\(x[i]\\) to denote scalars. Depending on the context, we’ll use \\(x[i]\\) or \\(x_i\\) to refer to the \\(i\\)th entry of the vector \\(\\mathbf{x}\\). We can write the inner product in several equivalent ways: \\[\\begin{align}\n\\langle \\mathbf{x}, \\mathbf{y} \\rangle\n= \\mathbf{x}^\\top \\mathbf{y}\n= \\mathbf{y}^\\top \\mathbf{x}\n= \\sum_{i=1}^d x[i] y[i].\n\\end{align}\\] When we take the inner product between a vector and itself, we get the squared norm: \\[\\begin{align}\n\\langle \\mathbf{x}, \\mathbf{x} \\rangle\n= \\mathbf{x}^\\top \\mathbf{x}\n= \\sum_{i=1}^d x[i]^2\n= \\|\\mathbf{x}\\|_2^2.\n\\end{align}\\] As we know from linear algebra, the inner product is a measure of how similar two vectors are. In particular, we can write \\[\\begin{align}\n\\langle \\mathbf{x}, \\mathbf{y} \\rangle\n= \\|\\mathbf{x}\\|_2 \\|\\mathbf{y}\\|_2 \\cos \\theta,\n\\end{align}\\] where \\(\\theta\\) is the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). When \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are both unit vectors (their squared norm is 1), the inner product is just the cosine of the angle between them. When \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal, the angle between them is \\(\\pi/2\\) and their inner product is 0.\n\nOrthogonal Vectors\nLet’s start off with a simple question. What is the size of the largest set of mutually orthogonal unit vectors in \\(d\\) dimensions? In other words, what is the largest value of \\(t\\) so that we can have a set of vectors \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_t\\) such that \\(\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle = 0\\) for all \\(i \\neq j\\)?\nThe answer is \\(d\\). One example of such a set of vectors is the standard basis vectors: \\(\\mathbf{x}_i\\) has a 1 in the \\(i\\)th entry and 0 everywhere else.\nWe can also see that \\(d\\) is the largest possible size of such a set by contradiction. Suppose we have a set of more than \\(d\\) orthogonal unit vectors. Since we have \\(d\\) orthogonal vectors in \\(d\\) dimensions, we know that they span the entire space. Then, we can write the \\(d+1\\)th vector as \\[\\begin{align}\n\\mathbf{x}_{d+1} = \\sum_{i=1}^d \\alpha_i \\mathbf{x}_i\n\\end{align}\\] for some coefficients \\(\\alpha_i\\) where there is at least one \\(j\\) such that \\(\\alpha_j \\neq 0\\). In words, we can write \\(\\mathbf{x}_{d+1}\\) as the span of the first \\(d\\) vectors. Taking the inner product of both sides with \\(\\mathbf{x}_j\\) gives \\[\\begin{align}\n\\langle \\mathbf{x}_{d+1}, \\mathbf{x}_j \\rangle\n= \\sum_{i=1}^d \\alpha_i \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle\n= \\alpha_j.\n\\end{align}\\] We have a contradiction since we assumed \\(\\langle \\mathbf{x}_{d+1}, \\mathbf{x}_j \\rangle = 0\\) for all \\(j \\in [d]\\).\nNow let’s pose a related question. What is the size of the largest set of nearly orthogonal unit vectors in \\(d\\) dimensions? In other words, what is the largest value of \\(t\\) so that we can have a set of vectors \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_t\\) such that \\(|\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle| \\leq \\epsilon\\) for all \\(i \\neq j\\)?\nWe may expect that the answer is at least close to \\(d\\). But, it turns out, there is a set with an exponential number of nearly orthogonal vectors.\n\n\nProbabilistic Method\nWe’ll prove this with the probabilistic method. Let \\(t=2^{c \\epsilon^2 d}\\) for some constant \\(c\\). We’ll construct a random process that generates random vectors \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_t\\) that are unlikely to have large inner products. Then we’ll show that, with non-zero probability, the inner product \\(|\\mathbf{x}_i^\\top \\mathbf{x}_j| \\leq \\epsilon\\) for all \\(i \\neq j\\). We can then conclude that there must exist some set of \\(t\\) unit vectors with all inner products less than \\(\\epsilon\\).\nThe random vectors we’ll consider each have entries that are independently \\(\\frac1{\\sqrt{d}}\\) and \\(-\\frac1{\\sqrt{d}}\\) with equal probability. Then we can make several observations. The vectors have unit norm: \\[\\begin{align}\n\\|\\mathbf{x}_i\\|_2^2 = \\sum_{j=1}^d x_i[j]^2 = \\sum_{j=1}^d \\frac1{d} = 1.\n\\end{align}\\] The expected inner product between vectors \\(i\\neq j\\) is 0: \\[\\begin{align}\n\\mathbb{E}[\\mathbf{x}_i^\\top \\mathbf{x}_j]\n= \\sum_{k=1}^d \\mathbb{E}\\left[x_i[k] x_j[k]\\right]\n= \\sum_{k=1}^d \\mathbb{E}\\left[x_i[k]] \\mathbb{E}[x_j[k]\\right] = 0.\n\\end{align}\\] The penultimate equality is because each entry is independent. The last equality is because the expected value of each entry is 0. The next observation we can make is that the variance of the inner product is \\(\\frac1{d}\\): \\[\\begin{align}\n\\textrm{Var}[\\mathbf{x}_i^\\top \\mathbf{x}_j]\n&= \\sum_{k=1}^d \\textrm{Var}[x_i[k] x_j[k]]\\\\\n&= \\sum_{k=1}^d \\mathbb{E}[x_i[k]^2 x_j[k]^2] - \\mathbb{E}[x_i[k] x_j[k]]^2 \\\\\n&= \\sum_{k=1}^d \\frac1{d^2} - 0^2 = \\frac1{d}.\n\\end{align}\\] The first equality is by linearity of variance. The third equality is because \\(x_i[k]^2 = \\frac1{d}\\).\nDefine a random variable \\(Z = \\mathbf{x}_i^\\top \\mathbf{x}_j\n= \\sum_{k=1}^d C_k\\) where each \\(C_k\\) is \\(\\frac1{d}\\) or \\(-\\frac1{d}\\) with equal probability. Since \\(Z\\) is a sum of many i.i.d. random variables, we expect \\(Z\\) to approximate a Gaussian. If \\(Z\\) were a Gaussian, then \\[\\begin{align}\n\\Pr\\left( | Z | \\geq \\alpha \\frac1{\\sqrt{d}}\\right) =\n\\Pr\\left( | Z - \\mathbb{E}[Z] | \\geq \\alpha \\sigma\\right)\n\\leq O(e^{-\\alpha^2}).\n\\end{align}\\] Then we’d be done by setting \\(\\alpha = \\epsilon \\sqrt{d}\\).\nBut, since \\(Z\\) is only approximates a Gaussian, we need to be a bit more careful. We’d like to apply Chernoff’s bound. However, Chernoff’s bound requires that the random variable be binary. With some clever algebra, we can write \\(Z\\) as a sum of binary random variables. \\[\\begin{align}\nZ &= \\sum_{k=1}^d C_k = \\frac2{d} \\sum_{k=1}^d \\frac{d}{2} C_k \\\\\n&= \\frac2{d} \\sum_{k=1}^d \\left( -\\frac1{2} + B_k \\right) \\\\\n&= \\frac2{d} \\left( - \\frac{d}{2} + \\sum_{k=1}^d B_k \\right).\n\\end{align}\\] Here, we used the fact that \\(\\frac{d}{2} C_k\\) is \\(\\frac1{2}\\) or \\(-\\frac1{2}\\) with equal probability. Let \\(B_k\\) be a binary random variable that is 1 or 0 with equal probability. Then \\(\\frac{d}{2} C_k =  - \\frac12 + B_k\\).\nWe’re interested in the event that \\(Z &gt; \\epsilon\\). With our expression for \\(Z\\), this is equivalent to \\[\\begin{align}\n\\sum_{k=1}^d B_k &gt; \\frac{d}{2} + \\frac{d}{2} \\epsilon.\n\\end{align}\\] Similarly, the event that \\(Z &lt; -\\epsilon\\) is equivalent to \\[\\begin{align}\n\\sum_{k=1}^d B_k &lt; \\frac{d}{2} - \\frac{d}{2} \\epsilon.\n\\end{align}\\] Let \\(B = \\sum_{k=1}^d B_k\\). Notice that \\(\\mathbb{E}[B] = \\frac{d}{2}\\). Putting these last three observations together, we have that \\[\\begin{align}\n\\Pr( | Z | \\geq \\epsilon)\n&= \\Pr( B \\geq (1+\\epsilon) \\mathbb{E}[B] )\n+ \\Pr(B \\leq (1-\\epsilon) \\mathbb{E}[B] ) \\\\\n&= \\Pr(| B - \\mathbb{E}[B] | \\geq \\epsilon \\mathbb{E}[B]) = (*)\n\\end{align}\\] Notice this is exactly in the right form for Chernoff’s bound. Then Chernoff’s bound gives \\[\\begin{align}\n(*) \\leq 2 \\exp\\left(\\frac{-\\epsilon^2 \\mathbb{E}[B]}{3}\\right)\n= 2 \\exp\\left(\\frac{-\\epsilon^2 d}{6}\\right).\n\\end{align}\\]\nBy the union bound, the probability that any pair of vectors has an inner product larger than \\(\\epsilon\\) is at most the number of pairs times the probability that a particular pair has an inner product larger than \\(\\epsilon\\). That is, \\[\\begin{align}\n\\Pr( \\exists i \\neq j : | \\mathbf{x}_i^\\top \\mathbf{x}_j | \\geq \\epsilon)\n\\leq \\binom{t}{2} 2 \\exp\\left(\\frac{-\\epsilon^2 d}{6}\\right).\n\\end{align}\\] We want to choose \\(t\\) so that the probability that any pair of vectors has an inner product larger than \\(\\epsilon\\) is strictly less than 1. Solving for \\(t\\) we find \\[\\begin{align}\n\\binom{t}{2} 2 \\exp\\left(\\frac{-\\epsilon^2 d}{6}\\right) &lt; 1\n&\\Leftrightarrow\nt(t-1) &lt; \\exp\\left( \\frac{\\epsilon^2d}{6} \\right) \\\\\n&\\Leftarrow t = \\exp\\left( \\frac{\\epsilon^2d}{12} \\right)\n= 2^{c \\epsilon^2 d}\n\\end{align}\\] for \\(c = \\frac{\\log_2(e)}{12} \\approx 0.12\\). Since the probability that any pair of vectors has an inner product larger than \\(\\epsilon\\) is strictly less than 1, there must exist some set of \\(t\\) vectors with all inner products less than \\(\\epsilon\\).\nA corollary of the proof that we just saw is that random vectors tend to be far apart in high dimensions. This means that, if our data distribution is random, we typically need an exponential amount of data to learn a pattern. Nonetheless, we often get away with a much smaller amount of data in practice. The reason is that the there generally exists a lower dimensional structure in our data that we can learn. For example, data may lie in a low-dimensional subspace (or it does so after a transformation). Alternatively, the pattern we’re learning can come from a specific class of functions that we can learn with a small amount of data.\nBefore we discuss how to learn low-dimensional structure, let’s see how high-dimensional geometry is so different from what our intuition tells us.\n\n\nWhere Points Live\nLet \\(\\mathcal{B}_d(R)\\) be the ball in \\(d\\) dimensions with radius \\(R\\). Formally, \\[\\begin{align}\n\\mathcal{B}_d(R) = \\{ \\mathbf{x} \\in \\mathbb{R}^d : \\|\\mathbf{x}\\|_2 \\leq R \\}.\n\\end{align}\\] We’ll refer to the unit ball simply as \\(\\mathcal{B}_d\\). What fraction of the volume of the unit ball \\(\\mathcal{B}_d\\) falls within \\(\\epsilon\\) of its surface?\n\n\n\nWell we know that the volume of a radius \\(R\\) ball in \\(d\\) dimensions is \\(\\frac{\\pi^{d/2}}{(d/2)!} R^d\\). (Here, we assume that \\(d\\) is even but there is a similar expression for odd \\(d\\).) Then the volume of the ball within \\(\\epsilon\\) of the surface is \\[\\begin{align}\n\\frac{\\textrm{Vol}(\\mathcal{B_d}) - \\textrm{Vol}(\\mathcal{B_d}(1-\\epsilon))}{\\textrm{Vol}(\\mathcal{B_d})}\n&= 1- \\frac{\\frac{\\pi^{d/2}}{(d/2)!} (1-\\epsilon)^d}{\\frac{\\pi^{d/2}}{(d/2)!} 1^d}\n= 1 - (1-\\epsilon)^d \\\\\n&= 1- \\left((1-\\epsilon)^{1/\\epsilon}\\right)^{\\epsilon d}\n\\approx 1 - \\frac1{e^{\\epsilon d}}.\n\\end{align}\\] So all but a \\(\\frac1{2^{c \\epsilon d}}\\) fraction of the volume of the unit ball is within \\(\\epsilon\\) of its surface for a constant \\(c\\).\nWe may wonder about how the unit ball compares to other high-dimensional shapes. In fact, by the isoperimetric inequality, the unit ball has the minimum ratio between the surface area and volume of any shape. If we randomly sample points from any high-dimensional shape, nearly all will fall near its surface. One interpretation of this fact is that nearly all points are outliers. Let’s see this intuition for cubes.\n\n\n\nConsider the cubes with side length \\(10\\) in the figure above. In one dimension, the ratio of the cubes on the surface to the total number of cubes is \\(\\frac{2}{10}=.2\\). In two dimensions, the ratio is \\(\\frac{10^2 - 8^2}{10^2} = .36\\). In two dimensions, the ratio is \\(\\frac{10^3 - 8^3}{10^3} = .488\\). As we increase the dimension, the ratio approaches 1.\nNow let’s consider a slightly different question about the sphere. What fraction of the volume of the unit ball falls within \\(\\epsilon\\) of its equator?\n\n\n\nLet \\(S\\) be the set of points within \\(\\epsilon\\) of the equator. Formally, \\[\\begin{align}\nS = \\{ \\mathbf{x} \\in \\mathcal{B}_d : |x_1| \\leq \\epsilon \\}.\n\\end{align}\\] Surprisingly, we’ll show that all but a small fraction of the volume of the unit ball falls within \\(\\epsilon\\) of its equator. By the symmetry of the unit ball, this is true for any equator.\nSo:\n\nWe know that all but a \\(\\left(\\frac1{2}\\right)^{c \\epsilon d}\\) fraction of the volume of the unit ball is within \\(\\epsilon\\) of its surface and\nWe (are about to) know that all but a \\(\\left(\\frac1{2}\\right)^{c \\epsilon^2 d}\\) fraction of the volume of the unit ball is within \\(\\epsilon\\) of any equator for some constant \\(c\\).\n\n\n\n\nClearly, the high-dimensional unit ball looks nothing like the circle!\nLet’s show our claim that all but a small fraction of the volume of the unit ball falls within \\(\\epsilon\\) of its equator. We can show this probabilistically by drawing random points from the unit ball. Formally, we’ll show that if we draw a random point \\(\\mathbf{x}\\) from the unit ball, then \\(|x_1| \\leq \\epsilon\\) with large probability.\nGenerating a random vector \\(\\mathbf{x}\\) from the interior of the unit ball is quite difficult. We’ll instead consider a random vector \\(\\mathbf{w}\\) from the surface of the unit ball. Given a random vector \\(\\mathbf{x}\\) from the unit ball, we can put it onto the surface of the unit ball by normalizing it: define \\(\\mathbf{w} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\\). Because \\(\\|\\mathbf{x}\\|_2 \\leq 1\\), we know \\[\\begin{align}\n\\Pr( |x_1| \\leq \\epsilon) \\geq \\Pr( |w_1| \\leq \\epsilon).\n\\end{align}\\]\nSo it suffices to show that \\(|w_1| \\leq \\epsilon\\) with probability at least \\(1-\\frac1{2^{c \\epsilon^2 d}}\\). Let’s consider how to generate a random vector \\(\\mathbf{w}\\) from the surface of the unit ball.\n\n\nRotational Invariance of the Gaussian\nLet \\(\\mathbf{g} \\in \\mathbb{R}^d\\) be a random Gaussian vector with each entry drawn from the standard normal distribution \\(\\mathcal{N}(0,1)\\). Then \\(\\mathbf{w} = \\frac{\\mathbf{g}}{\\|\\mathbf{g}\\|_2}\\) is uniformly distributed on the surface of the unit ball.\nTo see why, consider \\(p: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) the probability density function of the surface of the unit ball. There are two key properties of \\(p\\):\n\nEach point on the surface of the unit ball is equally likely.\nThe distribution of each dimension is the same.\n\nThese two properties are enough to show that \\(p = \\exp( -\\|\\mathbf{w}\\|_2^2)\\), at least up to constants. For a phenomenal explanation, check out this 3Blue1Brown video.\nIn order to analyze \\(\\mathbf{w}\\), we’ll first analyze \\(\\mathbf{g}\\). The expectation of \\(\\|\\mathbf{g}\\|_2^2\\) is 0: \\[\\begin{align}\n\\mathbb{E}[ \\|\\mathbf{g}\\|_2^2 ]\n= \\sum_{i=1}^d \\mathbb{E}[g_i^2]\n= \\sum_{i=1}^d \\textrm{Var}(g_i) = d.\n\\end{align}\\] The second equality is because each entry has mean 0 and variance 1.\nWe will use the following inequality: \\[\\begin{align}\n\\Pr \\left(\n\\| \\mathbf{g} \\|_2^2 \\leq \\frac{d}{2}\n\\right)\n\\leq \\frac{1}{2^{c d}}\n\\end{align}\\] for some constant \\(c\\). The proof uses the Johnson-Lindenstrauss lemma which we’ll introduce shortly.\nWe will condition on the event that \\(\\| \\mathbf{g} \\|_2 \\geq \\sqrt{d/2}\\) for the next inequality. Then \\[\\begin{align}\n\\Pr\\left( |w_1| \\leq \\epsilon \\mid \\| \\mathbf{g} \\|_2 \\geq \\sqrt{d/2}\\right)\n&= \\Pr\\left( |w_1| \\sqrt{d/2} \\leq \\epsilon \\sqrt{d/2} \\mid \\| \\mathbf{g} \\|_2 \\geq \\sqrt{d/2}\\right) \\\\\n&\\geq \\Pr\\left( |g_1| \\leq \\epsilon \\sqrt{d/2} \\mid \\| \\mathbf{g} \\|_2 \\geq \\sqrt{d/2}\\right) \\\\\n&\\geq 1 - \\frac{1}{2^{\\left(c \\epsilon \\sqrt{d/2}\\right)^2}}.\n\\end{align}\\] The last inequality follows by the Gaussian tail bound.\nWe need both \\(\\| \\mathbf{g} \\|_2 \\geq \\sqrt{d/2}\\) and \\(|g_1| &lt; \\epsilon \\sqrt{d/2}\\) for \\(|w_1| \\leq \\epsilon\\). By the union bound, we have \\[\\begin{align}\n\\Pr( |w_1| \\leq \\epsilon)\n\\geq 1 - \\frac{1}{2^{\\left(c \\epsilon \\sqrt{d/2}\\right)^2}} - \\frac{1}{2^{c d}}\n\\end{align}\\]\nThis proves the claim that nearly all of the volume of the unit ball falls within \\(\\epsilon\\) of its equator.\nLet \\(\\mathcal{C}_d\\) be the \\(d\\)-dimensional unit cube. Formally, \\[\\begin{align}\n\\mathcal{C}_d = \\{ \\mathbf{x} \\in \\mathbb{R}^d : |x_i| \\leq 1 \\textrm{ for all } i \\in [d] \\}.\n\\end{align}\\] In two dimensions, the cube is pretty similar to the ball.\nBut in high dimensions, the cube is very different from the ball. The volume of the unit cube is \\(2^d\\) while the volume of the unit ball is \\(\\frac{\\pi^{d/2}}{(d/2)!}\\). The ratio of the volume of the unit cube to the unit ball is \\[\\begin{align}\n\\frac{\\textrm{Vol}(\\mathcal{C}_d)}{\\textrm{Vol}(\\mathcal{B}_d)}\n= \\frac{2^d (d/2)!}{\\pi^{d/2}}\n\\approx d^d.\n\\end{align}\\] The cube has way way more volume!\nThere are some other ways to see that these shapes are very different. We have \\[\\begin{align}\n\\max_{\\mathbf{x} \\in \\mathcal{B}_d} \\|\\mathbf{x}\\|_2^2 = 1\n\\hspace{1em} \\textrm{ but } \\hspace{1em}\n\\max_{\\mathbf{x} \\in \\mathcal{C}_d} \\|\\mathbf{x}\\|_2^2 = d.\n\\end{align}\\] We may think that such vectors are extreme. However, we have \\[\\begin{align}\n\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{B}_d} [\\|\\mathbf{x}\\|_2^2 ]\\leq 1\n\\end{align}\\] but \\[\\begin{align}\n\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{C}_d} [\\|\\mathbf{x}\\|_2^2 ]\n= \\sum_{i=1}^d \\mathbb{E}_{x_i \\sim \\mathcal{U}(-1,1)}[ x_i^2]\n= \\sum_{i=1}^d \\frac13 = \\frac{d}{3}.\n\\end{align}\\] For the penultimate equality, we used the following fact about the uniform distribution on \\([-1,1]\\): The expectation of \\(x^2\\) for \\(x \\sim \\mathcal{U}(-1,1)\\) is \\[\\begin{align}\n\\int_{x=-1}^1 x^2 \\frac1{2} dx = \\frac13.\n\\end{align}\\]\n\n\n\nTogether, we have painted the following picture: Almost all of the volume of the unit cube falls in its corners and these corners lie far outside the unit ball.\nCheck out the Quanta Magazine article The Journey to Define Dimension for another fun example comparing cubes and balls.\nDespite all this warning that low-dimensional space looks nothing like high-dimensional space, we’ll next learn about how to compress high-dimensional vectors into low-dimensional vectors.\nWe will be very careful not to compress the vectors too far. An extremely simple method known as the Johnson-Lindenstrauss random projection pushes right up to the edge of how much compression is possible."
  },
  {
    "objectID": "notes/09_gradient_descent.html",
    "href": "notes/09_gradient_descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "We will consider the general following problem: Given a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\), we want to find a vector \\(\\hat{\\mathbf{x}} \\in \\mathbb{R}^d\\) that approximately minimizes \\(f\\). That is, we want \\(\\hat{\\mathbf{x}}\\) such that \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq \\min_{\\mathbf{x}} f(\\mathbf{x}) + \\epsilon\n\\end{align*}\\] for some small constant \\(\\epsilon\\).\nThe problem is at the core of machine learning. Typically, the function \\(f\\) is a loss function that measures how well a model fits the data. The goal is to find parameters \\(\\hat{\\mathbf{x}}\\) that achieve small loss.\nLeast Squares Regression Example\nAs an example, we’ll consider least squares regression where we fit the data with a linear model and the loss function is the \\(\\ell_2\\)-norm. Consider \\(n\\) data points \\(\\mathbf{a}^{(1)}, \\ldots, \\mathbf{a}^{(n)} \\in \\mathbb{R}^d\\) with corresponding labels \\(y^{(1)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). We will consider the linear model \\(\\mathbf{x}^\\top \\mathbf{a}\\) where \\(\\mathbf{x}\\) is the parameter vector. The loss function is given by \\[\\begin{align*}\nf(\\mathbf{x}) = \\sum_{i=1}^n (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)})^2\n= \\| \\mathbf{A x} - \\mathbf{y} \\|_2^2.\n\\end{align*}\\] In the last expression, we used compact linear algebra notation where the data matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) has rows \\(\\mathbf{a}^{(i)}\\) and the label vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\) has entries \\(y^{(i)}\\).\nDetermining how to find the best parameters \\(\\mathbf{x}\\) is a fundamental problem in machine learning. The choice of algorithm depends on the form of the function \\(f\\) (e.g. linear, quadratic, finite sum structure) and additional constraints on the parameters.\nGradient descent is a greedy algorithm for minimizing a function of multiple variables that often works amazingly well in practice. The algorithm iteratively updates the parameters \\(\\mathbf{x}\\) by moving in the direction of steepest descent. In this way, the algorithm hopefully decreases the function at each iteration.\n\n\n\nBefore we analyze gradient descent, we’ll review some calculus.\n\n\nFor \\(i \\in \\{1, \\ldots, d\\} = [d]\\), let \\(x_i\\) be the \\(i\\)th entry of \\(\\mathbf{x}\\). We will use \\(\\mathbf{e}^{(i)}\\) to denote the \\(i\\)th standard basis vector with 0s everywhere except for a 1 in the \\(i\\)th entry.\nRecall the partial derivative of a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) with respect to the \\(i\\)th variable is given by\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_i}\n(\\mathbf{x}) = \\lim_{t \\to 0} \\frac{f(\\mathbf{x} + t \\mathbf{e}^{(i)}) - f(\\mathbf{x})}{t}.\n\\end{align*}\\]\nThe directional derivative of \\(f\\) in the direction \\(\\mathbf{v} \\in \\mathbb{R}^d\\) is given by \\[\\begin{align*}\nD_{\\mathbf{v}} f(\\mathbf{x}) =\n\\lim_{t \\to 0} \\frac{f(\\mathbf{x} + t \\mathbf{v}) - f(\\mathbf{x})}{t}.\n\\end{align*}\\]\nIn this notation, the partial derivative is the directional derivative in the direction of the \\(i\\)th standard basis vector. That is, \\(\\frac{\\partial f}{\\partial x_i}(\\mathbf{x}) = D_{\\mathbf{e}^{(i)}} f(\\mathbf{x})\\).\nWe are interested not just in the partial derivative of the function with respect to one variable but the gradient of the function with respect to all variables.\nThe gradient of \\(f\\) is the vector of partial derivatives \\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_d}(\\mathbf{x})\n\\end{bmatrix}.\n\\end{align*}\\]\nWith this notation, we can write the directional derivative as \\[\\begin{align*}\nD_\\mathbf{v} f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^\\top \\mathbf{v}.\n\\end{align*}\\] While not immediately obvious, the proof is straightforward and can be found here.\nWhen we solve the problem of minimizing the function \\(f\\), we need a way of accessing \\(f\\). We generally have a function oracle which evaluates \\(f(\\mathbf{x})\\) for any \\(\\mathbf{x}\\) and a gradient oracle which evaluates \\(\\nabla f(\\mathbf{x})\\) for any \\(\\mathbf{x}\\). We say access to the function oracle gives zeroth-order information and access to the gradient oracle gives first-order access. We view the implementation of these oracles as a black box. However, computing the function value and gradient can be computationally expensive.\nComputational Cost for Least Squares Regression Let’s consider the computational cost of evaluating the oracles for the least squares regression problem. When \\(f(\\mathbf{x}) = \\| \\mathbf{A x} - \\mathbf{y} \\|_2^2\\), the function oracle requires computing a matrix-vector product and a vector-vector addition. Since \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{x} \\in \\mathbb{R}^d\\), the matrix-vector product costs \\(O(nd)\\) operations. Since \\(\\mathbf{A x} \\in \\mathbb{R}^n\\) and \\(\\mathbf{y} \\in \\mathbb{R}^n\\), the vector-vector addition costs \\(O(n)\\) operations. So the total cost of the function evaluation is \\(O(nd)\\).\nIn order to compute the cost of the gradient, let’s compute the gradient for the least squares regression problem.\nRecall we can write\n\\[\\begin{align*}\nf(\\mathbf{x}) \\sum_{i=1}^n (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)})^2\n\\end{align*}\\]\nso, by the chain rule, the partial derivative is given by\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_j} (\\mathbf{x})\n&= \\sum_{i=1}^n 2 (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)}) \\frac{\\partial}{\\partial x_j} (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)}) \\\\\n&= \\sum_{i=1}^n 2 a_j^{(i)} (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)}) \\\\\n&= 2 \\mathbf{A}_j^\\top (\\mathbf{A x} - \\mathbf{y})\n\\end{align*}\\] where \\(\\mathbf{A}_j\\) is the \\(j\\)th column of \\(\\mathbf{A}\\).\nThen the gradient is given by \\(\\nabla f(\\mathbf{x}) = 2 \\mathbf{A}^\\top (\\mathbf{A x} - \\mathbf{y})\\). We can compute \\(\\mathbf{A x} - \\mathbf{y} \\in \\mathbb{R}^n\\) with \\(O(nd)\\) operations. Then, to get the full gradient, we compute \\(\\mathbf{A}^\\top (\\mathbf{A x} - \\mathbf{y}) \\in \\mathbb{R}^d\\) with \\(O(n^2 d)\\) operations since \\(\\mathbf{A}^\\top \\in \\mathbb{R}^{d \\times n}\\). So the total cost of the gradient evaluation is \\(O(n^2 d)\\).\n\n\n\nLet’s consider the general gradient descent algorithm. The key idea is to iteratively update the parameters \\(\\mathbf{x}\\) by making a small adjustment that decreases \\(f(\\mathbf{x})\\). In particular, we will update \\(\\mathbf{x} \\gets \\mathbf{x} + \\eta \\mathbf{v}\\) where \\(\\eta &gt; 0\\) is the step size.\nIn order to determine how we should set \\(\\mathbf{v}\\), we will consider the case when \\(\\eta\\) is small. In this case, we can approximate \\[\\begin{align*}\nf(\\mathbf{x} + \\eta \\mathbf{v})\n- f(\\mathbf{x})\n\\approx D_\\mathbf{v} f(\\mathbf{x}) \\eta\n= \\nabla f(\\mathbf{x})^\\top \\mathbf{v} \\eta.\n\\end{align*}\\] In order to decrease \\(f(\\mathbf{x})\\), we want to choose \\(\\mathbf{v}\\) such that \\(\\nabla f(\\mathbf{x})^\\top \\mathbf{v}\\) is as negative as possible. Recall the inner product is maximized when the vectors are parallel so we set \\(\\mathbf{v} = -\\nabla f(\\mathbf{x})\\). Then \\[\\begin{align*}\nf(\\mathbf{x} + \\eta \\mathbf{v})\n- f(\\mathbf{x})\n\\approx -\\nabla f(\\mathbf{x})^\\top \\nabla f(\\mathbf{x}) \\eta = -\\| \\nabla f(\\mathbf{x}) \\|_2^2 \\eta.\n\\end{align*}\\]\nLet’s formalize the gradient descent algorithm. We will initialize \\(\\mathbf{x}^{(0)} \\in \\mathbb{R}^d\\) to some initial point. For each iteration \\(t \\in [T]\\), we will compute the gradient \\(\\nabla f(\\mathbf{x}^{(t)})\\). Then we will update the parameters by setting \\(\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)})\\). Finally, we return the best parameters \\(\\hat{\\mathbf{x}} = \\arg \\min_{t \\in [T]} \\mathbf{x}^{(t)}\\).\nNotice we need to choose the step size parameter \\(\\eta\\) and the number of iterations \\(T\\).\n\n\n\nToday, we’ll show that if \\(f\\) is a convex function, then gradient descent converges to a near global minimum when the step size \\(\\eta\\) is small and the number of iterations \\(T\\) is large. Formally, \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq f(\\mathbf{x}^*) + \\epsilon\n\\end{align*}\\] where \\(\\mathbf{x}^*\\) is a global minimum of \\(f\\). Examples of convex functions include least squares reduction, logistic regression, kernel regression, and support vector machines.\nGradient descent also works for non-convex function but the convergence guarantees are not as strong. In particular, gradient descent will converge to a near stationary point. Formally, \\[\\begin{align*}\n\\| \\nabla f(\\mathbf{x}^{(T)}) \\|_2 \\leq \\epsilon.\n\\end{align*}\\] Examples of non-convex functions include neural networks, matrix completion, and mixture models.\n\n\n\nFor machine learning applications, we are primarily interested in how fast gradient descent converges. We will be able to bound the iteration complexity if we make some assumptions on the function \\(f\\). As usual, the stronger the assumptions we can make, the better bounds we can get.\nWe will start by assuming that \\(f\\) is convex. A function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\) and \\(\\lambda \\in [0, 1]\\), we have \\[\\begin{align*}\n(1-\\lambda) f(\\mathbf{x}) + \\lambda f(\\mathbf{y}) \\geq f((1-\\lambda) \\mathbf{x} + \\lambda \\mathbf{y}).\n\\end{align*}\\]\nIn words, every line between two points on the function is above the function.\n\n\n\nEquivalently, the tangent line at any point is below the function. In mathematical notation, we can express the tangent line constraint as \\[\\begin{align*}\nf(\\mathbf{x + z}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\mathbf{z}\n\\end{align*}\\] for all \\(\\mathbf{x}, \\mathbf{z} \\in \\mathbb{R}^d\\). By setting \\(\\mathbf{z} = \\mathbf{y} - \\mathbf{x}\\), we can rewrite the constraint as \\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{y} - \\mathbf{x})\n&\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top (\\mathbf{y} - \\mathbf{x}) \\\\\n&=f(\\mathbf{x}) - \\nabla f(\\mathbf{x})^\\top (\\mathbf{x} - \\mathbf{y}).\n\\end{align*}\\] Rearranging, we get \\[\\begin{align*}\nf(\\mathbf{x}) - f(\\mathbf{y}) \\leq \\nabla f(\\mathbf{x})^\\top (\\mathbf{x} - \\mathbf{y}).\n\\end{align*}\\] We will use this inequality in the analysis of the convergence rate of gradient descent.\nAnother assumption we will make on the function \\(f\\) is that \\(G\\)-Lipschitz. Formally, \\(f\\) is \\(G\\)-Lipschitz if, for all \\(\\mathbf{x}\\), \\(\\| \\nabla f(\\mathbf{x}) \\|_2 \\leq G\\). This means that the function \\(f\\) cannot change too quickly.\nFor our analysis of gradient descent, we will assume that \\(f\\) is convex, \\(f\\) is \\(G\\)-Lipschitz, and our starting point \\(\\mathbf{x}^{(0)}\\) is within \\(R\\) of the global minimum \\(\\mathbf{x}^*\\).\nGradient Descent Convergence Bound: If we run gradient with step size \\(eta =\\frac{R}{G \\sqrt{T}}\\) for \\(T \\geq \\frac{R^2 G^2}{\\epsilon^2}\\) iterations, then \\(f(\\hat{\\mathbf{x}}) \\leq f(\\mathbf{x}^*) + \\epsilon\\).\nIntuitively, the proof is tricky because the iterates do not necessarily decrease monotonically. We will prove that the average iterate gives a good solution and, therefore, the best iterate must also give a good solution.\nWe will first prove an intermediate result that if the function value is far from the global minimum, then we take a large step towards the global minimum. This property is intuitively useful because it means that we make good progress towards the global minimum even when we are far way.\nConsider the difference between the next iterate and the global minimum \\[\\begin{align*}\n\\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2\n&= \\| \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}) - \\mathbf{x}^* \\|_2^2 \\\\\n&= \\| (\\mathbf{x}^{(t)} - \\mathbf{x}^*) - \\eta \\nabla f(\\mathbf{x}^{(t)}) \\|_2^2 \\\\\n&= \\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - 2 \\eta \\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t)} - \\mathbf{x}^*) + \\eta^2 \\| \\nabla f(\\mathbf{x}^{(t)}) \\|_2^2 \\\\\n&\\leq \\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - 2 \\eta \\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t)} - \\mathbf{x}^*) + \\eta^2 G^2\n\\end{align*}\\] where the third equality followed because the squared \\(\\ell_2\\) norm is an inner product and the inequality followed by the \\(G\\)-Lipschitz assumption. Dividing by \\(2 \\eta\\) and rearranging, we get \\[\\begin{align*}\n\\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t)} - \\mathbf{x}^*)\n\\leq \\frac{\\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\]\nBy convexity, it follows that \\[\\begin{align*}\nf(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^*)\n\\leq \\frac{\\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\]\nThe next step is to consider the average difference between the iterates and the global minimum \\[\\begin{align*}\n\\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)} - f(\\mathbf{x}^*)\n&\\leq \\frac{\\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2} \\\\\n&+\\frac{\\| \\mathbf{x}^{(1)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(2)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2} \\\\\n&+ \\ldots + \\\\\n&+ \\frac{\\| \\mathbf{x}^{(T-1)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(T)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\] The expression is a telescoping sum because some terms cancel out. Then we can bound the average as \\[\\begin{align*}\n\\frac1{T} \\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)} - f(\\mathbf{x}^*)\n&\\leq \\frac{\\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2  - \\| \\mathbf{x}^{(T)} - \\mathbf{x}^* \\|_2^2}{2 T \\eta} + \\frac{\\eta G^2}{2} \\\\\n&\\leq \\frac{R^2 - 0}{2 T \\eta} + \\frac{\\eta G^2}{2} \\\\\n&= \\frac{R^2 G \\sqrt{T} }{2 T R} + \\frac{R G^2}{2 G \\sqrt{T}} = \\frac{R G}{\\sqrt{T}} \\epsilon^2.\n\\end{align*}\\] The second inequality used that \\(\\mathbf{x}^{(0)}\\) is within \\(R\\) of the global minimum \\(\\mathbf{x}^*\\) and that \\(\\| \\cdot \\|_2^2\\) is non-negative. The first equality followed by plugging in the step size \\(\\eta = \\frac{R}{G \\sqrt{T}}\\). The second equality followed by plugging in the number of iterations \\(T= \\frac{R^2 G^2}{\\epsilon^2}\\).\nFor the final step, we will consider the average function values of the iterates. Then we can bound the difference between the average iterate and the global minimum as \\[\\begin{align*}\n\\frac1{T} \\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)})\n- f(\\mathbf{x}^*)\n\\leq \\epsilon.\n\\end{align*}\\] It must be that the best iterate gives a solution that is at least as good as the average. Then \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq \\frac1{T} \\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)}) \\leq f(\\mathbf{x}^*) + \\epsilon\n\\end{align*}\\] as desired.\nIt may appear that the bound is not particularly useful because we need to know the right learning rate. In practice, we can run gradient descent several times with different learning rates and choose the best parameters."
  },
  {
    "objectID": "notes/09_gradient_descent.html#gradient-descent",
    "href": "notes/09_gradient_descent.html#gradient-descent",
    "title": "Gradient Descent",
    "section": "",
    "text": "We will consider the general following problem: Given a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\), we want to find a vector \\(\\hat{\\mathbf{x}} \\in \\mathbb{R}^d\\) that approximately minimizes \\(f\\). That is, we want \\(\\hat{\\mathbf{x}}\\) such that \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq \\min_{\\mathbf{x}} f(\\mathbf{x}) + \\epsilon\n\\end{align*}\\] for some small constant \\(\\epsilon\\).\nThe problem is at the core of machine learning. Typically, the function \\(f\\) is a loss function that measures how well a model fits the data. The goal is to find parameters \\(\\hat{\\mathbf{x}}\\) that achieve small loss.\nLeast Squares Regression Example\nAs an example, we’ll consider least squares regression where we fit the data with a linear model and the loss function is the \\(\\ell_2\\)-norm. Consider \\(n\\) data points \\(\\mathbf{a}^{(1)}, \\ldots, \\mathbf{a}^{(n)} \\in \\mathbb{R}^d\\) with corresponding labels \\(y^{(1)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). We will consider the linear model \\(\\mathbf{x}^\\top \\mathbf{a}\\) where \\(\\mathbf{x}\\) is the parameter vector. The loss function is given by \\[\\begin{align*}\nf(\\mathbf{x}) = \\sum_{i=1}^n (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)})^2\n= \\| \\mathbf{A x} - \\mathbf{y} \\|_2^2.\n\\end{align*}\\] In the last expression, we used compact linear algebra notation where the data matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) has rows \\(\\mathbf{a}^{(i)}\\) and the label vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\) has entries \\(y^{(i)}\\).\nDetermining how to find the best parameters \\(\\mathbf{x}\\) is a fundamental problem in machine learning. The choice of algorithm depends on the form of the function \\(f\\) (e.g. linear, quadratic, finite sum structure) and additional constraints on the parameters.\nGradient descent is a greedy algorithm for minimizing a function of multiple variables that often works amazingly well in practice. The algorithm iteratively updates the parameters \\(\\mathbf{x}\\) by moving in the direction of steepest descent. In this way, the algorithm hopefully decreases the function at each iteration.\n\n\n\nBefore we analyze gradient descent, we’ll review some calculus.\n\n\nFor \\(i \\in \\{1, \\ldots, d\\} = [d]\\), let \\(x_i\\) be the \\(i\\)th entry of \\(\\mathbf{x}\\). We will use \\(\\mathbf{e}^{(i)}\\) to denote the \\(i\\)th standard basis vector with 0s everywhere except for a 1 in the \\(i\\)th entry.\nRecall the partial derivative of a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) with respect to the \\(i\\)th variable is given by\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_i}\n(\\mathbf{x}) = \\lim_{t \\to 0} \\frac{f(\\mathbf{x} + t \\mathbf{e}^{(i)}) - f(\\mathbf{x})}{t}.\n\\end{align*}\\]\nThe directional derivative of \\(f\\) in the direction \\(\\mathbf{v} \\in \\mathbb{R}^d\\) is given by \\[\\begin{align*}\nD_{\\mathbf{v}} f(\\mathbf{x}) =\n\\lim_{t \\to 0} \\frac{f(\\mathbf{x} + t \\mathbf{v}) - f(\\mathbf{x})}{t}.\n\\end{align*}\\]\nIn this notation, the partial derivative is the directional derivative in the direction of the \\(i\\)th standard basis vector. That is, \\(\\frac{\\partial f}{\\partial x_i}(\\mathbf{x}) = D_{\\mathbf{e}^{(i)}} f(\\mathbf{x})\\).\nWe are interested not just in the partial derivative of the function with respect to one variable but the gradient of the function with respect to all variables.\nThe gradient of \\(f\\) is the vector of partial derivatives \\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_d}(\\mathbf{x})\n\\end{bmatrix}.\n\\end{align*}\\]\nWith this notation, we can write the directional derivative as \\[\\begin{align*}\nD_\\mathbf{v} f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^\\top \\mathbf{v}.\n\\end{align*}\\] While not immediately obvious, the proof is straightforward and can be found here.\nWhen we solve the problem of minimizing the function \\(f\\), we need a way of accessing \\(f\\). We generally have a function oracle which evaluates \\(f(\\mathbf{x})\\) for any \\(\\mathbf{x}\\) and a gradient oracle which evaluates \\(\\nabla f(\\mathbf{x})\\) for any \\(\\mathbf{x}\\). We say access to the function oracle gives zeroth-order information and access to the gradient oracle gives first-order access. We view the implementation of these oracles as a black box. However, computing the function value and gradient can be computationally expensive.\nComputational Cost for Least Squares Regression Let’s consider the computational cost of evaluating the oracles for the least squares regression problem. When \\(f(\\mathbf{x}) = \\| \\mathbf{A x} - \\mathbf{y} \\|_2^2\\), the function oracle requires computing a matrix-vector product and a vector-vector addition. Since \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{x} \\in \\mathbb{R}^d\\), the matrix-vector product costs \\(O(nd)\\) operations. Since \\(\\mathbf{A x} \\in \\mathbb{R}^n\\) and \\(\\mathbf{y} \\in \\mathbb{R}^n\\), the vector-vector addition costs \\(O(n)\\) operations. So the total cost of the function evaluation is \\(O(nd)\\).\nIn order to compute the cost of the gradient, let’s compute the gradient for the least squares regression problem.\nRecall we can write\n\\[\\begin{align*}\nf(\\mathbf{x}) \\sum_{i=1}^n (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)})^2\n\\end{align*}\\]\nso, by the chain rule, the partial derivative is given by\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_j} (\\mathbf{x})\n&= \\sum_{i=1}^n 2 (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)}) \\frac{\\partial}{\\partial x_j} (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)}) \\\\\n&= \\sum_{i=1}^n 2 a_j^{(i)} (\\mathbf{x}^\\top \\mathbf{a}^{(i)} - y^{(i)}) \\\\\n&= 2 \\mathbf{A}_j^\\top (\\mathbf{A x} - \\mathbf{y})\n\\end{align*}\\] where \\(\\mathbf{A}_j\\) is the \\(j\\)th column of \\(\\mathbf{A}\\).\nThen the gradient is given by \\(\\nabla f(\\mathbf{x}) = 2 \\mathbf{A}^\\top (\\mathbf{A x} - \\mathbf{y})\\). We can compute \\(\\mathbf{A x} - \\mathbf{y} \\in \\mathbb{R}^n\\) with \\(O(nd)\\) operations. Then, to get the full gradient, we compute \\(\\mathbf{A}^\\top (\\mathbf{A x} - \\mathbf{y}) \\in \\mathbb{R}^d\\) with \\(O(n^2 d)\\) operations since \\(\\mathbf{A}^\\top \\in \\mathbb{R}^{d \\times n}\\). So the total cost of the gradient evaluation is \\(O(n^2 d)\\).\n\n\n\nLet’s consider the general gradient descent algorithm. The key idea is to iteratively update the parameters \\(\\mathbf{x}\\) by making a small adjustment that decreases \\(f(\\mathbf{x})\\). In particular, we will update \\(\\mathbf{x} \\gets \\mathbf{x} + \\eta \\mathbf{v}\\) where \\(\\eta &gt; 0\\) is the step size.\nIn order to determine how we should set \\(\\mathbf{v}\\), we will consider the case when \\(\\eta\\) is small. In this case, we can approximate \\[\\begin{align*}\nf(\\mathbf{x} + \\eta \\mathbf{v})\n- f(\\mathbf{x})\n\\approx D_\\mathbf{v} f(\\mathbf{x}) \\eta\n= \\nabla f(\\mathbf{x})^\\top \\mathbf{v} \\eta.\n\\end{align*}\\] In order to decrease \\(f(\\mathbf{x})\\), we want to choose \\(\\mathbf{v}\\) such that \\(\\nabla f(\\mathbf{x})^\\top \\mathbf{v}\\) is as negative as possible. Recall the inner product is maximized when the vectors are parallel so we set \\(\\mathbf{v} = -\\nabla f(\\mathbf{x})\\). Then \\[\\begin{align*}\nf(\\mathbf{x} + \\eta \\mathbf{v})\n- f(\\mathbf{x})\n\\approx -\\nabla f(\\mathbf{x})^\\top \\nabla f(\\mathbf{x}) \\eta = -\\| \\nabla f(\\mathbf{x}) \\|_2^2 \\eta.\n\\end{align*}\\]\nLet’s formalize the gradient descent algorithm. We will initialize \\(\\mathbf{x}^{(0)} \\in \\mathbb{R}^d\\) to some initial point. For each iteration \\(t \\in [T]\\), we will compute the gradient \\(\\nabla f(\\mathbf{x}^{(t)})\\). Then we will update the parameters by setting \\(\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)})\\). Finally, we return the best parameters \\(\\hat{\\mathbf{x}} = \\arg \\min_{t \\in [T]} \\mathbf{x}^{(t)}\\).\nNotice we need to choose the step size parameter \\(\\eta\\) and the number of iterations \\(T\\).\n\n\n\nToday, we’ll show that if \\(f\\) is a convex function, then gradient descent converges to a near global minimum when the step size \\(\\eta\\) is small and the number of iterations \\(T\\) is large. Formally, \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq f(\\mathbf{x}^*) + \\epsilon\n\\end{align*}\\] where \\(\\mathbf{x}^*\\) is a global minimum of \\(f\\). Examples of convex functions include least squares reduction, logistic regression, kernel regression, and support vector machines.\nGradient descent also works for non-convex function but the convergence guarantees are not as strong. In particular, gradient descent will converge to a near stationary point. Formally, \\[\\begin{align*}\n\\| \\nabla f(\\mathbf{x}^{(T)}) \\|_2 \\leq \\epsilon.\n\\end{align*}\\] Examples of non-convex functions include neural networks, matrix completion, and mixture models.\n\n\n\nFor machine learning applications, we are primarily interested in how fast gradient descent converges. We will be able to bound the iteration complexity if we make some assumptions on the function \\(f\\). As usual, the stronger the assumptions we can make, the better bounds we can get.\nWe will start by assuming that \\(f\\) is convex. A function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\) and \\(\\lambda \\in [0, 1]\\), we have \\[\\begin{align*}\n(1-\\lambda) f(\\mathbf{x}) + \\lambda f(\\mathbf{y}) \\geq f((1-\\lambda) \\mathbf{x} + \\lambda \\mathbf{y}).\n\\end{align*}\\]\nIn words, every line between two points on the function is above the function.\n\n\n\nEquivalently, the tangent line at any point is below the function. In mathematical notation, we can express the tangent line constraint as \\[\\begin{align*}\nf(\\mathbf{x + z}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\mathbf{z}\n\\end{align*}\\] for all \\(\\mathbf{x}, \\mathbf{z} \\in \\mathbb{R}^d\\). By setting \\(\\mathbf{z} = \\mathbf{y} - \\mathbf{x}\\), we can rewrite the constraint as \\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{y} - \\mathbf{x})\n&\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top (\\mathbf{y} - \\mathbf{x}) \\\\\n&=f(\\mathbf{x}) - \\nabla f(\\mathbf{x})^\\top (\\mathbf{x} - \\mathbf{y}).\n\\end{align*}\\] Rearranging, we get \\[\\begin{align*}\nf(\\mathbf{x}) - f(\\mathbf{y}) \\leq \\nabla f(\\mathbf{x})^\\top (\\mathbf{x} - \\mathbf{y}).\n\\end{align*}\\] We will use this inequality in the analysis of the convergence rate of gradient descent.\nAnother assumption we will make on the function \\(f\\) is that \\(G\\)-Lipschitz. Formally, \\(f\\) is \\(G\\)-Lipschitz if, for all \\(\\mathbf{x}\\), \\(\\| \\nabla f(\\mathbf{x}) \\|_2 \\leq G\\). This means that the function \\(f\\) cannot change too quickly.\nFor our analysis of gradient descent, we will assume that \\(f\\) is convex, \\(f\\) is \\(G\\)-Lipschitz, and our starting point \\(\\mathbf{x}^{(0)}\\) is within \\(R\\) of the global minimum \\(\\mathbf{x}^*\\).\nGradient Descent Convergence Bound: If we run gradient with step size \\(eta =\\frac{R}{G \\sqrt{T}}\\) for \\(T \\geq \\frac{R^2 G^2}{\\epsilon^2}\\) iterations, then \\(f(\\hat{\\mathbf{x}}) \\leq f(\\mathbf{x}^*) + \\epsilon\\).\nIntuitively, the proof is tricky because the iterates do not necessarily decrease monotonically. We will prove that the average iterate gives a good solution and, therefore, the best iterate must also give a good solution.\nWe will first prove an intermediate result that if the function value is far from the global minimum, then we take a large step towards the global minimum. This property is intuitively useful because it means that we make good progress towards the global minimum even when we are far way.\nConsider the difference between the next iterate and the global minimum \\[\\begin{align*}\n\\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2\n&= \\| \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}) - \\mathbf{x}^* \\|_2^2 \\\\\n&= \\| (\\mathbf{x}^{(t)} - \\mathbf{x}^*) - \\eta \\nabla f(\\mathbf{x}^{(t)}) \\|_2^2 \\\\\n&= \\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - 2 \\eta \\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t)} - \\mathbf{x}^*) + \\eta^2 \\| \\nabla f(\\mathbf{x}^{(t)}) \\|_2^2 \\\\\n&\\leq \\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - 2 \\eta \\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t)} - \\mathbf{x}^*) + \\eta^2 G^2\n\\end{align*}\\] where the third equality followed because the squared \\(\\ell_2\\) norm is an inner product and the inequality followed by the \\(G\\)-Lipschitz assumption. Dividing by \\(2 \\eta\\) and rearranging, we get \\[\\begin{align*}\n\\nabla f(\\mathbf{x}^{(t)})^\\top (\\mathbf{x}^{(t)} - \\mathbf{x}^*)\n\\leq \\frac{\\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\]\nBy convexity, it follows that \\[\\begin{align*}\nf(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^*)\n\\leq \\frac{\\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\]\nThe next step is to consider the average difference between the iterates and the global minimum \\[\\begin{align*}\n\\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)} - f(\\mathbf{x}^*)\n&\\leq \\frac{\\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2} \\\\\n&+\\frac{\\| \\mathbf{x}^{(1)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(2)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2} \\\\\n&+ \\ldots + \\\\\n&+ \\frac{\\| \\mathbf{x}^{(T-1)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(T)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\] The expression is a telescoping sum because some terms cancel out. Then we can bound the average as \\[\\begin{align*}\n\\frac1{T} \\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)} - f(\\mathbf{x}^*)\n&\\leq \\frac{\\| \\mathbf{x}^{(0)} - \\mathbf{x}^* \\|_2^2  - \\| \\mathbf{x}^{(T)} - \\mathbf{x}^* \\|_2^2}{2 T \\eta} + \\frac{\\eta G^2}{2} \\\\\n&\\leq \\frac{R^2 - 0}{2 T \\eta} + \\frac{\\eta G^2}{2} \\\\\n&= \\frac{R^2 G \\sqrt{T} }{2 T R} + \\frac{R G^2}{2 G \\sqrt{T}} = \\frac{R G}{\\sqrt{T}} \\epsilon^2.\n\\end{align*}\\] The second inequality used that \\(\\mathbf{x}^{(0)}\\) is within \\(R\\) of the global minimum \\(\\mathbf{x}^*\\) and that \\(\\| \\cdot \\|_2^2\\) is non-negative. The first equality followed by plugging in the step size \\(\\eta = \\frac{R}{G \\sqrt{T}}\\). The second equality followed by plugging in the number of iterations \\(T= \\frac{R^2 G^2}{\\epsilon^2}\\).\nFor the final step, we will consider the average function values of the iterates. Then we can bound the difference between the average iterate and the global minimum as \\[\\begin{align*}\n\\frac1{T} \\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)})\n- f(\\mathbf{x}^*)\n\\leq \\epsilon.\n\\end{align*}\\] It must be that the best iterate gives a solution that is at least as good as the average. Then \\[\\begin{align*}\nf(\\hat{\\mathbf{x}}) \\leq \\frac1{T} \\sum_{t=0}^{T-1} f(\\mathbf{x}^{(t)}) \\leq f(\\mathbf{x}^*) + \\epsilon\n\\end{align*}\\] as desired.\nIt may appear that the bound is not particularly useful because we need to know the right learning rate. In practice, we can run gradient descent several times with different learning rates and choose the best parameters."
  },
  {
    "objectID": "notes/09_gradient_descent.html#projected-gradient-descent",
    "href": "notes/09_gradient_descent.html#projected-gradient-descent",
    "title": "Gradient Descent",
    "section": "Projected Gradient Descent",
    "text": "Projected Gradient Descent\nWe so far assumed that the parameters \\(\\mathbf{x}\\) are unconstrained. However, in many applications, we have additional constraints on the parameters. We may have:\n\nA regularization constraint that \\(\\| \\mathbf{x} \\|_2\\) is small. This is common in machine learning applications where we want to avoid overfitting.\nA positivity constraint that \\(\\mathbf{x} \\geq 0\\). This is common in applications where the parameters correspond to real world quantities that cannot be negative.\nA linear constraint that \\(\\mathbf{M x} \\leq \\mathbf{b}\\). This is common in training support vector machines, industrial optimization, and subroutines in integer programming.\n\nToday, we will consider the case where the parameters \\(\\mathbf{x}\\) are constrained to be inside a convex set \\(\\mathcal{S}\\). Notice we are now using the term convex in two different ways: a function is convex if it satisfies the tangent line constraint and a set is convex if the line between any two points in the set is in the set.\nWe can check visually that the set \\(\\mathcal{S}\\) is convex by drawing a line between any two points in the set and verifying that the line is in the set.\n\n\n\nFor example, the set on the left is convex but the set on the right is not convex.\nFormally, a set \\(\\mathcal{S}\\) is convex if for any \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}\\) and \\(\\lambda \\in [0,1]\\), we have \\[\\begin{align*}\n(1- \\lambda) \\mathbf{x} + \\lambda \\mathbf{y}\n\\in \\mathcal{S}.\n\\end{align*}\\]\nThe challenge for gradient descent is that even if we start with an initial point \\(\\mathbf{x}^{(0)} \\in \\mathcal{S}\\), there is no guarantee that the next iterate \\(\\mathbf{x}^{(0)} - \\eta \\nabla f(\\mathbf{x}^{(0)})\\) will be in \\(\\mathcal{S}\\).\nWe will consider an extremely simple modification: We force the next iterate to be in \\(\\mathcal{S}\\) by projecting it onto \\(\\mathcal{S}\\). In order to do this, we will need a third oracle for projections that returns \\[\\begin{align*}\nP_{\\mathcal{S}} (\\mathbf{x})\n= \\arg \\min_{\\mathbf{y} \\in \\mathcal{S}}\n\\| \\mathbf{x} - \\mathbf{y} \\|_2^2.\n\\end{align*}\\]\nProjected gradient descent is a simple modification of gradient descent. Instead of setting the next iterate to \\[\\begin{align*}\nx^{(t+1)} = x^{(t)} - \\eta \\nabla f(x^{(t)}),,,,\n\\end{align*}\\] we set the next iterate to \\[\\begin{align*}\nx^{(t+1)} = P_{\\mathcal{S}} (x^{(t)} - \\eta \\nabla f(x^{(t)})).\n\\end{align*}\\]\nThe analysis for projected gradient descent is almost identical to the analysis for gradient descent. We just need one additional claim that, for any \\(\\mathbf{y} \\in \\mathcal{S}\\), \\[\\begin{align*}\n\\| \\mathbf{y} - P_{\\mathcal{S}} (\\mathbf{x}) \\|_2^2\n\\leq \\| \\mathbf{y} - \\mathbf{x} \\|_2^2.\n\\end{align*}\\]\nWe then argue that \\[\\begin{align*}\nf(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^*)\n&\\leq\n\\frac{\\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t)} - \\eta \\nabla f(\\mathbf{x}^{(t)}) - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2} \\\\\n&\\leq \\frac{\\| \\mathbf{x}^{(t)} - \\mathbf{x}^* \\|_2^2 - \\| \\mathbf{x}^{(t+1)} - \\mathbf{x}^* \\|_2^2}{2 \\eta}  + \\frac{\\eta G^2}{2}.\n\\end{align*}\\] The rest of the analysis follows as before.\nThe bounds we showed for gradient descent and projected gradient descent have a \\(1/\\epsilon^2\\) dependence. Since we care about the approximation, we would ideally likely a \\(1/\\epsilon\\) or even \\(\\log(1/\\epsilon)\\) dependence. Similarly, we would like to reduce or eliminate the dependence on the Lipschitz constant \\(G\\) and the radius \\(R\\). Unfortunately, in order to improve the bounds, we need to make stronger assumptions on the function \\(f\\)."
  },
  {
    "objectID": "notes/15_power_method.html",
    "href": "notes/15_power_method.html",
    "title": "Power Method",
    "section": "",
    "text": "Previously, we used the SVD to find the best rank-\\(k\\) approximation to a matrix \\(\\mathbf{X}\\). For this application, notice that we didn’t really need to find all the singular vectors and values.\nWe can save time by computing an approximate solution to the SVD. In particular, we will only compute the top \\(k\\) singular vectors and values. We can do this with iterative algorithms that achieve time complexity \\(O(ndk)\\) instead of \\(O(nd^2)\\). There are many algorithms for this problem:\nThe power method computes the top right singular vector, \\(\\mathbf{v}_1\\), of a matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n\\times d}\\) with singular value decomposition \\(\\mathbf{X} = \\mathbf{U\\Sigma V}^\\top\\). The method is as follows:\nPower Method:\nIn other words, each of our iterates \\(\\mathbf{z}^{(i)}\\) is simply a scaling of a column in the following matrix: \\[\nK = \\begin{bmatrix} \\mathbf{z}^{(0)} & \\mathbf{A}\\mathbf{z}^{(0)} & \\mathbf{A}^2 \\mathbf{z}^{(0)} & \\mathbf{A}^3\\mathbf{z}^{(0)} \\ldots \\mathbf{A}^q \\mathbf{z}^{(0)}\\end{bmatrix},\n\\] where \\(\\mathbf{A} = \\mathbf{X}^\\top\\mathbf{X}\\). Typically we run \\(q \\ll d\\) iterations, so \\(K\\) is a tall, narrow matrix. The span of \\(K\\)’s columns is called a Krylov subspace because it is generated by starting with a single vector \\(\\mathbf{z}^{(0)}\\) and repeatedly multiplying by a fixed matrix \\(A\\). We will return to \\(K\\) shortly."
  },
  {
    "objectID": "notes/15_power_method.html#the-lanczos-method",
    "href": "notes/15_power_method.html#the-lanczos-method",
    "title": "Power Method",
    "section": "The Lanczos Method",
    "text": "The Lanczos Method\nWe will now see how to improve on power method using what is known as the Lanczos method. Like power method, Lanczos is considered a Krylov subspace method because it will return a solution in the span of the Krylov subspace that we introduced before. Power method clearly does this – it returns a scaling of the last column of \\(K\\). The whole idea behind Lanczos is to avoid “throwing away’” information from earlier columns like power method, but instead to take advantage of the whole space. It turns at that doing so can be very helpful – we will get a bound that depends on \\(1/\\sqrt{\\gamma}\\) instead of \\(1/\\gamma\\).\nSpecifically, to define the Lanczos method, we will let \\(\\mathbf{Q}\\in \\mathbb{R}^{d\\times k}\\) be a matrix with orthonormal columns that spans \\(K\\). In practice, you need to be careful about how this is computed for numerical stability reasons, but we won’t worry about that for now. Imagine your computer has infinite precision and we just compute \\(\\mathbf{Q}\\) by orthonormalizing \\(K\\).\nLanczos Method\n\nCompute an orthonormal span \\(\\mathbf{Q}\\) for the degree \\(q\\) Krylov subspace.\nLet \\(\\mathbf{z}\\) be the top eigenvector of \\(\\mathbf{Q}^\\top \\mathbf{A}\\mathbf{Q} = \\mathbf{Q}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{Q}\\)\nReturn \\(\\mathbf{Q} \\mathbf{z}\\).\n\nImportantly, the first step only requires \\(q\\) matrix-vector multiplications with \\(\\mathbf{X}^\\top\\mathbf{X}\\), each of which can be implemented in \\(O(nd)\\) time, just like we did for power method. The second step might look a bit circular at first glance. We want an approximation algorithm for computing the top eigenvector of \\({\\mathbf{X}}^\\top\\mathbf{X}\\) and the above method uses a top eigenvector algorithm as a subroutine. But note that \\(\\mathbf{Q}^\\top {\\mathbf{AQ}}\\) only has size \\(q\\times q\\), where \\(q\\ll d\\) is our iteration count. So even if it’s too expensive to compute a direct eigendecomposition of \\(\\mathbf{X}^\\top\\mathbf{X}\\), \\({\\mathbf{Q}}^\\top{\\mathbf{X}}^\\top{\\mathbf{X}}{\\mathbf{Q}}\\) can be computed in \\(O(ndq) + O(dq^2) + O(q^3) + O(ndq)\\) time (the first part is the time to construct \\(\\mathbf{Q}\\) and \\(\\mathbf{X}\\mathbf{Q}\\), the second to compute \\({\\mathbf{Q}}^\\top{\\mathbf{X}}^\\top{\\mathbf{X}}{\\mathbf{Q}}\\), and the third to find its top eigenvector using a direct method).\n\nAnalysis Preliminaries\nOur first claim is that Lanczos returns the best approximate singular vector in the span of the Krylov subspace. Then we will argue that there always exists some vector in the span of the subspace that is significantly better than what power method returns, so the Lanczos solution must be significantly better as well.\nClaim 2: Amongst all vectors in the span of the Krylov subspace (i.e., any vector \\(\\mathbf{y}\\) that can be written as \\(\\mathbf{y} = \\mathbf{Q}\\mathbf{x}\\) for some \\(x\\in \\mathbb{R}^k\\)), \\(\\mathbf{y}^* = \\mathbf{Q}\\mathbf{z}\\) minimizes the low-rank approximation error \\(\\|\\mathbf{X} - \\mathbf{X}\\mathbf{y}\\mathbf{y}^\\top\\|_F^2\\).\nProof. First, we can prove that \\(\\mathbf{y}\\) should always be chosen to have unit norm, so that \\(\\mathbf{y}\\mathbf{y}^\\top\\) is a projection. Accordingly, it must also be that \\(\\mathbf{x} = \\mathbf{Q}^\\top\\mathbf{y}\\) has unit norm. Then, proving the claim above is equivalent to proving that \\(\\mathbf{y}^* = {\\mathbf{Q}}\\mathbf{z}\\) maximizes \\(\\|\\mathbf{X}\\mathbf{y}\\mathbf{y}^\\top\\|_F^2 = \\|\\mathbf{X}\\mathbf{y}\\|_F^2 = \\|\\mathbf{X}\\mathbf{y}\\|_2^2 = \\|\\mathbf{X}\\mathbf{Q}\\mathbf{x}\\|_2^2\\). It is not hard to check that setting \\(\\mathbf{x}\\) to be the maximum right singular vector of \\({\\mathbf{X}}{\\mathbf{Q}}\\) maximizes this \\(\\|\\mathbf{X}\\mathbf{Q}\\mathbf{x}\\|_2^2\\) amongst all unit vectors, and this right singular vector is the same at the top eigenvector of \\(\\mathbf{Q}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{Q}\\).\nClaim 3: If we run the Lanczos method for \\(O\\left(\\frac{\\log(d/\\epsilon)}{\\sqrt{\\gamma}}\\right)\\) There is some vector \\(\\mathbf{w}\\) of the form \\(\\mathbf{w} = \\mathbf{Q}\\mathbf{x}\\) such that either \\(\\langle\\mathbf{v}_1,\\mathbf{w}\\rangle \\geq 1-\\epsilon\\) or \\(\\langle- \\mathbf{v}_1,\\mathbf{w}\\rangle \\geq 1-\\epsilon\\).\nIn other words, there is some \\(\\mathbf{w}\\) in the Krylov subspace that has a large inner product with the true top singular vector \\(\\mathbf{v}_1\\). As seen earlier for power method, this can be used to prove that e.g. \\(\\|\\mathbf{X}\\mathbf{w}\\mathbf{w}^\\top\\|_F^2\\) is large, and from Claim 2, we know that the \\(\\mathbf{v}\\) returned by Lanczos can only do better. So, we focus on proving Claim 3.\n\n\nHeart of the Analysis\nThe key idea is to observe that any vector in the span of the Krylov subspace of size \\(q\\) – i.e. any vector \\(\\mathbf{w}\\) that can be written \\(\\mathbf{w} = \\mathbf{Q}\\mathbf{x}\\) is equal to: \\[\n\\mathbf{w} = p(\\mathbf{A})\\mathbf{z}^{(0)},\n\\] for some degree q polynomial \\(p\\). For example, we might have \\(p(\\mathbf{A}) = 2\\mathbf{\\mathbf{A}}^2 - 4\\mathbf{A}^3 + \\mathbf{A}^6\\) or \\(p(\\mathbf{A}) = I - \\mathbf{A} - 10\\mathbf{A}^5\\). And moreover, for any degree \\(q\\) polynomial \\(p\\), there is some \\(\\mathbf{x}\\) such that \\(\\mathbf{Q}\\mathbf{x} = p(\\mathbf{A})\\mathbf{z}^{(0)}\\). To see that this is true, we note that all of the monomials \\(\\mathbf{z}^{(0)}, A\\mathbf{z}^{(0)}, \\ldots, \\mathbf{A}^q\\mathbf{z}^{(0)}\\) lie in the span of \\(\\mathbf{Q}\\), so any linear combination does as well.\nThis means that showing there is a good approximate top singular vector \\(\\mathbf{w}\\) in the span of the Krylov subspace can be reduced to finding a good polynomial \\(p(\\mathbf{A})\\). Specifically, notice that, if we write \\(p(\\mathbf{A})\\mathbf{z}^{(0)}\\) in the span of the singular vectors we have: \\[\np(\\mathbf{A})\\mathbf{z}^{(0)} = g_1 \\mathbf{v}_1 + g_2 \\mathbf{v}_2 + \\ldots + g_d \\mathbf{v}_d\n\\] where \\[\ng_j = c_j^{(0)}p(\\sigma_j^2).\n\\] Our goal, which is exactly the same as when we analyzed power method, is to show that \\(g_1\\) is much larger than \\(g_j\\) for all \\(j \\neq 1\\). In other words we want to find a polynomial such that \\(p(t)\\) is small for all values of \\(0\\leq t&lt; \\sigma_1^2\\), but then the polynomial should suddenly jump to be large at \\(\\sigma_1\\). The simplest degree \\(q\\) polynomial that does this is \\(p(t) = t^q\\). However, it turns out there are more sharply jumping polynomials, which can be obtained by shifting/scaling a type of polynomials known as a Chebyshev polynomial. An example that jumps at \\(\\sigma_1^2 = 1\\) is shown below. The difference from \\(t^q\\) is pretty remarkable – even though the polynomial \\(p\\) shown below is nearly as small for all \\(0\\leq t&lt; 1\\), it is much larger at \\(t=1\\).\n\n\n\n\n\n\nConcretely we can claim the following, which is a bit tricky to prove, but well known (see Lemma 5 here for a full proof).\nClaim 4: There is a degree \\(O\\left(\\sqrt{\\frac{1}{\\gamma}}\\log\\frac{1}{\\epsilon}\\right)\\) degree polynomial \\(\\hat{p}\\) such that \\(\\hat{p}(1) = 1\\) and \\(|p(t)| \\leq \\epsilon\\) for \\(0 \\leq t \\leq 1-\\gamma\\).\nIn contrast, to ensure that \\(t^q\\) satisfies the above, we would need degree q = \\(O\\left({\\frac{1}{\\gamma}\\log\\frac{1}{\\epsilon}}\\right)\\) – a quadratically worse bound. This is what will account for the quadratic difference in performance between the Lanczos and Power Methods.\nThe fact that there exist much steeper low-degree polynomails than \\(t^q\\) is the sort of thing studied in the mathematical field known as Approximation Theory. That might seem pretty obscure, but steep polynomials are surprisingly useful in computer science, appearing everywhere from classic, to Quantum complexity theory, to learning theory. If you’re interested in learning more about this, check out slides for this talk.\n\n\nFinishing Up\nFinally, we use Claim 4 to finish the analysis of Lanczos. Consider the vector \\(\\hat{p}\\left(\\frac{1}{\\sigma_1^2}A\\right)\\mathbf{z}^{(0)}\\), which as argued above lies in the Krylov subspace. As discussed before, our job is to prove that: \\[\n\\frac{c_j^{(0)}\\hat{p}\\left(\\frac{1}{\\sigma_1^2}\\sigma_j^2\\right)}{c_1^{(0)}\\hat{p}\\left(\\frac{1}{\\sigma_1^2}\\sigma_1^2\\right)} \\leq \\sqrt{\\epsilon/d},\n\\] for all \\(j \\neq i\\), as long as \\(\\hat{p}\\) is chosen to have degree \\(q = O\\left(\\sqrt{\\frac{1}{\\gamma}}\\log\\frac{d}{\\epsilon}\\right)\\)\nConsider the numerator first. \\(\\frac{\\sigma_j^2}{\\sigma_1^2} = \\left(\\frac{\\sigma_j}{\\sigma_1}\\right)^2 = \\left(1 - \\left(1-\\frac{\\sigma_j}{\\sigma_1}\\right)\\right)^2 = \\left(1 - \\frac{\\sigma_1 - \\sigma_j}{\\sigma_1}\\right)^2 \\leq (1-\\gamma)^2 \\leq (1-\\gamma)\\) .\nAccordingly, if we set \\(q = O\\left(\\sqrt{\\frac{1}{\\gamma}}\\log\\frac{1}{\\epsilon'}\\right)\\) where \\(\\epsilon' = \\sqrt{\\epsilon/d}/d^3\\), then \\(\\hat{p}\\left(\\frac{1}{\\sigma_1^2}\\sigma_j^2\\right) \\leq \\epsilon'\\). And the denomenator is simply equal to \\(c_1^{(0)}\\hat{p}(1) =  c_1^{(0)}\\cdot 1\\). So, since \\(c_j^{(0)}/c_1^{(0)}\\leq d^3\\) with high probability, as argued earlier, the equation holds. And thus Claim 3 is proven by setting \\(\\mathbf{w} = \\hat{p}\\left(\\frac{1}{\\sigma_1^2} \\mathbf{A} \\right)\\mathbf{z}^{(0)}\\)."
  },
  {
    "objectID": "notes/16_spectral_graph_theory.html",
    "href": "notes/16_spectral_graph_theory.html",
    "title": "Spectral Graph Theory",
    "section": "",
    "text": "The main idea of spectral graph theory is to understand graph data by constructing natural matrix representations and studying their spectrums.\nThere are many natural datasets that appear naturally as graphs:\nFor now, we will assume that a graph \\(G=(V,E)\\) is undirected and unweighted on \\(n\\) nodes.\nThere are two common matrix representations of a graph. The first is an \\(n \\times n\\) adjacency matrix \\(\\mathbf{A}\\) where \\(A_{ij} = 1\\) if \\((i,j) \\in E\\) and \\(A_{ij} = 0\\) otherwise. The second is an \\(n \\times n\\) Laplacian matrix \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\) where \\(\\mathbf{D}\\) is the diagonal degree matrix with \\(D_{ii} = \\sum_{j=1}^n A_{ij}\\).\nIt is also common to look at normalized versions of both matrices \\[\n\\bar{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{AD}^{-1/2} \\qquad\n\\mathbf{L} = \\mathbf{I} - \\bar{\\mathbf{A}}.\n\\]\nThe adjacency and Laplacian matrices contain a lot of information about the graph.\nToday, we’ll see how eigenvectors are useful for clustering and visualizing graphs.\nWe’ll use the edge-incidence matrix \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(m\\) is the number of edges in the graph. Consider the edge \\((i,j) \\in [m]\\) and the node \\(k \\in V\\), then \\[\nB_{(i,j),k} = \\begin{cases}\n1 & \\text{if } k = i \\\\\n-1 & \\text{if } k = j \\\\\n0 & \\text{otherwise}\n\\end{cases}.\n\\]\nWe can write the Laplacian as \\[\n\\mathbf{L} = \\mathbf{B}^\\top \\mathbf{B}\n= \\mathbf{b}_1 \\mathbf{b}_1^\\top\n+ \\mathbf{b}_2 \\mathbf{b}_2^\\top\n+ \\ldots +\n\\mathbf{b}_m \\mathbf{b}_m^\\top\n\\] where \\(\\mathbf{b}_i\\) is the \\(i\\)th row of \\(\\mathbf{B}\\) (each row corresponds to a single edge).\nFrom this view, we can conclude that\nWith these observations in mind, consider the function \\(f(\\mathbf{x}) = \\mathbf{x}^\\top \\mathbf{L x}\\) for some vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\). Notice that \\(f(\\mathbf{x})\\) is small if \\(\\mathbf{x}\\) is smooth with respect to the graph. In terms of our linear algebraic view, if we plug a small eigenvector into \\(f\\), we get a small value.\nWe can formally see this connection through the the Courant-Fischer min-max principle. Let \\(\\mathbf{V} = [\\mathbf{v}_1, \\ldots, \\mathbf{v}_n]\\) be the eigenvectors of \\(\\mathbf{L}\\) where \\(\\mathbf{v}_n\\) corresponds to the smallest eigenvalue \\(\\lambda_n\\). Then \\[\\begin{align*}\n\\mathbf{v}_n &= \\arg \\min_{\\|\\mathbf{v}\\|_2 =1}\n\\mathbf{v}^\\top \\mathbf{L} \\mathbf{v} \\\\\n\\mathbf{v}_{n-1} &= \\arg \\min_{\\|\\mathbf{v}\\|_2 =1, \\mathbf{v} \\perp \\mathbf{v}_n}\n\\mathbf{v}^\\top \\mathbf{L} \\mathbf{v} \\\\\n&\\vdots \\\\\n\\mathbf{v}_1 &= \\arg \\min_{\\|\\mathbf{v}\\|_2 =1, \\mathbf{v} \\perp \\mathbf{v}_n, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_2} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v}.\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\n\\mathbf{v}_1 &= \\arg \\max_{\\|\\mathbf{v}\\|_2 =1} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v} \\\\\n\\mathbf{v}_{2} &= \\arg \\max_{\\|\\mathbf{v}\\|_2 =1, \\mathbf{v} \\perp \\mathbf{v}_1} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v} \\\\\n&\\vdots \\\\\n\\mathbf{v}_n &= \\arg \\max_{\\|\\mathbf{v}\\|_2 =1, \\mathbf{v} \\perp \\mathbf{v}_1, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{n-1}} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v}.\n\\end{align*}\\]"
  },
  {
    "objectID": "notes/16_spectral_graph_theory.html#spectral-clustering",
    "href": "notes/16_spectral_graph_theory.html#spectral-clustering",
    "title": "Spectral Graph Theory",
    "section": "Spectral Clustering",
    "text": "Spectral Clustering\nWe can draw another conclusion from our observation that \\(\\mathbf{L} = \\mathbf{B}^\\top \\mathbf{B}\\). Let \\(\\mathbf{c} \\in \\{-1, 1\\}^n\\) be a cut indicator vector. Consider a set of vertices \\(S \\subseteq V\\). We set \\(\\mathbf{c}_i = 1\\) if \\(i \\in S\\) and \\(\\mathbf{c}_i = -1\\) otherwise. Then \\[\\begin{align*}\n\\mathbf{c}^\\top \\mathbf{L} \\mathbf{c}\n&= \\mathbf{c}^\\top \\mathbf{B}^\\top \\mathbf{B} \\mathbf{c} \\\\\n&= \\sum_{(i,j) \\in E} (c_i - c_j)^2 \\\\\n&= 4 \\cdot \\text{cut}(S, S^c).\n\\end{align*}\\] where \\(\\text{cut}(S, S^c)\\) is the number of edges between \\(S\\) and its complement \\(S^c\\).\nPartitioning a graph is an important problem in:\n\nUnderstanding social networks,\nUnsupervised machine learning (clustering),\nGraph visualization, and\nMesh partitioning.\n\nWe will see how this problem can be solved heuristically using the eigenvectors of the Laplacian matrix.\nIn addition, we will give an “average case” analysis of the model for a common random graph model. The tools we will use are matrix concentration and eigenvector perturbation bounds.\nGiven a graph \\(G = (V,E)\\), we want to partition the vertices into two sets \\(S\\) and \\(S^c\\) such that\n\nthe number of edges between \\(S\\) and \\(S^c\\) is small and\n\\(S\\) and \\(S^c\\) are each not too small.\n\nAn example of this problem is understanding community structure in social networks. In 1977, Zachary studied a karate club that split into two groups.\n“At the beginning of the study there was an incipient conflict between the club president, John A., and Mr. Hi over the price of karate lessons. Mr. Hi, who wished to raise prices, claimed the authority to set his own lesson fees, since he was the instructor. John A., who wished to stabilize prices, claimed the authority to set the lesson fees since he was the club’s chief administrator. As time passed the entire club became divided over this issue, and the conflict became translated into ideological terms by most club members.”\nZachary constructed a social network by hand and used a minimum cut algorithm to correctly predict who would join each group. The paper is a classic in the field of social network analysis.\nThe problem is also generally useful for other clustering problems. Often, we can construct a synthetic graph for data that is hard to cluster.\n\n\n\nBalanced cut algorithms are also used in many other applications including distributing data in graph databases, partitioning finite element meshes in scientific computing (e.g., that arise when solving differential equations), and more.\nThere are many ways to formalize the balanced cut problem.\n\\(\\beta\\)-Balanced Cut: Consider \\(\\beta \\in [0,\\frac12]\\). Given a graph \\(G = (V,E)\\), the problem is to find the set \\(S \\subseteq V\\)\n\\[\\begin{align*}\n\\arg \\min_{S \\subseteq V} \\text{cut}(S, S^c) \\quad \\text{subject to} \\quad\n\\min( |S|, |S^c|) \\geq \\beta |V|.\n\\end{align*}\\]\nSparsest Cut: Given a graph \\(G = (V,E)\\), the problem is to find the set \\(S \\subseteq V\\) \\[\\begin{align*}\n\\arg \\min_{S \\subseteq V} \\frac{\\text{cut}(S, S^c)}{\\min(|S|, |S^c|)}.\n\\end{align*}\\]\nAll natural formalizations lead to NP-hard problems. There is lots of interest in designing polynomial time approximation algorithms but these tend to be slow. In practice, there are much simpler methods based on the spectrum of the Laplacian matrix.\nGenerally, spectral methods run in at most \\(O(n^3)\\) time. The runtime can be further sped up if we use iterative methods for computing the eigenvectors.\nThe basic spectral clustering method is to:\n\nCompute the second smallest eigenvector \\(\\mathbf{v}_{n-1}\\) of a graph.\nDefine \\(S\\) as the nodes with positive entries in \\(\\mathbf{v}_{n-1}\\).\nReturn the set \\(S\\).\n\nNote that this algorithm should not make sense yet. Shortly, we will see how this method is a “relax and round” algorithm in disguise.\nFrom the view of the Laplacian matrix, notice that\n\nthe cut size is \\(\\mathbf{c}^\\top \\mathbf{L} \\mathbf{c} = 4 \\cdot \\text{cut}(S, S^c)\\) and\nthe imbalance is \\(|\\mathbf{c}^\\top \\mathbf{1}|= ||S| - |S^c||\\).\n\nWe want to minimize both the cut size and the imbalance.\nWe can reach an equivalent formulation if we divide everything by \\(\\sqrt{n}\\) so that the cut indicator vector \\(\\mathbf{c}\\) has norm 1. Then \\(\\mathbf{c} \\in \\{-\\frac1{\\sqrt{n}}, \\frac1{\\sqrt{n}}\\}^n\\) and\n\n\\(\\mathbf{c}^\\top \\mathbf{L} \\mathbf{c} = \\frac{4}{n} \\cdot \\text{cut}(S, S^c)\\)\n\\(|\\mathbf{c}^\\top \\mathbf{1}| = \\frac{1}{\\sqrt{n}} \\cdot ||S| - |S^c||\\).\n\nWith this mathematical notation, the perfectly balanced cut problem is to find\n\\[\\begin{align*}\n\\min_{\\mathbf{c} \\in \\{-\\frac1{\\sqrt{n}}, \\frac1{\\sqrt{n}}\\}^n}\n\\mathbf{c}^\\top \\mathbf{L} \\mathbf{c} \\quad \\text{subject to} \\quad |\\mathbf{c}^\\top \\mathbf{1}| = 0.\n\\end{align*}\\]\nWe can also write the relaxed perfectly balanced cut problem to find\n\\[\\begin{align*}\n\\min_{\\mathbf{c} : \\|\\mathbf{c}\\|_2 = 1}\n\\mathbf{c}^\\top \\mathbf{L} \\mathbf{c} \\quad \\text{subject to} \\quad |\\mathbf{c}^\\top \\mathbf{1}| = 0.\n\\end{align*}\\]\nClaim: The solution to the relaxed perfectly balanced cut problem is the second smallest eigenvector \\(\\mathbf{v}_{n-1}\\) of \\(\\mathbf{L}\\).\nProof: By the Courant-Fischer min-max principle, the smallest eigenvector of a graph Laplacian \\(\\mathbf{L}\\) is \\[\\begin{align*}\n\\mathbf{v}_n = \\arg \\min_{\\mathbf{v}: \\|\\mathbf{v}\\|_2 = 1} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v}\n\\end{align*}\\]. The smallest eigenvector is the constant vector \\(\\mathbf{1} \\cdot \\frac1{\\sqrt{n}}\\).\nAgain by the Courant-Fischer min-max principle, the second smallest eigenvector is \\[\\begin{align*}\n\\mathbf{v}_{n-1} = \\arg \\min_{\\mathbf{v}: \\|\\mathbf{v}\\|_2 = 1, \\mathbf{v}^\\top \\mathbf{v}_n=0} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v}.\n\\end{align*}\\] Since we know that \\(\\mathbf{v}_n = \\mathbf{1} \\cdot \\frac1{\\sqrt{n}}\\), we know that \\[\\begin{align*}\n\\mathbf{v}_{n-1} = \\arg \\min_{\\mathbf{v}: \\|\\mathbf{v}\\|_2 = 1, \\mathbf{v}^\\top \\mathbf{1}=0} \\mathbf{v}^\\top \\mathbf{L} \\mathbf{v}.\n\\end{align*}\\] Notice this is exactly the relaxed perfectly balanced cut problem.\nOur approach will be to find \\(\\mathbf{v}_{n-1}\\). Then we will define \\(S\\) as all the nodes with positive entries in \\(\\mathbf{v}_{n-1}\\). Succinctly, we will set the cut indicator vector \\(\\mathbf{c} = \\text{sign}(\\mathbf{v}_{n-1})\\).\n\n\n\nThere are many variants of this approach used in practice.\n\nSome methods perform normalization on the edge weights. For example, the Shi-Malik algorithm uses the normalized Laplacian \\(\\bar{\\mathbf{L}} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}\\).\nSome methods choose different thresholds to compute the partitions.\nSome methods split the graph into more than two partitions.\n\nWe’ll briefly discuss multiway spectral partitioning. The approach is as follows:\n\nCompute the smallest \\(\\ell\\) eigenvectors \\(\\mathbf{v}_{n-1}, \\ldots, \\mathbf{v}_{n-\\ell}\\) of \\(\\mathbf{L}\\).\nRepresent each node by its corresponding row in the matrix \\(\\mathbf{V} \\in \\mathbb{R}^{n \\times \\ell}\\) whose columns are \\(\\mathbf{v}_{n-1}, \\ldots, \\mathbf{v}_{n-\\ell}\\).\nCluster the rows using \\(k\\)-means clustering (or any other clustering algorithm).\n\nSince we used a relaxed version of the perfectly balanced cut problem, it is not clear that the algorithm will work. However, intuitively, the vectors \\(\\mathbf{v} \\in \\{\\mathbf{v}_{n-1}, \\ldots, \\mathbf{v}_{n-\\ell}\\}\\) are smooth over the graph since \\[\n\\mathbf{v}^\\top \\mathbf{L} \\mathbf{v} = \\sum_{(i,j) \\in E} (v_i - v_j)^2\n\\] is small. The embedding explicitly encourages nodes connected by an edge to be placed in nearby locations in the embedding.\n\nAverage Case Analysis\nSo far, we showed that spectral clustering partitions a graph along a small cut between large pieces. Unfortunately, there are no formal guarantees on the quality of the partioning and it can fail for worst case input graphs.\nWe will consider a generative model that produces random but realistic inputs and analyze how the algorithm performs on graphs from this model. The general idea is common in algorithm design and analysis. Often, the approach is our best hope for understanding why some algorithms just work in practice. For example, linear regression is motivated by “average case” Bayesian modeling.\nWe will consider the stochastic block model (SBM) which is a random graph model that is commonly used to model social networks.\nStochastic Block Model: Let \\(0&lt;q &lt; p &lt; 1\\). We will call \\(G_n(p,q)\\) a distribution over graphs on \\(n\\) nodes. Each graph is split equally into two blocks \\(B\\) and \\(C\\) each with \\(\\frac{n}{2}\\) nodes. Any two nodes in the same group are connected with probability \\(p\\) (including self-loops) and any two nodes in different groups are connected with probability \\(q\\).\nConsider the adjacency matrix \\(\\mathbf{A}\\) of a graph \\(G_n(p,q)\\).\n\n\n\nNote that we ordered the nodes in the adjacency matrix given in the figure. In reality, the order of the nodes would be “scrambled” and the blocks would not be so obvious.\nGiven a graph drawn from the stochastic block model \\(G_n(p,q)\\), our goal is to find the ground truth balanced partitions \\(B\\) and \\(C\\) using spectral clustering. Our first step is to understand the second smallest eigenvector of the Laplacian \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\). We will start by considering the expected matrices \\(\\mathbb{E}[\\mathbf{L}] = \\mathbb{E}[\\mathbf{D}] - \\mathbb{E}[\\mathbf{A}]\\).\n\n\n\nWe will use the simplicity of the expected adjacency matrix to understand the eigenvectors of the expected Laplacian matrix. The top eigenvector \\(\\mathbf{v}_1\\) is proportional to the vector \\(\\mathbf{1}\\) with the eigenvalue \\(\\lambda_1 = \\frac{(p+q)n}{2}\\). The second eigenvector \\(\\mathbf{v}_2\\) is the cut indicator for groups \\(B\\) and \\(C\\) with eigenvalue \\(\\lambda_2 = \\frac{(p-q)n}{2}\\). We can check that the eigendecomposition is correct because there are only two eigenvectors.\n\n\n\nNotice that if we correctly compute the second eigenvector \\(\\mathbf{v}_2\\), then we can exactly recover the ground truth partition \\(B\\) and \\(C\\).\nBecause \\(\\mathbb{E}[\\mathbf{L}] = \\mathbb{E}[\\mathbf{D}] - \\mathbb{E}[\\mathbf{A}]\\), the second smallest eigenvector of \\(\\mathbb{E}[\\mathbf{L}]\\) is the second largest eigenvector of \\(\\mathbb{E}[\\mathbf{A}]\\). So we know that the cut indicator vector \\(\\mathbf{c}\\) is the second smallest eigenvector of \\(\\mathbb{E}[\\mathbf{L}]\\).\nIf the random graph \\(G\\) drawn from the stochastic block model \\(G_n(p,q)\\) were exactly equal to its expectation, we could recover the ground truth partition \\(B\\) and \\(C\\). However, the graph \\(G\\) is not exactly equal to its expectation because of the randomness in the generative process.\nNonetheless, we will use matrix concentration to show that the eigenvectors of \\(\\mathbf{L}\\) are close to the eigenvectors of \\(\\mathbb{E}[\\mathbf{L}]\\). The approach is analagous to scalar concentration inequalities that we’ve seen before like Markov’s, Chebyshev’s, and Chernoff’s inequalities.\nMatrix Concentration Inequality: If \\(p \\geq O \\left( \\frac{\\log^4 n}{n} \\right)\\), then with high probability \\[\\begin{align*}\n\\| \\mathbf{A} - \\mathbb{E}[ \\mathbf{A} ] \\|_2 \\leq O \\left( \\sqrt{pn} \\right)\n\\end{align*}\\] where \\(\\| \\cdot \\|_2\\) is the matrix spectral norm.\nRecall that the spectral norm of a matrix \\(\\mathbf{A}\\) is \\[\n\\| \\mathbf{A} \\|_2 = \\max_{\\mathbf{x} :\\|\\mathbf{x}\\|_2=1 } \\| \\mathbf{A} \\mathbf{x} \\|_2 = \\sigma_1(\\mathbf{A})\n\\] where \\(\\sigma_1(\\mathbf{A})\\) is the largest singular value of \\(\\mathbf{A}\\).\nWhen the graph is drawn from the stochastic block model, we know that the constant vector of the adjacency matrix is roughly the top eigenvector (because the smallest eigenvector of the Laplacian is roughly the constant vector). Then we know that spectral norm of the adjacency matrix \\(\\| \\mathbf{A} \\|_2\\) is on the order of \\(O(p \\sqrt{n})\\) so another way of thinking about the right hand side of the matrix concentration inequality is \\(\\| \\mathbf{A}\\|_2/\\sqrt{p}\\). In other words, the bound tightens as \\(p\\) increases.\nFor the stochastic block model application, we want to show that the second eigenvectors of \\(\\mathbf{A}\\) and \\(\\mathbb{E}[\\mathbf{A}]\\) are close. For this, we will use the following theorem.\nDavis-Kahan Eigenvector Perturbation Theorem: Suppose \\(\\mathbf{A}, \\bar{\\mathbf{A}} \\in \\mathbb{R}^{n \\times n}\\) are symmetric matrices with eigenvectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\) and \\(\\bar{\\mathbf{v}}_1, \\ldots, \\bar{\\mathbf{v}}_n\\), respectively. Let \\(\\theta(\\mathbf{v}, \\bar{\\mathbf{v}})\\) denote the angle between two vectors \\(\\mathbf{v}\\) and \\(\\bar{\\mathbf{v}}\\). Then \\[\\begin{align*}\n\\sin \\theta(\\mathbf{v}_i, \\bar{\\mathbf{v}}_i)\n\\leq \\frac{\\| \\mathbf{A} - \\bar{\\mathbf{A}} \\|_2 }{\\min_{j \\neq i} |\\lambda_i - \\lambda_j|}\n\\end{align*}\\] where \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_n\\) and are the eigenvalues of \\(\\bar{\\mathbf{A}}\\).\nWe can exhibit a matrix where the bound is tight. Let \\(\\mathbf{A}\\) and \\(\\bar{\\mathbf{A}}\\) be the matrices in the figure below. The top eigenvector \\(\\mathbf{v}_1\\) is \\([1,0]\\) whereas the top eigenvector \\(\\bar{\\mathbf{v}}_1\\) is \\([0,1]\\). The angle between these two vectors is \\(\\frac{\\pi}{2}\\) so \\(\\sin(\\theta(\\mathbf{v}_1, \\bar{\\mathbf{v}}_1)) = 1\\). Since \\(\\lambda_1 = 1+\\epsilon\\) and \\(\\lambda_2 = 1\\), the bound gives \\(\\frac{\\epsilon}{\\epsilon} = 1\\).\n\n\n\nWe will apply the Davis-Kahan theorem to the stochastic block model with \\(\\bar{\\mathbf{A}} = \\mathbb{E}[\\mathbf{A}]\\).\nRecall that \\(\\mathbb{E}[\\mathbf{A}]\\) has eigenvalues \\(\\lambda_1 = \\frac{n(p+q)}{2}\\) and \\(\\lambda_2 = \\frac{n(p-q)}{2}\\). Then \\(\\min_{j \\neq i} |\\lambda_i - \\lambda_j| = \\min( qn, \\frac{n(p-q)}{2})\\). Assume that \\(\\frac{n(p-q)}{2}\\) is the minimum of these two gaps.\nThen applying the matrix concentration inequality and the Davis-Kahan theorem, we get the following.\nFor \\(p \\geq O \\left( \\frac{\\log^4 n}{n} \\right)\\), with high probability we have \\[\\begin{align*}\n\\sin \\theta(\\mathbf{v}_2, \\bar{\\mathbf{v}}_2)\n&\\leq \\frac{O \\left( \\sqrt{pn} \\right)}{\\min_{j \\neq i} |\\lambda_i - \\lambda_j|} \\\\\n&\\leq \\frac{O \\left( \\sqrt{pn} \\right)}{\\frac{n(p-q)}{2}} \\\\\n&= O \\left( \\frac{\\sqrt{p}}{(p-q)\\sqrt{n}} \\right).\n\\end{align*}\\]\nTo relate the angle to the \\(\\ell_2\\)-norm difference, consider two unit vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\). We have \\[\\begin{align*}\n\\| \\mathbf{a - b} \\|_2^2\n&= \\| \\mathbf{a} \\|_2^2 + \\| \\mathbf{b} \\|_2^2 - 2 \\mathbf{a}^\\top \\mathbf{b} \\\\\n&= 2 - 2 \\cos(\\theta) \\\\\n&= 2 - 2 \\sqrt{1-\\sin^2(\\theta)} \\\\\n&\\leq 2 - 2 (1-\\sin^2(\\theta) \\\\\n& = 2 \\sin^2(\\theta)\n\\end{align*}\\] where the inequality follows since \\(\\sin^2(\\theta) \\leq 1\\). Then \\[\\begin{align*}\n\\| \\mathbf{v}_2 - \\bar{\\mathbf{v}}_2 \\|_2^2 \\leq\nO \\left( \\frac{p}{(p-q)^2 n} \\right).\n\\end{align*}\\]\nWe know that \\(\\bar{\\mathbf{v}}_2\\) is \\(\\frac1{\\sqrt{n}} \\cdot \\mathbf{c}\\) where \\(\\mathbf{c}\\) is the cut indicator vector for the ground truth partition \\(B\\) and \\(C\\).\nWe want to show that \\(\\text{sign}(\\mathbf{v}_2)\\) is close to \\(\\mathbf{v}_2\\). Notice they will only differ at locations where \\(\\mathbf{v}_2\\) and \\(\\bar{\\mathbf{v}}_2\\) have opposite signs. Since every entry \\(i\\) that differs in sign contributes at least \\(\\frac1{n}\\) to the squared \\(\\ell_2\\) norm of \\(\\mathbf{v}_2 - \\bar{\\mathbf{v}}_2\\), we know that \\(\\mathbf{v}_2\\) and \\(\\bar{\\mathbf{v}}_2\\) differ in at most \\(O \\left( \\frac{p}{(p-q)^2} \\right)\\) entries.\nAverage Case Result: If \\(G\\) is a stochastic block model graph with \\(p \\geq O \\left( \\frac{\\log^4 n}{n} \\right)\\). Then if we compute the second largest eigenvector \\(\\mathbf{v}_2\\) of \\(\\mathbf{A}\\) and assign nodes to the communities according to the sign pattern of this vector, we will correctly assign all but \\(O \\left( \\frac{p}{(p-q)^2} \\right)\\) nodes.\nNotice that the error is small when \\(p\\) is large and \\(q\\) is small. The problem becomes more challenging when \\(p \\approx q\\). In this setting, the Davis-Kahan theorem becomes loose and it is more difficult to tell the difference between the two groups."
  }
]